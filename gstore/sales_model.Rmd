---
title: "GStore Predictor"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(gbm)
library(data.table)
library(plyr)
library(stringi)
library(stringr)
library(ggplot2)
library(gridExtra)
library(zip)
library(xgboost)
library(corrplot)
library(forcats)
#library(pdp)
library(e1071)
library(jsonlite)
library(lubridate)

#library(rjson)
#library(caret)

#working_folder = 'C:/Dev/Kaggle/'
working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')


source(file.path(working_folder, '/Utils/common.R'))
```

## Loan Data

```{r load_data}

df_filename = file.path(working_folder,'gstore/data/all.rds')
df_agg_filename = file.path(working_folder,'gstore/data/all_agg.rds')


if(file.exists(df_filename)) {
  print(sprintf('reading: %s', df_filename))
  df = readRDS(df_filename)
}else {
  
  df_train = fread(file.path(working_folder,'gstore/data/train.csv'), check.names=T)#, nrows = 10000)
  df_test  = fread(file.path(working_folder,'gstore/data/test.csv'),  check.names=T)#, nrows = 10000)
  
  df_train[,is_train:=T ]
  df_test[, is_train:=F ]
  
  df = rbind(df_train, df_test)
  train_index = df$is_train
  
  #json column:fields 
  # device: browser, operatingSystem, isMobile, deviceCategory
  # geoNetwork: continent, subContinent, country, region, metro, networkDomain
  # totals:  visits, hits, pageviews, bounces, newVisits
  # trafficSource
  
  # TODO: try to think of a better way to do this
  parse_json <- function(json_str, fields){
    values = fromJSON(stri_replace_all_fixed(json_str, '\"\"','\"') )[fields]
    names(values) <-fields
    #values[sapply(values , is.null)] = 'NA'
    values = lapply(values, as.character)
    #print(sapply(values , class))
    return(values)
  }
  parse_json_all <- function(json_str){
    values = fromJSON(stri_replace_all_fixed(json_str, '\"\"','\"') )
    return(values)
  }
  #parse_geo_v <- Vectorize(parse_geo, SIMPLIFY = TRUE)
  
  geo_fields = c('continent', 'subContinent', 'country', 'region', 'metro', 'city', 'networkDomain')
  device_fields = c('browser', 'operatingSystem', 'isMobile', 'deviceCategory')
  trafficSource_fields = c('campaign', 'source', 'medium', 'keyword',
                           'isTrueDirect','adContent',
                           'adwordsClickInfo.page','adwordsClickInfo.slot','adwordsClickInfo.gclId','adwordsClickInfo.adNetworkType','adwordsClickInfo.isVideoAd','referralPath')
  totals_fields = c('visits', 'hits', 'pageviews','bounces', 'newVisits', 'transactionRevenue')
  
  geo_columns           = stri_join('geo_', geo_fields)
  device_columns        = stri_join('device_', device_fields)
  trafficSource_columns = stri_join('trafficSource_', trafficSource_fields)
  totals_columns        = stri_join('totals_', totals_fields)
  
  all_rows = seq_len(nrow(df))
  
  df[, (geo_columns)          := parse_json(geoNetwork, geo_fields),               by = all_rows]
  df[, (device_columns)       := parse_json(device, device_fields),                by = all_rows]
  df[, (trafficSource_columns):= parse_json(trafficSource, trafficSource_fields),  by = all_rows]
  df[, (totals_columns)       := parse_json(totals, totals_fields),                by = all_rows]
  
  df[,geoNetwork:=NULL]
  df[,device:=NULL]
  df[,trafficSource:=NULL]
  df[,totals:=NULL]
  
  bool_columns = c('device_isMobile','trafficSource_isTrueDirect')
  df[, (bool_columns):=lapply(.SD, as.logical), .SDcols = bool_columns]
  
  num_columns = c('totals_hits','totals_pageviews', 'totals_bounces', 'totals_newVisits','totals_transactionRevenue','totals_visits')
  df[, (num_columns):=lapply(.SD, as.numeric), .SDcols = num_columns]

  
  #parse_json(df$device[4],device_fields)
  #parse_json(df$trafficSource[4],trafficSource_fields)
  #parse_json(df$trafficSource[5],trafficSource_fields)
  #df[,geo_columns, with = F]
  #df[,device_columns, with = F]
  #df[,trafficSource_columns, with = F]
  #df[,totals_columns, with = F]
  
  #check forpossible values
  #temp = ldply(sample.int(nrow(df), 10000), function(i) {
  #   res = data.table(t(unlist(parse_json_all(df$trafficSource[i]))))  
  #   return (res)
  #} )
  #str(temp)
  #sapply(temp, function(x) length(unique(x)))
  
  
  #a = df[, .(geoNetwork, geo_continent)]
  #a = df[, .(device, device_isMobile)]
  #a = df[, .(trafficSource, trafficSource_isTrueDirect)]
  #df[, .(trafficSource, trafficSource_isTrueDirect)]
  
  saveRDS(df, df_filename)
}
df[, iso_date := as.Date(as.character(date), format = '%Y%m%d')]
df[, date_month := month(iso_date)]
df[, date_year := year(iso_date)]
df[, date_day := day(iso_date)]
df[, date_wday := weekdays(iso_date)]

#df$totals_transactionRevenue

#table(df[,.(is_train, date_year)])
#table(df[,.(is_train, visitNumber)])

#aggregate by visitor
#agg_visitors <- function(x) {
#    return(list(n = nrow(x), first_date = min(x$iso_date), last_date = max(x$iso_date), revenue = sum(x$totals_transactionRevenue, na.rm = T)))
#  }
#system.time(df[1:10000, agg_visitors(.SD), by = .(fullVisitorId, is_train)])
#system.time(df[1:10000, .(.N, first_date = min(iso_date), last_date = max(iso_date),revenue = sum(totals_transactionRevenue, na.rm = T)), by = .(fullVisitorId, is_train)])
if(file.exists(df_agg_filename)) {
  df_agg = readRDS(df_agg_filename)
}else {
    
  nth_most_freq <- function(x, nth = 1) {names(sort(table(x, useNA = "ifany"), decreasing=TRUE))[nth]}
  count_unique <- function(x) {length(unique(x))}
  
  df_agg = df[, .(.N, 
                  
                  first_date = min(iso_date), last_date = max(iso_date), 
                  day_diff_avg = mean(as.numeric(diff(sort(iso_date) )), na.rm = T), #this can be easily computed 
                  day_diff_max = max(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  day_diff_min = min(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  day_diff_sd  = sd(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  
                  channelGrouping    = nth_most_freq(channelGrouping),
                  channelGrouping_sec = nth_most_freq(channelGrouping, 2),
                  channelGrouping_n   = count_unique(channelGrouping),
                  
                  geo_continent      = nth_most_freq(geo_continent),
                  geo_continent_sec  = nth_most_freq(geo_continent, 2),
                  geo_continent_n    = count_unique(geo_continent),
                  
                  geo_subContinent      = nth_most_freq(geo_subContinent),
                  geo_subContinent_sec  = nth_most_freq(geo_subContinent, 2), 
                  geo_subContinent_n    = count_unique(geo_subContinent),
                  
                  geo_country     = nth_most_freq(geo_country),
                  geo_country_sec = nth_most_freq(geo_country,2), 
                  geo_country_n   = count_unique(geo_country),
                  
                  geo_region     = nth_most_freq(geo_region),
                  geo_region_sec = nth_most_freq(geo_region, 2), 
                  geo_region_n   = count_unique(geo_region),
                  
                  geo_city      = nth_most_freq(geo_city), 
                  geo_city_sec  = nth_most_freq(geo_city, 2),
                  geo_city_n    = count_unique(geo_city),
                  
                  date_wday1 = nth_most_freq(date_wday, 1),
                  date_wday2 = nth_most_freq(date_wday, 2),
                  date_wday3 = nth_most_freq(date_wday, 3),
                  
                  date_mon_pct = sum(date_wday == 'Monday', na.rm = T)/.N,
                  date_tue_pct = sum(date_wday == 'Tuesday', na.rm = T)/.N,
                  #date_wed_pct = sum(date_wday == 'Wednesday', na.rm = T)/.N, #exclude wed, since it is redundant 
                  date_thu_pct = sum(date_wday == 'Thursday', na.rm = T)/.N,
                  date_fri_pct = sum(date_wday == 'Friday', na.rm = T)/.N,
                  date_sat_pct = sum(date_wday == 'Saturday', na.rm = T)/.N,
                  date_sun_pct = sum(date_wday == 'Sunday', na.rm = T)/.N,
                  
                  date_m1_pct = sum(date_month == 1, na.rm = T)/.N,
                  date_m2_pct = sum(date_month == 2, na.rm = T)/.N,
                  date_m3_pct = sum(date_month == 3, na.rm = T)/.N,
                  date_m4_pct = sum(date_month == 4, na.rm = T)/.N,
                  date_m5_pct = sum(date_month == 5, na.rm = T)/.N,
                  date_m6_pct = sum(date_month == 6, na.rm = T)/.N,
                  date_m7_pct = sum(date_month == 7, na.rm = T)/.N,
                  date_m8_pct = sum(date_month == 8, na.rm = T)/.N,
                  date_m9_pct = sum(date_month == 9, na.rm = T)/.N,
                  date_m10_pct = sum(date_month == 10, na.rm = T)/.N,
                  date_m11_pct = sum(date_month == 11, na.rm = T)/.N,
                  date_m12_pct = sum(date_month == 12, na.rm = T)/.N,
                  
                  device_operatingSystem     = nth_most_freq(device_operatingSystem),
                  device_operatingSystem_sec = nth_most_freq(device_operatingSystem, 2),
                  device_operatingSystem_n   = count_unique(device_operatingSystem),
                  
                  trafficSource_source     = nth_most_freq(trafficSource_source),
                  trafficSource_source_sec = nth_most_freq(trafficSource_source, 2),
                  trafficSource_source_n   = count_unique(trafficSource_source),
                  
                  geo_networkDomain     = nth_most_freq(geo_networkDomain),
                  geo_networkDomain_sec = nth_most_freq(geo_networkDomain, 2),
                  geo_networkDomain_n   = count_unique(geo_networkDomain),
                  
                  trafficSource_medium     = nth_most_freq(trafficSource_medium),
                  trafficSource_medium_sec = nth_most_freq(trafficSource_medium, 2),
                  trafficSource_medium_n  = count_unique(trafficSource_medium),
                  
                  device_deviceCategory      = nth_most_freq(device_deviceCategory),
                  device_deviceCategory_sec  = nth_most_freq(device_deviceCategory, 2),
                  
                  desktop_n  = sum(device_deviceCategory == 'desktop', na.rm = T)/.N,
                  mobile_n  = sum(device_deviceCategory == 'mobile', na.rm = T)/.N,
                  tablet_n  = sum(device_deviceCategory == 'tablet', na.rm = T)/.N,
                  
                  device_browser      = nth_most_freq(device_browser),
                  device_browser_sec  = nth_most_freq(device_browser, 2),
                  device_browser_n    = count_unique(device_browser),
                  
                  totals_hits      = sum(totals_hits, na.rm = T)/.N,
                  totals_pageviews = sum(totals_pageviews, na.rm = T)/.N,
                  totals_bounces   = sum(totals_bounces, na.rm = T)/.N,
                  
                  totals_hits_sd      = sd(totals_hits, na.rm = T),
                  totals_pageviews_sd = sd(totals_pageviews, na.rm = T),
                  
                  totals_hits_max      = max(totals_hits, na.rm = T),
                  totals_pageviews_max = max(totals_pageviews, na.rm = T),
                  
                  revenue = log(1  + sum(totals_transactionRevenue, na.rm = T))),
              , by = .(fullVisitorId, is_train)]
  
  char_columns = names(df_agg)[which(lapply(df_agg, is.character) ==T)] %!in_set% c('fullVisitorId')
  df_agg[, (char_columns):=lapply(.SD, as.factor), .SDcols = char_columns]
  
  #df_agg[1:10, ..char_columns]
  
  saveRDS(df_agg, df_agg_filename)
}
tindex = df_agg$is_train

df_agg[, total_days:= as.numeric(difftime(last_date, first_date, units = 'days'))]
df_agg[, last_iso_date:= as.Date(ISOdate(year(last_date), month(last_date), 1)) ]
df_agg[, first_iso_date:= as.Date(ISOdate(year(first_date), month(first_date), 1)) ]
df_agg[, last_month:=  month(last_date) ]
df_agg[, first_month:= month(first_date) ]
df_agg[, first_week_date := factor(weekdays(first_date)) ]
df_agg[, date_wnd_pct := date_sun_pct + date_sat_pct]

date_pct_names = names(df_agg)[grep('date_m[0-9]+_pct$', names(df_agg))]
date_pct_pos_names = c(stri_join(date_pct_names, '_pos'), 'totals_bounces_pos')

df_agg[, (date_pct_pos_names):=lapply(.SD, function(x) as.numeric(x>0)), .SDcols = date_pct_names]

#correct missing variables
df_agg[is.infinite(totals_pageviews_max), totals_pageviews_max:= 0]
df_agg[is.infinite(day_diff_max),         day_diff_max:= NA]
df_agg[is.infinite(day_diff_min),         day_diff_min:= NA]


df_agg[, total_weeks:= as.numeric(difftime(last_date, first_date, units = 'weeks'))]
df_agg[, totals_pageviews_log:= log(totals_pageviews + 1)]
df_agg[, totals_pageviews_sd_log:= log(totals_pageviews_sd + 1)]
df_agg[, totals_pageviews_max_log:= log(totals_pageviews_max + 1) - totals_pageviews_log]

df_agg[, totals_hits_log:= log(totals_hits + 1)]
df_agg[, totals_hits_max_log:= log(totals_hits_max + 1)-totals_hits_log]
df_agg[, totals_hits_sd_log:= log(totals_hits_sd + 1)]

df_agg[, count_log:= log(N)]
df_agg[, day_diff_sd_log:= log(day_diff_sd + 1)]
df_agg[!is.finite(day_diff_avg), day_diff_avg:=NA]
#df_agg[, totals_hits_log:= log(totals_hits + 1)]

df_agg[, is_revenue := as.numeric(revenue>0)]

combine_levels <- function(x, n = 3000){
  counts = table(x)
  keep_levels = names(counts)[which(counts > n)]
  return( fct_other(x,keep = keep_levels) )
}

lo_pd_names = c('Lake Oswego', 'Salem','Fremont','Quebec City','Milpitas')
hi_pd_names = c('Maracaibo','Ann Arbor','San Bruno','Kirkland','Boulder','Irvine','Cambridge','Portland','Nashville')

df_agg[, geo_city_set1:=fct_collapse(geo_city,  lo_pd = lo_pd_names, 
                                                hi_pd = hi_pd_names,
                                                other = levels(geo_city) %!in_set% c(lo_pd_names, hi_pd_names) )]

df_agg[, geo_city_set1:=fct_other(geo_city, keep = c('Atlanta', 'Chicago', 'Mountain View','New York','Sunnyvale'))]

df_agg[, geo_city_major:=combine_levels(geo_city)]
df_agg[, geo_country_major:=combine_levels(geo_country)]
df_agg[, geo_region_major:=combine_levels(geo_region)]
df_agg[, geo_networkDomain_major:=combine_levels(geo_networkDomain)]
df_agg[, trafficSource_source_major:=combine_levels(trafficSource_source)]
df_agg[, device_browser_major:=combine_levels(device_browser)]
df_agg[, device_browser_main:=combine_levels(device_browser, 40000)]
df_agg[, device_operatingSystem_major:=combine_levels(device_operatingSystem)]
df_agg[, device_operatingSystem_main:=combine_levels(device_operatingSystem, 10000)]
df_agg[, channelGrouping_main:=fct_other(channelGrouping, c('Direct', 'Display', 'Organic Search', 'Paid Search', 'Referral'))]

df_agg[, c("geo_networkDomain_level1", "geo_networkDomain_level2", "geo_networkDomain_level3") := tstrsplit(as.character(geo_networkDomain), ".", fixed=TRUE)]

df_agg[, geo_networkDomain_levels:= str_count(as.character(geo_networkDomain), fixed("."))]

char_columns = names(which(lapply(df_agg, is.character) ==T)) %!in_set% c('fullVisitorId')
df_agg[, (char_columns):=lapply(.SD, as.factor), .SDcols = char_columns]

df_agg[, geo_networkDomain_level1_major:=combine_levels(geo_networkDomain_level1)]
df_agg[, geo_networkDomain_level2_major:=combine_levels(geo_networkDomain_level2)]
df_agg[, geo_networkDomain_level3_major:=combine_levels(geo_networkDomain_level3)]
df_agg[, geo_networkDomain_is_edu:=as.numeric(geo_networkDomain_level2 == 'edu')]
df_agg[, geo_networkDomain_is_cantv:=as.numeric(geo_networkDomain_level1 == 'cantv')]

df_agg[, geo_subContinent_is_SoutheastAsia:=as.numeric(geo_subContinent == 'Southeast Asia')]
df_agg[, geo_subContinent_is_EasternAsia:=as.numeric(geo_subContinent == 'Eastern Asia')]
df_agg[, geo_subContinent_is_caribbean:=as.numeric(geo_subContinent == 'Caribbean')]
df_agg[, geo_country_is_puertorico:=as.numeric(geo_country == 'Puerto Rico')]

max_levels = 100 #gbm can handle up to 1024
var_names = names(which(lapply(df_agg, function(x) length(levels(x))>max_levels)==TRUE)) %!in_set% c('fullVisitorId')

#df_agg[,geo_city_top100    :=fct_lump(geo_city, 100)]
#df_agg[,geo_country_top100 :=fct_lump(geo_country, 100)]
#df_agg[,geo_region_top100  :=fct_lump(geo_region, 100)]
#df_agg[,geo_networkDomain_top100  :=fct_lump(geo_networkDomain, 100)]
#df_agg[,trafficSource_source_top100  :=fct_lump(trafficSource_source, 100)]

var_names_top = stri_join(var_names, sprintf('_top%d',max_levels))

df_agg[, (var_names_top):=lapply(.SD, function(x) fct_lump(x, max_levels)), .SDcols = var_names]


#which(sapply(df_agg, function(x) length(levels(x))) > 50)



table(df_agg[,.(geo_city_major)])
table(df_agg[,.(geo_country_major)])
table(df_agg[,.(geo_region_major)])
table(df_agg[,.(geo_networkDomain_major)])
table(df_agg[,.(trafficSource_source_major)])
table(df_agg[,.(device_browser_major)])
table(df_agg[,.(device_operatingSystem_major)])
table(df_agg[,.(geo_networkDomain_level1, geo_networkDomain_is_cantv)])

#df[fullVisitorId =='1131660440785968503',log(1+sum(totals_transactionRevenue, na.rm = T)), by =  .(fullVisitorId, is_train) ]
```


## View Data

```{r view_data}

ggplot(df_agg, aes(totals_pageviews_max_log, group = is_train, fill = is_train)) + stat_ecdf()
ggplot(df_agg, aes(totals_hits_log, group = is_train, fill = is_train)) + stat_ecdf()

#cor(df_agg[,.(totals_pageviews_max_log-totals_pageviews_log, totals_pageviews_log)], use = 'pairwise.complete.obs')
```

## Direct Revenue Model

```{r direct_model, eval = FALSE}

actual = df_agg$revenue

#only keep several car_11 levels

all_vars = names(df_agg) %!in_set% c('fullVisitorId','is_train','first_date','last_date','revenue', #id and outcome var
                                     'geo_city','geo_country','geo_region', 'geo_networkDomain','trafficSource_source', #too many levels
                                     'total_days','last_iso_date','first_iso_date', 'last_month', 'first_month',
                                     'totals_pageviews','totals_pageviews_max', 'totals_pageviews_sd', 'totals_hits', 'totals_hits_max', 'N','day_diff_sd') #replaced with logs

exclude_vars = c()

set.seed(1012356)

formula.gbm = formula(stri_join( 'revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('revenue')
var.monotone = rep(0, length(model_vars))

#df_agg[1:10, ..model_vars]
#str(df_agg[, ..model_vars])

#num_vars  = model_vars %in_set% names(which(sapply(df_agg, is.numeric)))
#corr_matrix = cor(df_agg[, ..num_vars ], use="complete.obs")
#corrplot(corr_matrix, method="number", number.cex = 0.5)
#corrplot(corr_matrix, method="circle", order="hclust")


mon_inc_vars = c('totals_pageviews_log')
mon_dec_vars = c()

var.monotone[model_vars %in% mon_inc_vars]  =  1
var.monotone[model_vars %in% mon_dec_vars]  = -1

cv_folds = 0
max_it = 2000
#0.49
model.gbm  = gbm(formula.gbm,
                 distribution = "gaussian",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=4,
                 train.fraction = 0.7,
                 bag.fraction = 0.8,# 0.5 for small samples, 0.7 for large
                 n.cores = 4,
                 var.monotone = var.monotone,
                 data = df_agg[tindex , all.vars(formula.gbm), with = F],
                 verbose = FALSE)

#saveRDS(model.gbm, file.path(working_folder,'gstore/model.rds'))
#model.gbm = readRDS(file.path(working_folder,'gstore/model.rds'))

plot_gbmiterations(model.gbm)

best_it.gbm = gbm.perf(model.gbm, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model.gbm, plot.it = F))

pred.gbm  = predict(model.gbm, n.trees = best_it.gbm, newdata = df_agg)

summary(lm('actual ~ model ', data.frame(actual = actual[tindex], model = pred.gbm[tindex]))) #0.3951
ggplot(data.frame(actual = actual[tindex], model = pred.gbm[tindex]), aes(model, actual)) + geom_point()

#influence
var_inf = summary(model.gbm, n.trees = best_it.gbm, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.5])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model.gbm, best_it.gbm, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)

plots = llply(imp_vars, function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

#all vars
plots = llply(all.vars(formula.gbm), function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$last_date[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$first_date[tindex]), error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$last_iso_date[tindex], error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],df_agg$first_iso_date[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$first_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$last_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],month(df_agg$last_date)[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$total_weeks[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$date_mon_pct[tindex], error_band = 'normal')

```

## Two Step Model 
first predict that transaction probability is non-zero, then predict value

### Logistic PD model 



                                                          var     rel.inf
totals_pageviews_sd_log               totals_pageviews_sd_log 33.55155899
totals_pageviews_log                     totals_pageviews_log 27.16983868
geo_country_major                           geo_country_major 13.79550794
day_diff_sd_log                               day_diff_sd_log  6.47699450
count_log                                           count_log  4.54625106
geo_city_major                                 geo_city_major  4.44316311
totals_hits_sd_log                         totals_hits_sd_log  3.34331848
device_deviceCategory                   device_deviceCategory  2.23236681
trafficSource_medium                     trafficSource_medium  1.90490694
totals_pageviews_max_log             totals_pageviews_max_log  0.95216231
geo_city_set1                                   geo_city_set1  0.63299029
geo_networkDomain_level2_major geo_networkDomain_level2_major  0.60214298
date_wday1                                         date_wday1  0.30179808
device_browser_major                     device_browser_major  0.04699984


totals_pageviews_max                     totals_pageviews_max 55.3357005
geo_country_major                           geo_country_major 13.3955991
geo_city_major                                 geo_city_major  3.7948043
device_operatingSystem                 device_operatingSystem  3.0960613
trafficSource_source_major         trafficSource_source_major  2.9998297
totals_hits_sd                                 totals_hits_sd  2.3500221
geo_subContinent                             geo_subContinent  2.1978275
totals_pageviews                             totals_pageviews  2.1848407
geo_region_major                             geo_region_major  1.4177066
totals_pageviews_sd                       totals_pageviews_sd  1.3202036
day_diff_sd                                       day_diff_sd  1.1492822
geo_networkDomain_major               geo_networkDomain_major  0.9458780
totals_hits                                       totals_hits  0.9354676
totals_hits_max                               totals_hits_max  0.8736529
count_log                                           count_log  0.7652445
first_month                                       first_month  0.6108520
device_operatingSystem_major     device_operatingSystem_major  0.4045553
date_wday3                                         date_wday3  0.3818714
geo_networkDomain_level1_major geo_networkDomain_level1_major  0.3661544
date_wday2                                         date_wday2  0.3570869
totals_hits_max_log                       totals_hits_max_log  0.3430342
geo_networkDomain_level2_major geo_networkDomain_level2_major  0.3075023
totals_bounces                                 totals_bounces  0.3027080
date_m9_pct                                       date_m9_pct  0.3010800
last_month                                         last_month  0.2894253
date_m5_pct                                       date_m5_pct  0.2721678
day_diff_avg                                     day_diff_avg  0.2273954
totals_pageviews_max_log             totals_pageviews_max_log  0.2207763
day_diff_min                                     day_diff_min  0.2205164
date_wday1                                         date_wday1  0.2193923
date_m12_pct                                     date_m12_pct  0.1697426
date_m8_pct                                       date_m8_pct  0.1517010
channelGrouping_sec                       channelGrouping_sec  0.1474055
day_diff_max                                     day_diff_max  0.1423824

```{r pd_predict_model}
actual.pd = df_agg$is_revenue

#only keep several car_11 levels
exclude_vars = c('fullVisitorId','is_train','first_date','last_date','last_iso_date','first_iso_date','revenue', 'is_revenue','device_deviceCategory_sec', 'N') #id and outcome var #replaced with logs

too_many_levels = names(which(sapply(df_agg, function(x) length(levels(x))) >= 100)) #technically max is 1024 

#all_vars = names(df_agg) %!in_set% exclude_vars
all_vars = c('totals_pageviews_sd_log','totals_pageviews_log','count_log','totals_pageviews_max_log','device_deviceCategory','device_browser_major','geo_country_major','geo_city_major',
             'date_wday1','trafficSource_medium','trafficSource_medium_sec', 'totals_hits_sd_log','day_diff_sd_log','day_diff_min','day_diff_avg','geo_city_set1','geo_networkDomain_level2_major', 'device_operatingSystem_n','mobile_n',
             'date_m1_pct','date_m2_pct','date_m3_pct','date_m4_pct','date_m5_pct','date_m6_pct','date_m7_pct', 'date_m8_pct','date_m9_pct', 'date_m10_pct','date_m11_pct', 'date_m12_pct',
             'geo_subContinent_is_caribbean','geo_networkDomain_is_cantv', 'tablet_n','desktop_n','totals_bounces_pos','geo_country_n','geo_country_is_puertorico',
             'channelGrouping_sec','geo_continent_sec','geo_networkDomain_level3_major')

all_vars = names(df_agg) %!in_set% c(exclude_vars, too_many_levels)


set.seed(1012356)

#formula.pd = formula(stri_join( 'is_revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))
formula.pd = formula(stri_join( 'is_revenue ~ ', stri_join(unique(all_vars), collapse = ' + ')))

model_vars = all.vars(formula.pd) %!in_set% c('is_revenue')
var.monotone = rep(0, length(model_vars))

#df_agg[1:10, ..model_vars]
#str(df_agg[, ..model_vars])

#num_vars  = model_vars %in_set% names(which(sapply(df_agg, is.numeric)))
#corr_matrix = cor(df_agg[, ..num_vars ], use="complete.obs")
#corrplot(corr_matrix, method="number", number.cex = 0.5)
#corrplot(corr_matrix, method="circle", order="hclust")

mon_inc_vars = c('totals_pageviews_log','count_log','totals_pageviews_sd_log','totals_pageviews_max_log','day_diff_sd_log') #totals_pageviews_max_log
mon_dec_vars = c('totals_hits_log','totals_hits_sd_log','totals_hits_max_log')

#var.monotone[model_vars %in% mon_inc_vars]  =  1
#var.monotone[model_vars %in% mon_dec_vars]  = -1

# day_diff_sd_log is quiestionable

cv_folds = 4
max_it = 3000

model.pd  = gbm(formula.pd,
                 distribution = "bernoulli",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=6,
                 train.fraction = 1.0,
                 bag.fraction = 0.9,# 0.5 for small samples, 0.7 for large
                 n.cores = 2,
                 var.monotone = var.monotone,
                 data = df_agg[tindex , all.vars(formula.pd), with = F],
                 verbose = FALSE)

saveRDS(model.pd, file.path(working_folder,'gstore/model_pd.rds'))
#model.pd = readRDS(file.path(working_folder,'gstore/model_pd.rds'))

plot_gbmiterations(model.pd) #0.03795, AUC

best_it.pd = gbm.perf(model.pd, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model.pd, plot.it = F))

pred.pd  = predict(model.pd, n.trees = best_it.pd, newdata = df_agg, type = 'response')
plot_binmodel_roc(actual.pd[tindex], pred.pd[tindex])
plot_binmodel_cdf(actual.pd[tindex], pred.pd[tindex])
plot_binmodel_percentiles(actual.pd[tindex], pred.pd[tindex], 100)
gbm.roc.area(actual.pd[tindex], pred.pd[tindex]) #0.9912461

#summary(glm('actual.pd ~ model ', data = data.frame(actual.pd = actual.pd[tindex], model = pred.gbm[tindex]), family = binomial(link = "logit"))) 

#influence
var_inf = summary(model.pd, n.trees = best_it.pd, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.1])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model.pd, best_it.pd, imp_vars, output_type = 'response')
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)

plots = llply(all.vars(formula.pd), function(var_name) {
  p = plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg[[var_name]][tindex], error_band = 'binom') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

#all vars
plots = llply(names(df_agg) %!in_set% c('fullVisitorId', all.vars(formula.pd)), function(var_name) {
  #print(var_name)
  p = plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg[[var_name]][tindex], error_band = 'binom') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 6, ncol = 6, top = NULL)

plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$totals_bounces[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$date_m8_pct[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$date_m4_pct_pos[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$date_m5_pct_pos[tindex], error_band = 'binom')

plot_profile(pred.pd[tindex], actual.pd[tindex],fct_reorder(df_agg$geo_country[tindex],actual.pd[tindex], sum), error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],fct_reorder(df_agg$geo_city_major[tindex],actual.pd[tindex], mean), error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],fct_reorder(df_agg$geo_city_major[tindex],actual.pd[tindex], sum), error_band = 'binom')

plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_country[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_country_major[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_city_major[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_city[tindex], error_band = 'binom')

plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$first_month[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$last_month[tindex], error_band = 'binom')

plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_networkDomain_major[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],factor(df_agg$geo_networkDomain_levels[tindex]), error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_networkDomain_level1_major[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_networkDomain_level2_major[tindex], error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg$geo_networkDomain_level3_major[tindex], error_band = 'binom')

plot_profile(pred.pd[tindex], actual.pd[tindex],factor(df_agg$last_month[tindex]), error_band = 'binom')
plot_profile(pred.pd[tindex], actual.pd[tindex],factor(df_agg$first_month[tindex]), error_band = 'binom')


plots = llply(names(df_agg)[grep('date_m', names(df_agg))], function(var_name) {
  p = plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg[[var_name]][tindex], error_band = 'binom', bucket_count = 100) +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)


#temp = df_agg[tindex, .(geo_networkDomain)]
#temp[, actual:=actual.pd[tindex]]
#temp[, model:=pred.pd[tindex]]
#temp[,.(.N, mean(actual - model)), by =.(geo_networkDomain)]
tstrsplit(as.character(b$geo_networkDomain[1:2]), '.', fixed = T)
b[, c("geo_networkDomain_level1", "geo_networkDomain_level2") := tstrsplit(as.character(geo_networkDomain), ".", fixed=TRUE)]

plots = llply(names(df_agg)[grep('day', names(df_agg))], function(var_name) {
  p = plot_profile(pred.pd[tindex], actual.pd[tindex],df_agg[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 3, ncol = 3, top = NULL)
#cor(df_agg[tindex, .(day_diff_min, day_diff_max, day_diff_avg, day_diff_sd,day_diff_sd_log)], use = 'pairwise.complete.obs')

```


### Revenue Model,  rgs - revenue given sale 

cv-10: 1.10421

                                                                var    rel.inf
totals_hits_max                                     totals_hits_max 25.197897401
N                                                                 N  8.549090452
device_operatingSystem                       device_operatingSystem  7.244946000
trafficSource_source_top50               trafficSource_source_top50  4.375488531
day_diff_sd                                             day_diff_sd  3.962424938
geo_city_major                                       geo_city_major  3.887315713
totals_hits_sd                                       totals_hits_sd  3.760915809
geo_city_top50                                       geo_city_top50  3.512627095
totals_hits                                             totals_hits  2.932657660
date_wnd_pct                                           date_wnd_pct  2.695979138
geo_networkDomain_top50                     geo_networkDomain_top50  2.263658274
geo_region_top50                                   geo_region_top50  1.786324721
totals_pageviews                                   totals_pageviews  1.723654635
date_wday2                                               date_wday2  1.637725823
geo_country_major                                 geo_country_major  1.602141039
totals_pageviews_max_log                   totals_pageviews_max_log  1.580600510
device_browser_major                           device_browser_major  1.480776623
totals_pageviews_sd                             totals_pageviews_sd  1.335286969
day_diff_max                                           day_diff_max  1.297624869
total_days                                               total_days  1.287413405
channelGrouping                                     channelGrouping  1.253171645
date_wday3                                               date_wday3  1.187169509
geo_networkDomain_major                     geo_networkDomain_major  1.136758756
```{r revenue_predict_model, eval = FALSE}

actual.rgs = df_agg$revenue

tindex_rgs = tindex & df_agg$is_revenue

exclude_vars = c('fullVisitorId','is_train','first_date','last_date','last_iso_date','first_iso_date','revenue', 'is_revenue','device_deviceCategory_sec', 'N') #id and outcome var #replaced with logs

too_many_levels = names(which(sapply(df_agg, function(x) length(levels(x))) >= 100)) #technically max is 1024 

all_vars = names(df_agg) %!in_set% c(exclude_vars, too_many_levels)

set.seed(1012356)

rgs_vars = c('count_log','totals_hits_sd','totals_hits_log', 'date_wnd_pct','device_operatingSystem_main','device_deviceCategory','channelGrouping_main', 'device_browser_main', 'geo_networkDomain_is_edu','geo_networkDomain_is_cantv', 'date_m3_pct_pos', 'date_m4_pct_pos','geo_subContinent_is_SoutheastAsia','geo_subContinent_is_EasternAsia','geo_city_set1','totals_bounces')
rgs_vars = all_vars # use all vars

#formula.rgs = formula(stri_join( 'revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))
formula.rgs = formula(stri_join( 'revenue ~ ', stri_join(unique(rgs_vars),collapse = ' + ')))

model_vars = all.vars(formula.rgs) %!in_set% c('revenue')
var.monotone = rep(0, length(model_vars))

mon_inc_vars = c('count_log','totals_hits_log','totals_bounces')
mon_dec_vars = c('date_wnd_pct','totals_hits_sd')

#var.monotone[model_vars %in% mon_inc_vars]  =  1
#var.monotone[model_vars %in% mon_dec_vars]  = -1

cv_folds = 10
max_it = 2000
#0.49
model.rgs  = gbm(formula.rgs,
                 distribution = "gaussian",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=7,
                 train.fraction = 1.0,
                 bag.fraction = 0.8,# 0.5 for small samples, 0.7 for large
                 n.cores = 4,
                 var.monotone = var.monotone,
                 data = df_agg[tindex_rgs , all.vars(formula.rgs), with = F],
                 verbose = FALSE)

#saveRDS(model.rgs, file.path(working_folder,'gstore/model_rgs.rds'))
#model.rgs = readRDS(file.path(working_folder,'gstore/model_rgs.rds'))

plot_gbmiterations(model.rgs) # 1.09765

best_it.rgs = gbm.perf(model.rgs, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model.rgs, plot.it = F))

#influence
var_inf = summary(model.rgs, n.trees = best_it.rgs, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.5])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model.rgs, best_it.rgs, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow = 3, ncol = 3, top = NULL)

#var_interaction = gbm_interactions(model.rgs, df_agg[tindex_rgs,], iter = best_it.rgs, min_influence = 1, degree = 2) 
#plot_gbminteractions(subset(var_interaction, interaction_score>0.1))
#print(var_interaction)

#plots = plot_gbmpartial_2d(model.rgs, best_it.rgs, as.character(subset(var_interaction,interaction_score>0.1)$vars), output_type = 'link')
#marrangeGrob(plots, nrow = 2, ncol = 2, top = NULL)


pred.rgs = predict(model.rgs, n.trees = best_it.rgs, newdata = df_agg)
pred.rev = pred.rgs * pred.pd
summary(lm('actual ~ model', data.frame(actual = df_agg$revenue[tindex],     model = pred.rev[tindex]))) #0.4343
summary(lm('actual ~ model', data.frame(actual = df_agg$revenue[tindex_rgs], model = pred.rgs[tindex_rgs]))) #0.316
ggplot(data.frame(actual = df_agg$revenue[tindex_rgs], model = pred.rgs[tindex_rgs]), aes(model, actual)) + geom_point() + geom_abline(slope = 1, color = 'red')

#all vars
plots = llply(all.vars(formula.rgs), function(var_name) {
  p = plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs], df_agg[[var_name]][tindex_rgs], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

#candidate vars
plots = llply(names(df_agg) %!in_set% c('fullVisitorId'), function(var_name) {
  p = plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs],df_agg[[var_name]][tindex_rgs], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 6, ncol = 7, top = NULL)

#candidate vars
plots = llply(names(df_agg) %!in_set% c('fullVisitorId'), function(var_name) {
  p = plot_profile(pred.rev[tindex],df_agg$revenue[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 6, ncol = 7, top = NULL)


plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs],df_agg$geo_city[tindex_rgs], error_band = 'normal')
plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs],df_agg$geo_networkDomain_level1[tindex_rgs], error_band = 'normal')
plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs],df_agg$device_browser_major[tindex_rgs], error_band = 'normal')
plot_profile(pred.rgs[tindex_rgs], actual.rgs [tindex_rgs],df_agg$device_operatingSystem_major[tindex_rgs], error_band = 'normal')

plot_profile(pred.rgs[tindex_rgs], df_agg$revenue[tindex_rgs],df_agg$totals_bounces[tindex_rgs], error_band = 'normal', bucket_count = 20)

#revenue
plot_profile(pred.rev[tindex], df_agg$revenue[tindex],df_agg$device_operatingSystem_major[tindex], error_band = 'normal')


table(df_agg[is_revenue == T, .(is_train, first_month)])
```

## Save Results
1.4476 - best
1.4483 - latest

```{r save_results}

test_index = df_agg$is_train == FALSE

min(pred.rev[test_index])

submit = data.table(fullVisitorId = df_agg$fullVisitorId[test_index], PredictedLogRevenue = pmax(0, pred.rev[test_index]))
  
file = file.path(working_folder, "gstore/solution.csv")
  
fwrite(submit, file = file, row.names = FALSE)

zip(paste(file, '.zip', sep = ''), file)
  
print(file)

#fullVisitorId,PredictedLogRevenue

```