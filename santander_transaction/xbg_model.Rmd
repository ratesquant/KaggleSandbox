---
title: "Santander Transaction Model"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(plyr)

library(data.table)
library(stringi)
library(ggplot2)
library(tabplot)
library(gridExtra)
library(zip)
library(corrplot)
library(forcats)
#library(pdp)
library(e1071)
library(lubridate)

library(gbm)
#library(randomForestSRC)
library(xgboost)
library(pdp) 
library(vip) 
library(car)

library(rBayesianOptimization)

#library(lightgbm)

#working_folder = 'C:/Dev/Kaggle/'
working_folder = 'F:/Github/KaggleSandbox/'
#working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')

source(file.path(working_folder, '/Utils/common.R'))
```

## Load Data

```{r load_data}

  df_train = fread(file.path(working_folder,'santander_transaction/data/train.csv'), check.names=T)#, nrows = 10000)
  df_test  = fread(file.path(working_folder,'santander_transaction/data/test.csv'),  check.names=T)#, nrows = 10000)

  df_train[,is_train:=TRUE  ]
  df_test[, is_train:=FALSE ]
  df_test[, target:=NA ]
  
  df = rbind(df_train, df_test)
  df = df[sample.int(nrow(df), nrow(df)),]
  train_index = df$is_train
  
  df_test = NULL
  
  gc(reset = TRUE)
  
  #normalize data
  var_names = names(df)[grep('var', names(df))]
  df[, (var_names):=lapply(.SD, function(x) {ecdf(x)(x) - 0.5/length(x)} ), .SDcols = var_names]
  df[, (var_names):=lapply(.SD, qnorm), .SDcols = var_names] #convert to normal

```

## Plot data

```{r plot_data}
  

  ggplot(df, aes(var_89, group = is_train, color = is_train)) + stat_ecdf()

  ggplot(df[var_89>12,], aes(var_139, group = is_train, color = is_train)) + stat_ecdf()
  ggplot(df[1:1000,], aes(sample=var_89, group = is_train, color = is_train)) + stat_qq()
  
  ggplot(df_train[1:10000,], aes(var_81, var_139, color = factor(target) )) + geom_point(alpha = 0.2)
  
    #ggplot(df_train[1:30000,], aes(var_80, var_139)) + geom_point(alpha = 0.2) + geom_rug(alpha = 0.01) + facet_wrap(~target)
  ggplot(df_train, aes(var_80, var_139)) + geom_bin2d() + facet_wrap(~target) + scale_fill_custom('jet', discrete = FALSE)
  ggplot(df_train, aes(var_81, var_139, z = target)) + stat_summary_2d(fun = function(x) mean(x)) + scale_fill_custom('jet', discrete = FALSE)
  #ggplot(df_train, aes(var_81, var_139, z = target)) + stat_summary_hex(fun = function(x) mean(x)) +  scale_fill_custom('jet', discrete = FALSE)
  
  imp_vars = c('var_81','var_53','var_164','var_109','var_139','var_166','var_78','var_12','var_177','var_80','var_110','var_26','var_6')
  all_combinations = t(combn(imp_vars, 2))
  plots = llply(seq(nrow(all_combinations)), function(i){
    ggplot(df_train, aes_string(all_combinations[i,1], all_combinations[i,2])) + geom_bin2d() + facet_wrap(~target) + scale_fill_custom('jet', discrete = FALSE) + theme(legend.position = 'none')
  })
  marrangeGrob(plots, nrow = 5, ncol = 9, top = NULL)
  
  plots = llply(seq(nrow(all_combinations)), function(i){
    ggplot(df_train, aes_string(all_combinations[i,1], all_combinations[i,2], z = 'target')) + stat_summary_2d(fun = function(x) ifelse(length(x)<8, NA, mean(x))) + scale_fill_custom('jet', discrete = FALSE)+ theme(legend.position = 'none')
    #ggplot(df_train, aes_string(all_combinations[i,1], all_combinations[i,2], z = 'target')) + stat_summary_hex(fun = function(x) mean(x)) + scale_fill_custom('jet', discrete = FALSE)+ theme(legend.position = 'none')
  })
  marrangeGrob(plots, nrow = 5, ncol = 9, top = NULL)

  #plot model prediction
  df_train_wm = cbind(df[tindex,], mdl=pred.xgb[tindex], actual=actual[tindex])
  plots = llply(seq(nrow(all_combinations)), function(i){
    ggplot(df_train_wm, aes_string(all_combinations[i,1], all_combinations[i,2], z = 'mdl')) + stat_summary_2d(bins =100, fun = function(x) mean(x)) + scale_fill_custom('jet', discrete = FALSE)+ theme(legend.position = 'none')
    #ggplot(df_train, aes_string(all_combinations[i,1], all_combinations[i,2], z = 'target')) + stat_summary_hex(fun = function(x) mean(x)) + scale_fill_custom('jet', discrete = FALSE)+ theme(legend.position = 'none')
  })
  marrangeGrob(plots, nrow = 5, ncol = 9, top = NULL)

   p1 = ggplot(df_train_wm, aes(var_81, var_139, z = mdl))    + stat_summary_2d(bins =100, fun = function(x) mean(x)) + scale_fill_custom('jet', discrete = FALSE)#+ theme(legend.position = 'none')
   p2 = ggplot(df_train_wm, aes(var_81, var_139, z = target)) + stat_summary_2d(bins =100, fun = function(x) ifelse(length(x)<4, NA, mean(x))) + scale_fill_custom('jet', discrete = FALSE)#+ theme(legend.position = 'none')
   grid.arrange(p1, p2, nrow = 1)
  
    #tableplot(df_train[1:10000,], select = c('target',stri_join('var_', seq(0, 10))), sortCol = 'var_0')
  #tableplot(df_train[1:10000,], select = c('target','var_1'), sortCol = 'var_0')

```


## Logistic regression

```{r logreg_model}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

exclude_vars = c('ID_code', 'is_train', obj_var)
all_vars = names(df) %!in_set% c(exclude_vars)

formula.glm = formula(stri_join( obj_var, ' ~ ', stri_join(unique(all_vars), collapse = ' + ')))

model.glm = glm(formula.glm, df[tindex,], family = binomial(link = "logit"))
summary(model.glm)

#could be dec: var_41, var_7, var_98        var_10       var_185
#could be inc:        var_38       var_117        var_60 

pred.glm          = predict(model.glm, newdata = df, type = 'response')
pred.glm.logodds  = predict(model.glm, newdata = df, type = 'link')
plot_binmodel_roc(actual[tindex], pred.glm[tindex])
plot_binmodel_cdf(actual[tindex], pred.glm[tindex])
plot_binmodel_percentiles(actual[tindex], pred.glm[tindex], 100)
gbm.roc.area(actual[tindex], pred.glm[tindex]) #0.8601852


```

## XGBoost Model

```{r xgboost_model}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

#only keep several car_11 levels
exclude_vars = c('ID_code', 'is_train', obj_var) #replaced with logs
all_vars = names(df) %!in_set% c(exclude_vars)

mon_inc_vars = stri_join('var_', c(110, 53, 26, 22, 6, 99, 2, 78,
                                   164,94,133,190,179,40,0,1,170,184,191,
                                   18,147,173,91,67,118,95,
                                   71, 52, 167, 51, 89, 180, 130, 35, 106, 
                                   157, 128, 32, 119, 162, 49, 195, 90, 175, 24, 82,111,
                                   155,5,145,163,125,151,137,48,135,55,196,105,144,97,
                                   66,8,140,16,176,171,189,46,19,11,15,62,25,181,
                                   112,70,199,47,168,134,187,69,159,61,65,84,74,3,138,4,41,
                                   29,126,161,79,37, 96, 7, 100, 124))
mon_dec_vars = stri_join('var_', c(81, 139, 12, 146, 174, 166, 109, 80, 76, 165, 21, 198,
                                   44, 13, 148, 108,34, 33, 92,
                                   9,169,121,123,115,86,122,188,75,149,127,107,56,
                                   177, 87, 23, 172, 36, 154, 192, 141,43, 131, 186,
                                   193,142,152,129, 197, 93, 150, 83,132,58,114,104, 102,
                                   28,85,31,116,101, 143,64,59,42,178,194,113,50,45,54,72,57,68,
                                   20, 156,63,158,103,120,153,14,160,73,
                                   88,182,39,30,17,136, 77, 183, 27))
var.monotone = rep(0, length(all_vars))
var.monotone[all_vars %in% mon_inc_vars]  =  1
var.monotone[all_vars %in% mon_dec_vars]  = -1

dtrain = data.matrix(df[tindex,all_vars, with = F])
#Round = 98	eta = 0.2366	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 6.0000	Value = 0.8965 
#eta = 0.0387	max_depth = 1.0000	subsample = 0.9067	min_child_weight = 8.0000	gamma = 0.3154	monotone = 1.0000	Value = 0.8974 (12352)

model.xgb <- xgboost(
  max_depth = 1, 
  eta = 0.0387, 
  nthread = 4,
  subsample = 0.9,
  min_child_weight = 8.0,
  gamma = 0.3,
  objective = "binary:logistic",
  eval_metric = "auc",
  monotone_constraints = var.monotone,
  data = dtrain, label = actual[tindex],
  print_every_n = 100,
  nrounds = 13000, #13000
  verbose = 1)

ggplot(model.xgb$evaluation_log, aes(iter, train_auc)) + geom_line()


pred.xgb <- predict(model.xgb, data.matrix(df[,all_vars, with = F]) )

plot_binmodel_roc(actual[tindex], pred.xgb[tindex])
plot_binmodel_cdf(actual[tindex], pred.xgb[tindex])
plot_binmodel_percentiles(actual[tindex], pred.xgb[tindex], 100)
gbm.roc.area(actual[tindex], pred.xgb[tindex]) #0.7474184

importance_matrix <- xgb.importance(model = model.xgb)
print(importance_matrix)
fwrite( importance_matrix, file.path(working_folder,'santander_transaction/var.imp.csv'))

xgb.ggplot.importance(importance_matrix = importance_matrix)
xgb.ggplot.deepness(model.xgb)
xgb.ggplot.deepness(model.xgb, which = '2x1')
xgb.ggplot.deepness(model.xgb, which = 'max.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.depth')
#xgb.plot.shap(as.matrix(df[tindex,all_vars, with = F]), model = model.xgb, top_n = 12, n_col = 4) #takes a very long time time to produce
#xgb.plot.importance(importance_matrix = importance_matrix)

plots = llply(as.character(importance_matrix$Feature)[1:10], function(var_name) {
  p = plot_profile(pred.xgb[tindex], actual[tindex],df[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6), axis.title.y = element_blank()) 
  return( p )
})
#marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

gplots = lapply(plots, ggplotGrob)
ggsave(filename = file.path(working_folder,"santander_transaction/xbg.profiles_d20.pdf"), plot = marrangeGrob(gplots, nrow=5, ncol=6), device = 'pdf', width = 11, height = 8.5, dpi = 240)

#PD plots -------------
pd_plots = llply(as.character(importance_matrix$Feature), function(vname){
  temp = partial(model.xgb, pred.var = vname, train = dtrain, prob = TRUE)
  ggplot(temp, aes_string(vname, 'yhat')) + geom_line()
})
marrangeGrob(pd_plots, nrow = 5, ncol = 5, top = NULL)
gplots = lapply(pd_plots, ggplotGrob)
ggsave(filename = file.path(working_folder,"santander_transaction/xbg.pd.pdf"), plot = marrangeGrob(gplots, nrow=5, ncol=6), device = 'pdf', width = 14, height = 8.5, dpi = 240)

```

## XGBoost Model with offset

```{r xgboost_model}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

#only keep several car_11 levels
exclude_vars = c('ID_code', 'is_train', obj_var) #replaced with logs
all_vars = names(df) %!in_set% c(exclude_vars)

var.monotone = rep(0, length(all_vars))
var.monotone[all_vars %in% mon_inc_vars]  =  1
var.monotone[all_vars %in% mon_dec_vars]  = -1

dtrain = data.matrix(df[tindex,all_vars, with = F])
#Round = 98	eta = 0.2366	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 6.0000	Value = 0.8965 
#eta = 0.0387	max_depth = 1.0000	subsample = 0.9067	min_child_weight = 8.0000	gamma = 0.3154	monotone = 1.0000	Value = 0.8974 (12352)

params = list(max_depth = 1, 
  eta = 0.03, 
  nthread = 4,
  subsample = 0.9,
  min_child_weight = 8.0,
  gamma = 0.3,
  verbose = 1,
  objective = "binary:logistic",
  eval_metric = "auc",
  base_score = mean(actual[tindex]))

dtrain <- xgb.DMatrix(data.matrix(df[tindex,all_vars, with = F]), label =  actual[tindex])
setinfo(dtrain, "base_margin", pred.glm.logodds[tindex])

dtest <- xgb.DMatrix(data.matrix(df[,all_vars, with = F]), label =  actual)
setinfo(dtest, "base_margin", pred.glm.logodds)

model.xgb <- xgb.train(params,
  #monotone_constraints = var.monotone,
  data = dtrain,
  print_every_n = 100,
  early_stopping_rounds = 50,
  nrounds = 13852, 
  watchlist <- list(train = dtrain))

ggplot(model.xgb$evaluation_log, aes(iter, train_auc)) + geom_line()

pred.xgb <- predict(model.xgb,dtest)

plot_binmodel_roc(actual[tindex], pred.xgb[tindex])
plot_binmodel_cdf(actual[tindex], pred.xgb[tindex])
plot_binmodel_percentiles(actual[tindex], pred.xgb[tindex], 100)
gbm.roc.area(actual[tindex], pred.xgb[tindex]) #0.7474184

importance_matrix <- xgb.importance(model = model.xgb)
print(importance_matrix)
fwrite( importance_matrix, file.path(working_folder,'santander_transaction/var.imp.with_margin.csv'))

xgb.ggplot.importance(importance_matrix = importance_matrix)

plots = llply(as.character(importance_matrix$Feature), function(var_name) {
  p = plot_profile(pred.xgb[tindex], actual[tindex],df[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6), axis.title.y = element_blank()) 
  return( p )
})
marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

##  CV -------------------
set.seed(132140937)
  
  xgb_cv <- xgboost::xgb.cv(params,
    data = dtrain,
    nrounds = 20000, 
    nfold = 5,  
    print_every_n = 1000,
    early_stopping_rounds = 50)
  
data.frame(best_it = xgb_cv$best_iteration, xgb_cv$evaluation_log[xgb_cv$best_iteration,]) 


```

## XGBoost Model Params

```{r xgboost_model_params}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

#only keep several car_11 levels
exclude_vars = c('ID_code', 'is_train', obj_var) #replaced with logs
all_vars = names(df) %!in_set% c(exclude_vars)

mon_inc_vars = stri_join('var_', c(110, 53, 26, 22, 6, 99, 2, 78,
                                   164,94,133,190,179,40,0,1,170,184,191,
                                   18,147,173,91,67,118,95,
                                   71, 52, 167, 51, 89, 180, 130, 35, 106, 
                                   157, 128, 32, 119, 162, 49, 195, 90, 175, 24, 82,111,
                                   155,5,145,163,125,151,137,48,135,55,196,105,144,97,
                                   66,8,140,16,176,171,189,46,19,11,15,62,25,181,
                                   112,70,199,47,168,134,187,69,159,61,65,84,74,3,138,4,41,
                                   29,126,161,79,37, 96, 7, 100, 124))
mon_dec_vars = stri_join('var_', c(81, 139, 12, 146, 174, 166, 109, 80, 76, 165, 21, 198,
                                   44, 13, 148, 108,34, 33, 92,
                                   9,169,121,123,115,86,122,188,75,149,127,107,56,
                                   177, 87, 23, 172, 36, 154, 192, 141,43, 131, 186,
                                   193,142,152,129, 197, 93, 150, 83,132,58,114,104, 102,
                                   28,85,31,116,101, 143,64,59,42,178,194,113,50,45,54,72,57,68,
                                   20, 156,63,158,103,120,153,14,160,73,
                                   88,182,39,30,17,136, 77, 183, 27))


var.monotone = rep(0, length(all_vars))
var.monotone[all_vars %in% mon_inc_vars]  =  1
var.monotone[all_vars %in% mon_dec_vars]  = -1

train.fraction = 0.7

dtrain <- xgb.DMatrix(as.matrix(df[tindex,all_vars, with = F]), label = actual[tindex] )

train.sample = sample.int(nrow(dtrain), train.fraction * nrow(dtrain))
eval.sample = seq(nrow(dtrain)) %!in_set% train.sample

#eta = 0.0387	max_depth = 1.0000	subsample = 0.9067	min_child_weight = 8.0000	gamma = 0.3154	monotone = 1.0000	Value = 0.8974 (12352)

param <- list(max_depth = 1, 
              eta = 0.03, 
              nthread = 4,
              subsample = 0.9,
              min_child_weight = 8,
              gamma = 0.3,
              objective = "binary:logistic",
              eval_metric = "auc",
              base_score = mean(actual[tindex]),
              monotone_constraints = var.monotone)

model.xgb <- xgb.train(param, data = dtrain[train.sample,], 
                       watchlist = list(train = dtrain[train.sample,], eval = dtrain[eval.sample,]),
                       nrounds = 13000, 
                       verbose = 1, 
                       print_every_n = 10,
                       early_stopping_rounds = 30)

#a[eval_auc == max(eval_auc),]

ggplot(model.xgb$evaluation_log, aes(iter, train_auc)) + geom_line() + geom_line(aes(iter, eval_auc), color = 'red')

pred.xgb <- predict(model.xgb, data.matrix(df[,all_vars, with = F]), ntreelimit = 3787 )

#plot_binmodel_roc(actual[tindex], pred.xgb[tindex])
#plot_binmodel_cdf(actual[tindex], pred.xgb[tindex])
#plot_binmodel_percentiles(actual[tindex], pred.xgb[tindex], 20)
gbm.roc.area(actual[tindex], pred.xgb[tindex]) #0.7474184

vip(model.xgb, num_features = 30) 

importance_matrix <- xgb.importance(model = model.xgb)
print(importance_matrix)
xgb.ggplot.importance(importance_matrix = importance_matrix)
xgb.ggplot.deepness(model.xgb)
xgb.ggplot.deepness(model.xgb, which = '2x1')
xgb.ggplot.deepness(model.xgb, which = 'max.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.weight')
#xgb.plot.shap(as.matrix(df[tindex,all_vars, with = F]), model = model.xgb, top_n = 12, n_col = 4) #takes a very long time time to produce
#xgb.plot.importance(importance_matrix = importance_matrix)

pd_plots = llply(stri_join('var_',as.character(importance_matrix$Feature))[1:10], function(vname){
  temp = partial(model.xgb, pred.var = vname, train = data.matrix(df[tindex,all_vars, with = F]), prob = TRUE)
  ggplot(temp, aes_string(vname, 'yhat')) + geom_line()
})
marrangeGrob(pd_plots, nrow = 3, ncol = 4, top = NULL)

#partial(model.xgb, pred.var = c("var_1", "var_2"),
#              plot = TRUE, chull = TRUE, plot.engine = "ggplot2", train = x)


plots = llply(stri_join('var_',as.character(importance_matrix$Feature))[1:40], function(var_name) {
  p = plot_profile(pred.xgb[tindex], actual[tindex],df[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6), axis.title.y = element_blank()) 
  return( p )
})
#marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

### CV Grid Search ----------------
my_params = expand.grid(depth =c(1), eta = c(0.02, 0.03), subsample = c(0.9, 0.8), gamma = c(0.2, 0.3), mono =c(TRUE, FALSE))

df_train_set = data.matrix(df[tindex,all_vars, with = F])

param_res = ldply(seq(nrow(my_params)), function(run_index){
  print(my_params[run_index,])
  
  set.seed(132140937)
  
  xgb_cv <- xgboost::xgb.cv(
    data = df_train_set, label = actual[tindex], 
    verbose = 1, objective = "binary:logistic", eval_metric = "auc",
    nrounds = 50000, 
    max_depth = my_params$depth[run_index], 
    subsample = my_params$subsample[run_index],
    eta = my_params$eta[run_index], 
    monotone_constraints = ifelse(rep(my_params$mono[run_index],length(var.monotone)), var.monotone, rep(0, length(var.monotone))),
    gamma = my_params$gamma[run_index], 
    min_child_weight = 8,
    nfold = 5,  
    nthread = 4, 
    print_every_n = 1000,
    early_stopping_rounds = 50)
  gc(reset = TRUE)
  return ( data.frame(best_it = xgb_cv$best_iteration, xgb_cv$evaluation_log[xgb_cv$best_iteration,]) ) 
})
param_res = cbind(param_res, my_params)

ggplot(param_res, aes(depth, test_auc_mean, group = eta, color = factor(eta))) + geom_point() + geom_line() + 
  geom_errorbar(aes(ymin = test_auc_mean - test_auc_std, ymax = test_auc_mean + test_auc_std), alpha = 0.6, width = 0.1) + 
  facet_grid(mono ~ subsample)

ggplot(param_res, aes(depth, train_auc_mean/test_auc_mean, group = eta, color = factor(eta))) + geom_point() + 
  facet_grid(mono ~ subsample)
  

ggplot(param_res, aes(depth,  best_it, group = eta)) + geom_point() + facet_grid(mono ~ subsample)

### Param optimization ----------------
set.seed(132140937)
#Best Parameters Found:  Round = 29	eta = 0.1663	max_depth = 8.0000	subsample = 1.0000	Value = -522.3625 
#Best Parameters Found:  Round = 47	eta = 0.0335	max_depth = 13.0000	subsample = 0.7085	monotone = 0.0000	Value = -518.4975 
#Round = 98	eta = 0.2366	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 6.0000	Value = 0.8965 

xgb_cv_bayes <- function(eta, max_depth, subsample, monotone, min_child_weight) {
  cv <- xgb.cv(params = list(eta = eta,
                             max_depth = max_depth,
                             subsample = subsample, 
                             min_child_weight = min_child_weight,
                             monotone_constraints = ifelse(rep(monotone==1,length(var.monotone)), var.monotone, rep(0, length(var.monotone))),
                             colsample_bytree = 1.0,
                             oobjective = "binary:logistic", eval_metric = "auc"),
               data = df_train_set, label = actual[tindex],
               nround = 2000, #2000
               nfold = 5,
               gamma = 0, 
               early_stopping_rounds = 30,  
               verbose = 0)
  gc(reset = TRUE)
  
  list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration], Pred = cv$best_iteration)
}
sink(file.path(working_folder,'santander_transaction/log.txt'), append=FALSE, split=TRUE)
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
                                bounds = list(
                                  eta = c(0.001, 1.0),
                                  max_depth = c(1L, 10L),
                                  subsample = c(0.5, 1.0),
                                  monotone = c(0L, 1L),
                                  min_child_weight = c(1L, 10L)),
                                init_grid_dt = NULL, 
                                init_points = 10, #10
                                n_iter = 100,     #50
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
sink()

ggplot(OPT_Res$History, aes(eta, Value, size = min_child_weight)) + geom_point(alpha = 0.6) + geom_smooth() + facet_wrap(~monotone)
ggplot(OPT_Res$History, aes(subsample, Value, size = min_child_weight)) + geom_point(alpha = 0.6) + geom_smooth() + facet_wrap(~monotone)
ggplot(OPT_Res$History, aes(max_depth, Value, size = min_child_weight)) + geom_point(alpha = 0.3) + geom_smooth() + facet_wrap(~monotone)

### Param optimization top N features ----------------
#Round = 11	eta = 0.1115	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 10.0000	Value = 0.8970 
#top 10(22%) Round = 2	eta = 0.0533	max_depth = 2.0000	subsample = 0.8258	min_child_weight = 4.0000	Value = 0.7135476 
#top 20(37%) Round = 14	eta = 0.1215	max_depth = 1.0000	subsample = 1.0000	min_child_weight = 6.0000	Value = 0.7708 ()
#top 40(60%) Round = 23	eta = 0.0559	max_depth = 1.0000	subsample = 1.0000	min_child_weight = 8.0000	gamma = 0.1289	Value = 0.8301 
#top 50(60%) Round = 16	eta = 0.0205	max_depth = 1.0000	subsample = 0.9875	min_child_weight = 20.0000	gamma = 0.1031	Value = 0.8473
#           Round = 12	eta = 0.0387	max_depth = 1.0000	subsample = 0.9067	min_child_weight = 8.0000	gamma = 0.3154	monotone = 1.0000	Value = 0.8974 (12352)

importance_matrix = fread( file.path(working_folder,'santander_transaction/gbm.var_inf.d1.csv'))

#plot(cumsum(importance_matrix$rel.inf))

#df_train_set = data.matrix(df[tindex,importance_matrix$var[1:50], with = F])
df_train_set = data.matrix(df[tindex,all_vars, with = F])

xgb_cv_bayes <- function(eta, max_depth, subsample, min_child_weight, gamma,monotone) {
    set.seed(132140937) # to remove random noise from the parameters
  
    cv <- xgb.cv(params = list(eta = eta,
                             max_depth = max_depth,
                             subsample = subsample, 
                             min_child_weight = min_child_weight,
                             gamma = gamma, 
                             monotone_constraints = ifelse(rep(monotone==1,length(var.monotone)), var.monotone, rep(0, length(var.monotone))),
                             colsample_bytree = 1.0,
                             oobjective = "binary:logistic", eval_metric = "auc", nthread = 4),
               data = df_train_set, label = actual[tindex],
               nround = 50000, #2000
               nfold = 5,
               early_stopping_rounds = 50,  
               verbose = 0)
  gc(reset = TRUE)
  
  list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration], Pred = cv$best_iteration)
}
sink(file.path(working_folder,'santander_transaction/log.txt'), append=FALSE, split=TRUE)
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
                                bounds = list(
                                eta = c(0.01, 0.1), #0.1215
                                max_depth = c(1L, 2L), #1
                                subsample = c(0.8, 1.0), #1.0
                                min_child_weight = c(4L, 20L), #6.0000
                                gamma = c(0, 0.5),#0.1289
                                monotone = c(0L, 1L)), 
                                init_grid_dt = NULL, 
                                init_points = 10, #10
                                n_iter = 40,     #50
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
sink()
opt_res = data.table(OPT_Res$History, it_count=as.numeric(OPT_Res$Pred))
#setorder(opt_res, Value)

tableplot(opt_res, sortCol = 'Value')
tableplot(opt_res, sortCol = 'Round')

ggplot(opt_res, aes(eta, Value)) + geom_point(alpha = 0.6) + geom_smooth()
ggplot(opt_res, aes(subsample, Value, size = max_depth, color =min_child_weight)) + geom_point(alpha = 0.6) + geom_smooth()
ggplot(opt_res, aes(max_depth, Value, size = min_child_weight)) + geom_point(alpha = 0.3) + geom_smooth()

param.formula = as.formula('Value ~ eta + max_depth + subsample + min_child_weight + gamma')
param.model  = gbm(param.formula,
                 distribution = "gaussian",
                 n.trees = 1000,
                 cv.folds = 2,
                 shrinkage = 0.01, 
                 interaction.depth=3,
                 bag.fraction = 0.9,
                 n.cores = 4,
                 n.minobsinnode = 5,
                 data = opt_res[, all.vars(param.formula), with = FALSE],
                 verbose = TRUE)
plot_gbmiterations(param.model)
best_it.gbm = gbm.perf(param.model, plot.it = FALSE)

var_inf = summary(param.model, n.trees = best_it.gbm, plotit = F)
plots = plot_gbmpartial(param.model, best_it.gbm,all.vars(param.formula) %!in_set% c('Value'), output_type = 'link')
marrangeGrob(plots, nrow = 2, ncol = 3, top = NULL)

```

## Save Results
gbm - 0.897
xgb - 0.899

```{r save_results}

submit = df[,.(ID_code, target = pred.xgb)]

submit = submit[df$is_train==FALSE,]

setorder(submit, ID_code)

file = file.path(working_folder, "santander_transaction/solution.csv")
  
fwrite(submit, file = file, row.names = FALSE)

zip(paste(file, '.zip', sep = ''), file)
  
print(file)

#fullVisitorId,PredictedLogRevenue

```