---
title: "Allstate Claims Severity"
author: "Alex"
date: "October 13, 2016"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(reshape2)
library(ggplot2)
library(GGally)
library(Hmisc)
library(plyr)
library(gridExtra)

library(gbm)
library(np)
library(earth)
library(rpart)
library(randomForest)
library(nnet)
library(e1071)
library(MASS)

random_seed = 123

```

## Load Data

```{r load_data}
vars_cat = 116
vars_num = 14

col_classes = c('numeric', rep('factor', vars_cat), rep('numeric', vars_num), 'numeric')

train <- read.csv('C:/Dev/Kaggle/AllstateClaimsSeverity/input/train.csv', colClasses = col_classes)
test  <- read.csv('C:/Dev/Kaggle/AllstateClaimsSeverity/input/test.csv', colClasses = col_classes[-length(col_classes)])

test$loss <- NA
train$tag = 1
test$tag = 0

df = rbind(train, test)

df$log_loss = log(df$loss) 

test_index = df$tag == 0
train_index = df$tag == 1

#create a smaller samples for research
n = dim(df)[1]
train_ids = which(df$tag==1)
ntrain = length(train_ids)
train_index1 = rep(FALSE, n)
train_index2 = rep(FALSE, n)
train_index3 = rep(FALSE, n)
train_index1[sample(train_ids, 0.1*ntrain)] = TRUE #10% of training data
train_index2[sample(train_ids, 0.5*ntrain)] = TRUE #50% of training data
train_index3 = !train_index2 & train_index         #out of sample, sample 3 and 2 partition data

#188318,1074

#combine rare levels for categorical variables 
#sapply(df, class)
#sort(table(df$75))
min_count = 1000
tag_name = "OTHER"
for(cn in cat_vars){
  temp = df[,cn]
  temp = as.character(temp)
  temp[temp %in% names(which(table(temp)<min_count))] = tag_name
  
  #spread observations among other factors 
  other_count = sum(temp==tag_name)
  if(other_count<min_count){ 
    level_counts = table(temp[temp != tag_name])
    levels = sample(names(level_counts), other_count, replace = TRUE, prob = level_counts/sum(level_counts))
    temp[temp==tag_name] <- levels
  }
  df[,cn] = factor(temp)
}

#remove variables with one level

#define variables
cols = names(df)
cat_vars = cols[grep("cat", cols)] 
cat_vars_fake = names(which( sapply(df[,cat_vars], function(x) length(levels(x)) ) == 1))
cat_vars = setdiff(cat_vars, cat_vars_fake)
cont_vars = cols[grep("cont", cols)]
allvars = union ( cat_vars , cont_vars) 
formula.all = formula (paste( 'log_loss ~', paste(allvars, collapse = '+')) )
formula.cont = formula (paste( 'log_loss ~', paste(cont_vars, collapse = '+')) )
formula.cat = formula (paste( 'log_loss ~', paste(cat_vars, collapse = '+')) )

#sapply(df[,cont_vars], sd)
#sapply(df[,cat_vars], function(x) length(levels(x)) )

```

```{r functions}

r_sqr <-function(y, x) {
  return( summary(lm(y ~ x))$r.squared )
}
#plot missing values
ggplot_missing <- function(x){
  mx = melt(is.na(x))
  ggplot(mx, aes(Var2, Var1)) + geom_raster(aes(fill = value)) +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  scale_fill_grey(name = "", labels = c("Valid","NA")) +
  labs(x = "Variable name",   y = "Rows") + 
    ggtitle (paste('total number of missing values:',  sum(mx$value)))
}

#plot number of missing values
ggplot_missing_count <- function(x){
  mc = adply(is.na(x), 2, sum)
  names(mc) <- c('name', 'value')
  ggplot(mc, aes(name, value)) + geom_bar(stat = "identity") +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  labs(x = "Variable name",   y = "Missing Variables")
}

# Friedman's H-statistic to assess the relative strength of interaction effects in non-linear models. H is on the scale of [0-1] 
gbm_interactions <- function(gbm_model, data, min_influence = 1, degree = 2){
  gbm_summary = summary(gbm_model, plotit=FALSE)
  vars = gbm_summary$var[gbm_summary$rel.inf > min_influence]
  all_combinations = combn(as.vector(vars), degree, simplify = TRUE)
  df = ldply(seq(dim(all_combinations)[2]), function(i) {
    data.frame(vars = paste(all_combinations[,i], collapse = '-'), 
               interaction_score = interact.gbm(gbm_model, data, all_combinations[,i])) 
    })
  return ( df[order(df$interaction_score, decreasing = TRUE),] )
}
```

## Overview
```{r overview, fig.width = 8, fig.height = 6, dpi = 100}

ggplot(df[train_index,], aes(sample = log_loss )) + stat_qq()
ggplot(df[train_index,], aes(log_loss)) + stat_density()

#plot
#cat80, cat79, cat101, cat100, cat12

ggplot(df[train_index,], aes(x = loss, fill = cat1)) +stat_density()

ggplot(df[train_index,], aes(x = cat80, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat79, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat101, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat100, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat12, log_loss)) + geom_boxplot() + coord_flip()

ggplot(df, aes(cat80, fill = factor(tag) )) + geom_bar()
ggplot(df, aes(cat79, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat101, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat100, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat12, fill = factor(tag) )) + geom_bar() 

```


## Loss Prediction Models
top 20 variables explain about 90% of variance
top 43 variables explain about 98% of variance

### Recursive Partitioning
```{r rpart, fig.width = 8, fig.height = 6, dpi = 150}
model.rp = rpart(formula.all,
                 data = df[train_index1,], 
                 control = rpart.control(cp = 0.01, minsplit = 20))

printcp(model.rp)

par(mfrow=c(2,1))
plotcp(model.rp) 
plot(model.rp, uniform=TRUE) 
text(model.rp, use.n=TRUE, all=TRUE, cex=.8)

pred.rp = predict(model.rp, newdata = df)

cat(paste('rp (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.rp[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.rp[train_index3]), 4),
          sum(is.na(pred.rp)) ))
```

### Random Forest
```{r rf, fig.width = 8, fig.height = 6, dpi = 100}
model.rf <- randomForest(formula.all, data =  df[train_index1,])

pred.rf = to_numeric(predict(model.rf, newdata = full_df))

importance(model.rf)

cat(paste('rf (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.rf[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.rf[train_index3]), 4),
          sum(is.na(pred.rf)) ))
```

### Linear Regression
lm (in/out): 0.5183 0.5135 0 (1244.69108)
```{r lm, fig.width = 8, fig.height = 6, dpi = 100}
model.lm <- lm(formula.all, data = df[train_index1, all.vars(formula.all)])

summary(model.lm)
pred.lm = predict(model.lm, newdata = df)

cat(paste('lm (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.lm[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.lm[train_index3]), 4),
          sum(is.na(pred.lm)) ))
```

### Stepwise Regression
15 variables
step (in/out): 0.486 0.484 0

log_loss ~ cat80 + cat101 + cat100 + cat103 + cat111 + cat114 + 
    cont7 + cat112 + cat79 + cat57 + cat81 + cont2 + cat53 + 
    cat44 + cat12

```{r step, fig.width = 8, fig.height = 6, dpi = 100}
model.null <- lm(log_loss ~ 1, data = df[train_index1, all.vars(formula.all)])

model.step = stepAIC(model.null, scope = list(upper = formula.all, lower = ~1), step = 20)

summary(model.step)
#anova(model.step)

pred.step = predict(model.step, newdata = df)

cat(paste('step (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.step[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.step[train_index3]), 4),
          sum(is.na(pred.step)) ))

```

### Multivariate Adaptive Regression Splines
```{r mars, fig.width = 8, fig.height = 6, dpi = 100}
set.seed(random_seed)

model.mars <- earth(formula.cont, 
                    data = df[train_index1, all.vars(formula.cont)], 
                    degree = 2, nfold = 3, trace = 3)

plot(model.mars)
summary(model.mars)
par(mfrow=c(1,1))
plot(evimp(model.mars))
#plotmo(model.mars)

#0.495587497528404
pred.mars = as.vector(predict(model.mars, newdata = df))

cat(paste('mars (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.mars[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.mars[train_index3]), 4),
          sum(is.na(pred.mars)) ))
```


### GBM
gbm (in/out): 0.5428 0.5322 0 (1177.31384)
cat80   cat80 3.159216e+01
cat79   cat79 9.135857e+00
cat101 cat101 7.986626e+00
cat100 cat100 6.942942e+00
cat12   cat12 4.391445e+00
cat81   cat81 4.257994e+00
cat112 cat112 3.818140e+00
cat114 cat114 2.438633e+00
cont2   cont2 2.331851e+00
cat103 cat103 2.228449e+00
cat57   cat57 2.169527e+00
cat53   cat53 1.922854e+00
cont7   cont7 1.593516e+00
cat111 cat111 1.472601e+00
cat1     cat1 1.286359e+00
cat108 cat108 1.218852e+00
cat113 cat113 1.187182e+00
cat116 cat116 1.166922e+00
cat44   cat44 1.159283e+00
cat72   cat72 1.155994e+00
```{r gbm, fig.width = 8, fig.height = 6, dpi = 150}
set.seed(random_seed)

ntrees = 10000
model.gbm = gbm(formula.all, 
                data = df[train_index2, all.vars(formula.all)], 
                distribution = 'gaussian',
                n.trees = ntrees,
                shrinkage = 0.01,#0.005
                interaction.depth = 2,
                cv.folds = 3)
print(model.gbm)

par(mfrow = c(1, 2), las = 1)
best_it = gbm.perf(model.gbm, method = 'cv')
grid()
vars.importance = summary(model.gbm, n.trees = best_it) # influence
print(vars.importance)
grid()

par(mfrow=c(4,4))
for(i in seq( pmin(16, length(vars.importance$var))) ) {
  plot.gbm(model.gbm, n.trees =best_it,  i = as.character(vars.importance$var[i]) )
  grid()
}

#cat('Two-way interactions') 
#gbm_interactions(model.gbm, df[train_index1,], 5, 2)

pred.gbm = predict(model.gbm, n.trees = best_it, newdata = df)

#gbm (in/out): 0.5428 0.5322 0
cat(paste('gbm (in/out):', 
          round(r_sqr(df$log_loss[train_index2],  pred.gbm[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.gbm[train_index3]), 4),
          sum(is.na(pred.gbm)) ))

```

### SVM 
```{r svm, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE}
svn_formula = formula(Surv ~ Sex + Age + Pclass + Siblings + Parch + Embarked + Fare + CabinNumber + surv_family_count + died_family_count)
model.svm <- svm(svn_formula, data=full_df[train_index,all.vars(svn_formula)], cost = 1, gamma = 0.125)
summary(model.svm)

#svm_tune <- tune(svm, train.x=svn_formula,  data=full_df[train_index,],
#                 kernel="radial", 
#                 ranges=list(cost=2^(-6:6), gamma=2^(-6:6)))
#print(svm_tune)


pred.svm = to_numeric(predict(model.svm, newdata = full_df))

cat(paste("svm ccr:", model_ccr(Survived, pred.svm[train_index])))
plot_binmodel_predictions(Survived, pred.svm[train_index])

```

### N-Networks
```{r nnet, fig.width = 8, fig.height = 6, dpi = 100, eval = FALSE}
set.seed(random_seed)
model.nnet <- nnet(Survived ~ Sex + Age + Pclass + Siblings + Parch + Embarked + Fare, 
                   data = full_df[train_index,], size=6, maxit = 1000, trace=FALSE, decay = 1e-4)
pred.nnet <- predict(model.nnet, newdata = full_df)

print(model.nnet)

cat(paste('Neural Networks:', model_ccr(Survived, pred.nnet[train_index])))
plot_binmodel_predictions(Survived, pred.nnet[train_index])
```


## Compare
```{r compare, fig.width = 8, fig.height = 6, dpi = 100}

results = list()
#results$mars =pred.mars
results$lm =pred.lm
results$step =pred.step
results$gbm =pred.gbm

#results$np = pred.np
#results$glm = pred.glm
#results$glm_s = pred.glm_s
#results$gbm = pred.gbm 
#results$nnet = pred.nnet
#results$svm = pred.svm
#results$rf = pred.rf
#results$rp = pred.rp


res = ldply(results, .id = 'model', function(x) {
  c(r2_out = r_sqr(df$log_loss[train_index3],  x[train_index3]),
    r2_in = r_sqr(df$log_loss[train_index2],  x[train_index2]),
    na_count = sum(is.na(x[test_index])))
})

print(res)

ggplot(res, aes(model, r2_out)) + geom_bar(stat = "identity") + coord_flip() + coord_cartesian(ylim = c(0.9*min(res$r2_out), 1.0))

```

## Submit
```{r submit, fig.width = 8, fig.height = 6, dpi = 100}

folder = "C:/Dev/Kaggle/AllstateClaimsSeverity/"

for (model_name in names(results) ){
  submit <- data.frame(id = as.integer( as.numeric(df$id[test_index]) ), 
                       loss = exp(results[[model_name]][test_index]))
  
  submit = submit[order(submit$id),]
  
  file = file.path(folder, sprintf("my_solution_%s.csv", model_name))
  
  write.csv(submit, file = file, row.names = FALSE)
}


```
