---
title: 'Kaggle Playground: Feb 2020'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(stringi)
library(gbm)
library(ggplot2)
library(gridExtra)
#library(dplyr)
library(plyr)
library(corrplot)
library(xgboost)
#library(zip)
library(caret)
library(lightgbm)
library(forcats)
library(rBayesianOptimization)
library(tune) #https://datascienceplus.com/grid-search-and-bayesian-hyperparameter-optimization-using-tune-and-caret-packages/


working_folder = 'D:/Github/KaggleSandbox'
#working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')

source(file.path(working_folder, 'Utils/common.R'))

rmsqr <-function(actual, model) {
  sqrt( mean( (actual - model) * (actual - model) ) )
}

```

## Load Data

```{r load_data}
load_existing = FALSE

if (load_existing) {
  df <- fread(file.path(working_folder,'Playground/Feb2021/data/df.csv'), check.names = TRUE)
  
} else{
  train <- fread(file.path(working_folder,'Playground/Feb2021/data/train.csv'), check.names = TRUE)
  test  <- fread(file.path(working_folder,'Playground/Feb2021/data/test.csv'),  check.names = TRUE) # 1459   80
  test[, target:=NA]
  df = rbind(train, test)
  
  gc(reset=TRUE)
}
  

test_index = is.na(df$target)
train_index = !test_index

obj_var = 'target'
all_vars = names(df) %!in_set% c('id', obj_var) #14 variables
all_vars = all_vars[grep('^(cont|cat)', all_vars)]
cat_vars = all_vars[grep('^(cat)', all_vars)]

df[, (cat_vars):=lapply(.SD, function(x) fct_infreq(fct_lump_min(x, 0.01*nrow(df), other_level = "OT"))), .SDcols = cat_vars]

plot_profiles <-function(model, data)
{
  #stri_join('p_',all_vars)
    plots = llply(all_vars, function(var_name) {
    p = plot_profile(model,  data[['target']], data[[var_name]], bucket_count = 20, error_band = 'norm') +
      ggtitle(var_name) +  theme(title =element_text(size=6))
    return( ggplotGrob(p) )
  })
  marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)
}

#plot_profiles(df$target_lgb[train_index], df[train_index,])

plot_profiles_2d <-function(model, data)
{
   all_comb = data.table(t(combn(all_vars, m = 2)) )
   all_comb = all_comb[V1!=V2]
   #all_comb = all_comb[1:36]
  
    plots = llply(seq(nrow(all_comb)), function(i) {
      var1 = all_comb$V1[i]
      var2 = all_comb$V2[i]
     p = ggplot(cbind(data, model), aes_string(var1, var2, z = 'target - model')) + stat_summary_hex(fun = function(x) ifelse(length(x)>100, mean(x), NA), bins = 10) + scale_fill_gradient2() +theme(title =element_text(size=6)) +  theme(legend.position = "None")
     #p = ggplot(data, aes_string(var1, var2)) + geom_hex(bins = 10) + theme(legend.position = "None")
    
    return( ggplotGrob(p) )
  })
  marrangeGrob(plots, nrow = 6, ncol = 6, top = NULL)
}

#plot_profiles_2d(df$target_lgb[train_index], df[train_index,])
partialPlot <- function(obj, pred.data, xname, n.pt = 19, discrete.x = FALSE, 
                        subsample = pmin(1, n.pt * 100 / nrow(pred.data)), which.class = NULL,
                        xlab = deparse(substitute(xname)), ylab = "", type = if (discrete.x) "p" else "b",
                        main = "", rug = TRUE, seed = NULL, ...) {
  stopifnot(dim(pred.data) >= 1)
  
  if (subsample < 1) {
    if (!is.null(seed)) {
      set.seed(seed)
    } 
    n <- nrow(pred.data)
    picked <- sample(n, trunc(subsample * n))
    pred.data <- pred.data[picked, , drop = FALSE]
  }
  xv <- pred.data[, xname]
  
  if (discrete.x) {
    x <- unique(xv)
  } else {
    x <- quantile(xv, seq(0.03, 0.97, length.out = n.pt), names = FALSE)
  }
  y <- numeric(length(x))
  
  isRanger <- inherits(obj, "ranger")
  isLm <- inherits(obj, "lm") | inherits(obj, "lmrob") | inherits(obj, "lmerMod")

  for (i in seq_along(x)) {
   pred.data[, xname] <- x[i]

    if (isRanger) {
      if (!is.null(which.class)) {
        if (obj$treetype != "Probability estimation") {
          stop("Choose probability = TRUE when fitting ranger multiclass model") 
        }
        preds <- predict(obj, pred.data)$predictions[, which.class]
      }
      else {
        preds <- predict(obj, pred.data)$predictions
      }
    } else if (isLm) {
      preds <- predict(obj, pred.data) 
    } else {
      if (!is.null(which.class)) {
        preds <- predict(obj, pred.data, reshape = TRUE)[, which.class + 1] 
      } else {
        preds <- predict(obj, pred.data)
      }
    }
    
    y[i] <- mean(preds)
  }
  
  #plot(x, y, xlab = xlab, ylab = ylab, main = main, type = type, ...)
  data.frame(x = x, y = y)
}

```
##Plot Data

```{r plot_data}

cor_mat = cor(data.matrix(df[train_index,c('target', all_vars), with = FALSE]), use = 'pairwise.complete.obs')
corrplot(cor_mat, method="number", number.cex = 0.8, number.digits = 2,  order="hclust")
corrplot(cor_mat, method="circle", number.cex = 0.5, order="hclust")

p_index = sample(which(train_index), 10000 )
ggplot(df[p_index], aes(id, target)) + geom_point()

ggplot(df[p_index], aes(cont8, target)) + geom_point() + geom_smooth()
ggplot(df[p_index], aes(cont3, target)) + geom_point() + geom_smooth()

#check sample
s_index = sample.int(nrow(df), nrow(df))
plots = llply(all_vars, function(var_name){
  ggplot(df[s_index ], aes_string(var_name, group = 'is.na(target)', color = 'is.na(target)')) + geom_density(adjust = 0.1) + ggtitle(var_name)
  })
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)

var_pairs = data.frame(t(combn(all_vars, 2, simplify = TRUE)))
plots = llply(seq(nrow(var_pairs)), function(i) { 
   ggplot(df[p_index ], aes_string(var_pairs$X1[i], var_pairs$X2[i])) + geom_point(alpha = 0.5)
  })
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)
#ggplot(melt(data.table(cor_mat)), aes(Var1, Var2, fill = value)) + geom_tile()

#check correlations

combn(as.character(all_vars), 2, simplify = TRUE)
all_comb = data.table(expand.grid(all_vars, all_vars, all_vars, all_vars))
#all_comb = all_comb[Var1 != Var2]

res = ldply(seq(nrow(all_comb)), function(i) { 
  a1 = df[[all_comb$Var1[i]]][p_index]
  a2 = df[[all_comb$Var2[i]]][p_index]
  a3 = df[[all_comb$Var3[i]]][p_index]
  a4 = df[[all_comb$Var4[i]]][p_index]
  
  data.frame(i, rho = cor(df[p_index, target], (a1 - a2)/ (2 + a3 - a4) , use = 'pairwise.complete.obs' ))  } )
setDT(res)
res[order(abs(rho))]
res[!is.na(rho)]
all_comb[38415    ]

ggplot(df[p_index], aes((cont13 / cont14 ), target)) + geom_point() + geom_smooth()

```


## Winsorization

```{r Winsorization,  eval = FALSE}

winsoraze<-function(x, xt, alpha = 0.05) {
  q_bounds = quantile(xt, c(alpha/2, 1- alpha/2))
  x = pmax(pmin(x, q_bounds[2]), q_bounds[1])
  return (x)
}

w_vars = stri_join('w_', all_vars)
df[, (w_vars):=lapply(.SD, function(x) winsoraze(x, x[train_index], 0.009123116) ), .SDcols = all_vars]

#d_vars = stri_join('d6_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 6 ), .SDcols = all_vars]

#d_vars = stri_join('d7_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 7 ), .SDcols = all_vars]

```

## Count vars

```{r count_vars, eval = FALSE}

count_rows<-function(x, alpha = 0.5) {
  print(x)
  return ( sum(as.numeric(x > alpha)) )
}

df[, v_max := apply(.SD, 1, max ), .SDcols = all_vars]
df[, v_min := apply(.SD, 1, min ), .SDcols = all_vars]
df[, v7 := apply(.SD, 1, function(x) sum(as.numeric(x>0.7)) ), .SDcols = all_vars]
df[, v9 := apply(.SD, 1, function(x) sum(as.numeric(x>0.9)) ), .SDcols = all_vars]

ggplot(df[p_index, ], aes(temp, target )) + geom_point(alpha = 0.2) + geom_smooth()

#d_vars = stri_join('d6_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 6 ), .SDcols = all_vars]

#d_vars = stri_join('d7_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 7 ), .SDcols = all_vars]

```

##MARS

```{r mars}
library(earth)

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))


formula.mars = formula(stri_join( obj_var, ' ~ ', stri_join(all_vars, collapse = ' + ')))

model.mars <- earth(formula.mars, 
                    data = df[t_index_v1, c(obj_var, all_vars), with = FALSE], 
                    degree = 3, nfold = 5, trace = 2, nk = 100, pmethod="cv", thresh = 0.001)

#degree 1,  GRSq 0.03410652  RSq 0.03433832  mean.oof.RSq 0.03411973 (sd 0.00129)
#degree 2,  GRSq 0.03862549  RSq 0.03901  mean.oof.RSq 0.04015907 (sd 0.00344)

summary(model.mars)
plot(evimp(model.mars))

#basis_fun = data.table(id = df$id[t_index_v1], model.mars$bx)
#fwrite(basis_fun, file.path(working_folder,'Playground/Jan2021/data/basis_fun.csv'))

pred.mars <- predict(model.mars, df[,all_vars, with = F] )

rmsqr(df$target[train_index], pred.mars[train_index] )
#rmsqr(df$target[train_index], df$target_mars[train_index] )

plot_profiles(pred.mars[train_index], df[train_index,])

#df[, target_mars := pred.mars]
#fwrite(df, file.path(working_folder,'Playground/Jan2021/data/df.csv'))

```

#LightGBM

```{r light_gbm, eval = FALSE}

set.seed(132140937)

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

lgb_vars = all_vars
#lgb_vars = stri_join('p_', all_vars)
#lgb_vars = stri_join('w_', all_vars) #0.6564345

#lgb_vars = c(stri_join('w_', all_vars), stri_join('d6_', all_vars), stri_join('d7_', all_vars))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
params <- list(objective = "regression", metric = "rmse")
#params <- list(objective = "huber")

set.seed(140937345)

model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nrounds = 10000,
  nfold = 10,
  num_threads = 5, 
  
  min_data = 1152,
  learning_rate = 0.00821259,
  num_leaves = 12,
  bagging_fraction = 0.967491,
  min_data_in_bin = 6,
  
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100,
  force_col_wise=TRUE
)

#best.iter = model.lgb$best_iter #
#model.lgb$best_score #0.6983437

cv_error = as.numeric(model.lgb$record_evals$valid$rmse$eval)
ggplot(data.frame( i = seq(length(cv_error)), cv_error ), aes(i, cv_error)) + geom_line()

min(cv_error)

dm_all = data.matrix(df[,lgb_vars, with = F])
pred.lgb_cvi = ldply(seq(length(model.lgb$boosters)), function(i){ data.frame(cv = i, id = df$id, pred=  predict(model.lgb$boosters[[i]], dm_all)$booster) } )
setDT(pred.lgb_cvi)

pred.lgb_cv_summary = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(cv)]
pred.lgb_cv         = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(id)]

#var_imp   = lgb.importance(model.lgb$boosters[[1]][[1]], percentage = TRUE)
#lgb.plot.importance(var_imp, top_n = 20, measure = "Gain")

rmsqr(df$target[train_index], pred.lgb_cv$avg[train_index] )

#%% Plot -----
#plot_profiles(df$target_lgb[train_index], df[train_index,])
#plot_profiles_2d(df$target_lgb[p_index], df[p_index,])

df[pred.lgb_cv, target_lgb :=  i.avg, on=.(id)]

lgb_importance = lgb.importance(model.lgb$boosters[[1]][[1]], percentage = TRUE)
lgb.plot.interpretation(lgb_importance)
#
p_index = which(train_index)
p_index = sample(p_index, 10000)
ggplot(df[p_index, ], aes(target_lgb, target)) + geom_point(alpha = 0.2) + geom_abline(slope = 1, color = 'red')
#ggplot(df[p_index, ], aes(p_cont3, target)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE)
ggplot(df[p_index, ], aes(cont8, target)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE)

#all y ~ x counts
plots = llply(lgb_vars, function(vname){
  p = ggplot(df[train_index & target > 5, ], aes_string(vname, 'target')) + stat_bin_2d(bins = 200) +ggtitle(vname) + 
    theme(axis.title.x = element_blank(), legend.position = 'none')
})
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)


ggplot(df[train_index & target > 5, ], aes(cont13, cont3)) + stat_bin_2d(bins = 100)
ggplot(df[train_index & target > 5, ], aes(cont13, cont3, z = target)) + stat_summary_2d(bins = 100)
ggplot(df[train_index & target > 5, ], aes(cont13, cont3, z =target_lgb)) + stat_summary_2d(bins = 100)

ggplot(df[train_index & target > 5, ], aes(cont10, target)) + stat_bin_2d(bins = 200)
ggplot(df[train_index & target > 5, ], aes(w_cont10, target)) + stat_bin_2d(bins = 200)


ggplot(df[p_index, ], aes(sample = target_lgb - target)) + stat_qq() + stat_qq_line()


ggplot(df[p_index, ], aes(target_lgb, target)) + geom_point(alpha = 0.2) + geom_abline(slope = 1, color = 'red')
ggplot(df[p_index, ], aes(target_lgb, target  - target_lgb)) + geom_point(alpha = 0.2)
ggplot(df[p_index, ], aes(target_lgb, sqrt((target - target_lgb)^2) )) + geom_point(alpha = 0.2)
ggplot(df[p_index, ], aes(sample = (target - target_lgb) )) + stat_qq()


ggplot(df[train_index, ], aes(target)) + stat_ecdf()
ggplot(df[train_index, ], aes(target_lgb - target)) + stat_ecdf()

plot_profile(df[train_index, target],  df[train_index, target_lgb], 
             df[train_index,  cont8], bucket_count = 50, error_band = 'norm') 

#cor(df[train_index, .(target, shift(cont13, 100) )], use = 'pairwise.complete.obs')

pdp_index = sample(which(train_index), 100)

my_model = model.lgb$boosters[[1]][[1]]
df_plot = partialPlot(my_model, data.matrix(df[pdp_index,lgb_vars, with = FALSE]), xname = "cont1")
ggplot(df_plot, aes(x, y)) + geom_line()

plots = llply(lgb_vars, function(vname){
  df_plot = partialPlot(my_model, data.matrix(df[pdp_index,lgb_vars, with = FALSE]), xname = vname, n.pt = 100)
  df_plot[vname] = df_plot$x
  p = ggplot(df_plot, aes_string(vname, 'y')) + geom_line() + geom_point() + 
    geom_rug(data = df[pdp_index,c(vname, 'target'), with = FALSE], aes_string(vname, 'target'))
})
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)


```

#LightGBM Tuning
 best_it best_score  elapsed learning_rate bagging_fraction min_data min_data_in_bin num_leaves rank
1:    2521   0.843085 3.575185   0.007635167        0.9311752      619               9         72    1
1:    2521   0.843085 3.575185   0.007635167        0.95           700               3         70    1
1:    3132   0.842883 3.246344   0.007594201        0.9331378      932               6         42    1

   best_it best_score  elapsed learning_rate bagging_fraction min_data min_data_in_bin num_leaves rank
1:    7346  0.8426274 3.710974    0.00821259         0.967491     1152               6         12    1

```{r light_gbm_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = stri_join('w_', all_vars)
#lgb_vars = c(all_vars, 'target_knn')

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

params <- list(objective = "regression", metric = "rmse")

my_params = data.table(
                       learning_rate = 0.00821259,#0.0110, 
                       bagging_fraction = 0.967491, 
                       min_data = 1152, #default 20
                       min_data_in_bin = 6, #default: 3
                       num_leaves = seq(10, 20) )  #default: 31

n_runs = 100 # 10 runs per hour
my_params = data.table(
                       learning_rate = runif(n_runs, 0.007, 0.009), 
                       bagging_fraction = runif(n_runs, 0.9, 1.0), 
                       min_data = sample(seq(from = 850, to = 1200),n_runs, TRUE),
                       min_data_in_bin = sample(seq(5, 7),n_runs, TRUE),
                       num_leaves = sample(seq(10, 30),n_runs, TRUE))


param_res_raw = ldply(seq(nrow(my_params)), function(run_index){
  #run_index = 1
  print(run_index)
  print(my_params[run_index,])
  
  start_time <- Sys.time()
  
  dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
  
  model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nfold = 5,
  num_threads = 5, 
  verbose = 0,
  force_col_wise=TRUE,
  
  learning_rate = my_params$learning_rate[run_index],
  bagging_fraction = my_params$bagging_fraction[run_index],
  min_data = my_params$min_data[run_index],
  num_leaves = my_params$num_leaves[run_index],
  min_data_in_bin = my_params$min_data_in_bin[run_index],
  
  nrounds = 10000,
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100)
  
  print(model.lgb$best_score)
  
  gc(reset = TRUE)
  elapsed = as.numeric(difftime(Sys.time(),start_time,units="secs"))/60
  return ( data.frame(best_it = model.lgb$best_iter, best_score = model.lgb$best_score, elapsed = elapsed ) ) 
})

param_res = cbind(param_res_raw, my_params)
setDT(param_res)
setorder(param_res, best_score)
param_res[, rank:=seq(nrow(param_res))]
param_res[best_score  == min(best_score)]

#learning_rate = 0.008, bagging_fraction=0.95, min_data = 950, min_data_in_bin = 6

ggplot(param_res, aes(best_it, elapsed)) + geom_point()
ggplot(param_res, aes(best_it, learning_rate)) + geom_point()
ggplot(param_res, aes(learning_rate, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(bagging_fraction, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(min_data, best_score)) + geom_point()+ geom_smooth()
ggplot(param_res, aes(min_data_in_bin, best_score)) + geom_point() + geom_smooth(span  = 0.5)
ggplot(param_res, aes(num_leaves, best_score)) + geom_point()  + geom_smooth(span  = 0.5)

```


#LightGBM Bayes Tuning
Round = 14	learning_rate = 0.0092	bagging_fraction = 0.9005	min_data = 818.0000	num_leaves = 14.0000	min_data_in_bin = 6.0000	Value = -0.8501 
```{r light_gbm_bayes_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = all_vars

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 5] #exclude target < 5.0


params <- list(objective = "regression", metric = "rmse")

lgb_cv_bayes <- function(learning_rate, bagging_fraction, min_data, num_leaves, min_data_in_bin) {
    #set.seed(140937345)
  
    dtrain <- lgb.Dataset(as.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
    
    model.lgb <- lgb.cv(
    params = params,
    data = dtrain,
    nfold = 10,
    num_threads = 5, 
    verbose = 0,
    
    learning_rate = learning_rate,
    bagging_fraction = bagging_fraction,
    min_data =min_data,
    num_leaves = num_leaves,
    min_data_in_bin = min_data_in_bin,
    
    nrounds = 10000,
    boost_from_average = TRUE,
    eval_freq = 100,
    early_stopping_rounds = 100,
    force_col_wise=TRUE)
   
  gc(reset = TRUE)
  
  list(Score = -model.lgb$best_score, Pred =  model.lgb$best_iter)
}

OPT_Res <- BayesianOptimization(lgb_cv_bayes,
                                bounds = list(
                                learning_rate = c(0.007, 0.01),
                                bagging_fraction = c(0.8, 1.0), #default: 1.0
                                min_data = c(600L, 1200L), #default: 20
                                num_leaves =c(8L, 30L), #default: 31
                                min_data_in_bin =c(3L, 9L)), #default: 3
                                init_grid_dt = NULL, 
                                init_points = 5, #10
                                n_iter = 60,     #50
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
opt_res = data.table(OPT_Res$History, it_count=as.numeric(OPT_Res$Pred))
setorder(opt_res, Value)

opt_res[Value == max(Value)]

ggplot(opt_res, aes(Round , -Value)) + geom_line()
ggplot(opt_res, aes(learning_rate, -Value)) + geom_point()
ggplot(opt_res, aes(bagging_fraction, -Value)) + geom_point()
ggplot(opt_res, aes(min_data, -Value)) + geom_point()
ggplot(opt_res, aes(num_leaves, -Value)) + geom_point()
ggplot(opt_res, aes(min_data_in_bin, -Value)) + geom_point()
ggplot(opt_res, aes(Round, -Value)) + geom_point()
#ggplot(opt_res, aes(Round, it_count)) + geom_line(alpha = 0.6)

```

#Bayes test
```{r bayes_tune, eval = FALSE}

test_bayes <- function(x, y) {
  
  a = 1
  b = 100
  
  list(Score = - ((a - x)^2 + b*(y - x*x)^2), Pred =  model.lgb$best_iter)
}

OPT_Res <- BayesianOptimization(test_bayes,
                                bounds = list(
                                x = c(-10, 10),
                                y = c(-10, 10)),
                                init_points = 10, 
                                n_iter = 50,
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
opt_res = data.table(OPT_Res$History, it_count=as.numeric(OPT_Res$Pred))
setorder(opt_res, Value)

opt_res[Value == max(Value)]

ggplot(opt_res, aes(x, y)) + geom_path() + geom_point()

ggplot(opt_res, aes(x, log(-Value) )) + geom_point() + geom_vline(xintercept = 1)
ggplot(opt_res, aes(y, log(-Value) )) + geom_point() + geom_vline(xintercept = 1)

```

#Submit
MY BEST: 0.84357
v1: 0.84357 min(0.8431303)
v2: 0.84324 min(0.8426456)
```{r submit, echo=FALSE}
  #model_pred = pred.xgb
  #df[pred.lgb_cv, target_lgb :=  i.avg, on=.(id)]
  #fwrite(df, file.path(working_folder,'Playground/Feb2021/data/df.csv'))
 
  file = file.path(working_folder, "Playground/Feb2021/submit_v2.lgb.csv")
  fwrite(df[test_index, .(id, target=target_lgb)], file = file, row.names = FALSE)
  zip(paste(file, '.zip', sep = ''), file, flags = '-r9Xj')
  print(file)

```


