---
title: "GBM"
output: html_document
---
---
title: "Overfitting"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(reshape2)
library(ggplot2)
library(Hmisc)
library(plyr)
library(gridExtra)
library(corrplot)

library(gbm)
library(np)
library(earth) 
library(rpart)
library(party)
library(caret)
library(randomForest)
library(nnet)
library(e1071)
library(MASS)
library(lubridate)

library(knitr)
library(foreach)

knitr::opts_chunk$set(echo = TRUE)
```

## Load data
```{r load_data}
rm(list = ls())

# READ DATA ---- 
max_it_mult = 1000

inf_lowlimit = 0.1

#dont set seed
#random_seed = 12345678
#set.seed(random_seed)

#working_folder = 'C:/Dev/Kaggle/'
working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')

source(file.path(working_folder, 'Utils/common.R'))

sub_area_cluster_info = read.csv(file.path(working_folder,'SberbankHousing/clusters_info.csv') )
sub_area_cluster_info = subset(sub_area_cluster_info, cluster_algo == 'ward' & cluster_count == 10)
sub_area_cluster_info$cluster_group = factor(sub_area_cluster_info$cluster_group)

#read previous results
#prev_run = read.csv(file = file.path(working_folder, "SberbankHousing/train_sample_gbm_cv.csv"))
#prev_run$error = with(prev_run, log(price_mdl+1) - log(price_act+1))  

macro <- read.csv(file.path(working_folder,'SberbankHousing/macro.csv'))
train <- read.csv(file.path(working_folder,'SberbankHousing/train.csv'))
test  <- read.csv(file.path(working_folder,'SberbankHousing/test.csv')) # 1459   80
test$price_doc <- NA
df = rbind(train, test)
df$price_doc = 1e-3 * df$price_doc
# Variables and Data prep-rocessing  ---- 

life_na_index = is.na(df$life_sq) | df$life_sq>df$full_sq

df$price_log =  log(df$price_doc + 1)
df$full_sq_log = log(df$full_sq + 1)
df$life_sq[life_na_index] = df$full_sq[life_na_index] # set missing life to full or when life > full
df$life_sq_log = log(df$life_sq + 1)
df$area_m_log = log(df$area_m + 1)
df$office_sqm_2000_log = log(df$office_sqm_2000 + 1)

df$kitch_sq[is.na(df$kitch_sq) | df$kitch_sq>df$full_sq | df$kitch_sq < 3] = NA #kitchen cant be > full, but dont exclude from the set 

df$build_year[df$build_year<1900 | df$build_year > 2019]  = NA #dont exclude form the set 
table(df$build_year)

df$build_year5 = 5*floor(df$build_year/5)
df$build_year10 = 10*floor(df$build_year/10)

df$max_floor_adj = pmax(df$max_floor, df$floor)
df$floor_diff = df$max_floor_adj - df$floor 
df$date = as.Date(as.character(df$timestamp))
df$sale_year = year(df$date )
df$sale_month = month(df$date )
df$sale_time = df$sale_year + (df$sale_month - 1) / 12 
df$state = pmin(df$state, 4)
df$office_sqm_2000_log = log(df$office_sqm_2000 + 1)
df$office_sqm_1500_log = log(df$office_sqm_1500 + 1)
df$room_sq = df$full_sq / df$num_room

df$public_healthcare_near = pmax(1, pmin(10, df$public_healthcare_km))
df$kindergarten_near = pmax(0.5, pmin(2.0, df$kindergarten_km))


#set invalid values to NA
df$num_room[df$num_room == 0] = NA
df$floor[df$floor == 0] = NA
df$max_floor_adj[df$max_floor == 0] = NA

#merge with macro
macro$date = as.Date(as.character(macro$timestamp))
df  =  merge(df[,names(df) != 'timestamp'], macro[,names(macro) != 'timestamp'], by.x = 'date', by.y = 'date', all.x = TRUE)
df  =  merge(df, sub_area_cluster_info[,c('name', 'cluster_group')], by.x = 'sub_area', by.y = 'name', all.x = TRUE)
#df  =  merge(df, prev_run[,c('id','error')], by.x = 'id', by.y = 'id', all.x = TRUE)

test_index = is.na(df$price_doc)
train_index = !test_index

#print(ecdf(df$full_sq[train_index])(10))

#filter out outliers
max_sq = max(df$full_sq[test_index], na.rm = T)
train_index = train_index & df$full_sq <= max_sq & df$full_sq >= 10
train_index = train_index & (df$num_room <= 10  | is.na(df$num_room))
train_index = train_index & (df$max_floor <= 60 | is.na(df$max_floor))
train_index = train_index & (df$floor <= 50 | is.na(df$floor))

#filter out prices which are below 1000 (747)
#train_index = train_index & df$price_doc > 1000
#train_index = train_index & df$price_doc != 2000
#train_index = train_index & df$price_doc != 3000
#train_index = train_index & abs(df$error) < 1.0

plot(density(log(df$price_doc[train_index]), adjust = 0.1))

df$sample = 'exclude'
df$sample[train_index] = 'train'
df$sample[test_index] = 'test'
df$sample = factor(df$sample)
table(df$sample)

non_vars = c('price_log', 'price_doc', 'id', 'timestamp', 'sample', 'date') #exclude sale_year and month

sig_vars = as.character(read.csv(file.path(working_folder, 'SberbankHousing/sig_vars.csv'), header = FALSE)[,1])

print(sig_vars)

ggplot(df[train_index,], aes(office_sqm_2000_log, price_log)) + geom_point(size = 0.1 ) + geom_smooth() + facet_wrap(~product_type)
ggplot(df[train_index,], aes(office_sqm_2000_log, office_sqm_1500_log)) + geom_point(size = 0.1 ) + geom_smooth() + facet_wrap(~product_type)
ggplot(df[train_index,], aes(green_zone_km, price_log )) + geom_point(size = 0.1 ) + geom_smooth()
ggplot(df[train_index,], aes(life_sq, full_sq)) + geom_point(size = 0.1 )
ggplot(df[test_index,], aes(office_sqm_2000_log, office_sqm_1500_log)) + geom_point(size = 0.1 )

summary(df[train_index,'product_type'])
summary(df[test_index,'product_type'])

ggplot(df[df$full_sq_adj<100,], aes(full_sq, color = sample)) + stat_ecdf()
ggplot(df, aes(factor(sale_year), fill = sample)) + geom_histogram(stat="count", position = 'dodge')
ggplot(df, aes(factor(sub_area), fill = sample)) + geom_histogram(stat="count", position = 'dodge') + theme( axis.text.x = element_text(angle = 90, hjust = 1))

ggplot(df, aes(workplaces_km, color = sample)) + stat_ecdf()
ggplot(df, aes(sale_time, color = sample)) + geom_density(adjust = 0.8)

table(df$sample)
#write.clipboard(df[df$full_sq<10,all.vars(formula.all)])

plots = llply(sig_vars, function(vname) {
  temp = df[, c(vname, 'sample')]
  names(temp) = c('x', 'sample')
  ggplot(temp, aes( x, group = factor(sample), color = sample )) + 
    geom_density(adjust = 0.5) + ggtitle(vname) + xlab(vname)
  
} )
marrangeGrob(plots, nrow=2, ncol=1)

#macro plots
macro_melt = melt(macro[, c('date', sig_vars %in_set% names(macro) )], id.vars = 'date')

ggplot(macro_melt, aes(date, value, group = variable, color = variable)) + geom_line() + facet_wrap(~variable, scales = 'free') + theme(legend.position = 'none')

```


## Linear model
The half of test set is for 2016 - there is no 2016 in train set. need to manually adjust 2016 results 
```{r lm_model, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE, echo=TRUE}
start_time <- proc.time()

df_lm = df
df_lm$sale_year = factor(pmin(df$sale_year, 2015))

model.lm = lm(price_log ~ full_sq_log + life_sq_log + product_type + num_room + floor, data = df_lm[train_index,] )
summary(model.lm)
pred.lm = predict(model.lm, newdata = df_lm)
pred.lm = exp(pred.lm) - 1
#plot(model.lm)

rms_log(1e3*df$price_doc[train_index],  1e3*pred.lm[train_index])

print((proc.time() - start_time)[3])

ggplot(data.frame(error = df$price_log[train_index] - log(pred.lm[train_index] + 1)), aes(abs(error) )) + stat_ecdf()
ggplot(data.frame(error = df$price_log[train_index] - log(pred.lm[train_index] + 1)), aes(error )) + stat_ecdf()

ggplot(data.frame(actual = df$price_log[train_index] , model = log(pred.lm[train_index] + 1)), aes(model, actual, color = abs(actual - model)>1.0 )) + geom_point(size = 0.5)


```

## NP model
```{r np_model, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE, echo=TRUE}
start_time <- proc.time()

np.bw <- npregbw(price_log ~ full_sq +  sub_area + product_type + sale_time, bandwidth.compute = FALSE, bws = c(3, 2, 1, 2), bwscaling=TRUE, data = df[train_index,])
np.reg = npreg(np.bw)
pred.np = predict(np.reg, newdata  = df)
summary(np.bw)
summary(np.reg)

plot(np.bw)
plot(df$price_log[train_index], pred.np[train_index])

pred.np = exp(pred.np) - 1.0

rms_log(1e3*df$price_doc[train_index],  1e3*pred.np[train_index])

print((proc.time() - start_time)[3])

```

## GBM All
```{r gbm_sig_model, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE, echo=FALSE}
start_time <- proc.time()

allvars =  unique(names(df) %!in_set% c(non_vars, 'sub_area'))

formula.all = formula (paste( 'price_log ~', paste(allvars, collapse = '+')) )

print(formula.all)
print(length(allvars))

var.monotone = rep(0, length(allvars)) #1-increasing, -1 - decreasing, 0: any
var.monotone[allvars %in% c('full_sq', 'num_room','state', 'usdrub','eurrub')] =  1
var.monotone[allvars %in% c('metro_min_walk', 'fitness_km')] =  -1

max_it = 100*max_it_mult #64k is for s=0.001, 

model.gbm_all = gbm(formula.all, 
                data = df[train_index, all.vars(formula.all)], 
                distribution = 'gaussian',
                n.trees = max_it,
                shrinkage = 0.001, #0.001
                bag.fraction = 0.8,
                cv.folds = 5, #5
                interaction.depth = 2,#3
                train.fraction = 1.0,
                var.monotone = var.monotone,
                n.cores = 4,
                verbose = FALSE)

#show best iteration
best_it_all = gbm.perf(model.gbm_all, method = 'cv') 
plot_gbmiterations(model.gbm_all)
print(best_it_all)
grid()
pred.gbm_all = exp(predict(model.gbm_all, n.trees = best_it_all, newdata = df)) - 1.0

#show importance
vars.importance_all = summary(model.gbm_all, n.trees = best_it_all, plotit=FALSE) # influence
plot_gbminfluence(vars.importance_all[vars.importance_all$rel.inf>inf_lowlimit,])
kable(vars.importance_all[vars.importance_all$rel.inf>inf_lowlimit,])
kable(vars.importance_all[vars.importance_all$rel.inf<inf_lowlimit,])

imp_vars = as.character(vars.importance_all$var)[vars.importance_all$rel.inf>=.5]

write.csv(vars.importance_all, file.path(working_folder,'SberbankHousing/var.importance.all.csv'))

plots = plot_gbmpartial(model.gbm_all, best_it_all, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow=3, ncol=3)

plots <- llply(names(df) %in_set% imp_vars, function(vname){
  plot_result = plot_profile(log(pred.gbm_all[train_index]+1), log(df$price_doc[train_index]+1), df[train_index, vname], bucket_count = 10, min_obs = 10, error_band ='normal') + ggtitle(vname)
  return (plot_result)
})
marrangeGrob(plots, nrow=3, ncol=3)


print((proc.time() - start_time)[3])

```


## GBM

```{r gbm_cv, fig.width = 8, fig.height = 6, dpi = 150, eval = TRUE, echo=TRUE}
start_time <- proc.time()

allvars =  unique(c(sig_vars, 'cluster_group') %!in_set% c(non_vars, 'sub_area'))

formula.all = formula (paste( 'price_log ~', paste(allvars, collapse = '+')) )

print(formula.all)
print(length(allvars))

var.monotone = rep(0, length(allvars)) #1-increasing, -1 - decreasing, 0: any
var.monotone[allvars %in% c('full_sq','cafe_count_5000', 'usdrub', 'state', 'num_room')] = 1
var.monotone[allvars %in% c('metro_min_walk','metro_min_avto', 'fitness_km')] = -1
#var.monotone[allvars %in% c('full_sq','full_sq_log', 'num_room','state', 'usdrub','eurrub', 'kitch_sq', 'mosque_km', 'life_sq', 'cafe_count_5000', 'ppi')] =  1
#var.monotone[allvars %in% c('metro_min_walk', 'metro_min_avto', 'fitness_km', 'sadovoe_km', 'public_healthcare_km', 'green_zone_km', 'kindergarten_km', 'workplaces_km', 'hospice_morgue_km', 'additional_education_km', 'office_sqm_1500_log')] =  -1

max_it = 10*max_it_mult #64k is for s=0.001, 

model.gbm_cv = gbm(formula.all, 
                data = df[train_index, all.vars(formula.all)], 
                distribution = 'gaussian',
                n.trees = max_it,
                shrinkage = 0.01, #0.001
                bag.fraction = 0.8,
                interaction.depth = 2,#3
                cv.folds = 5, #5
                train.fraction = 1.0,
                var.monotone = var.monotone,
                n.cores = 4,
                verbose = FALSE)
#model.gbm_cv = gbm.more(model.gbm_cv, 3000)
#show best iteration
best_it_cv = gbm.perf(model.gbm_cv, method = 'cv') 
plot_gbmiterations(model.gbm_cv)
print(best_it_cv)
grid()
pred.gbm_cv = exp(predict(model.gbm_cv, n.trees = best_it_cv, newdata = df)) - 1.0


#plot interactions
level2_interactions = gbm_interactions(model.gbm_cv,  df[train_index, all.vars(formula.all)], iter = best_it_cv, 1, 2)
plot_gbminteractions(level2_interactions[level2_interactions$interaction_score>1e-2,])
kable(level2_interactions[level2_interactions$interaction_score>1e-2,])

#show importance
vars.importance_cv = summary(model.gbm_cv, n.trees = best_it_cv, plotit=FALSE) # influence
plot_gbminfluence(vars.importance_cv[vars.importance_cv$rel.inf>inf_lowlimit,])
kable(vars.importance_cv[vars.importance_cv$rel.inf>=inf_lowlimit,])
kable(vars.importance_cv[vars.importance_cv$rel.inf<inf_lowlimit,])

write.csv(vars.importance_cv, file.path(working_folder,'SberbankHousing/var.importance.csv'))

imp_vars = as.character(vars.importance_cv$var)[vars.importance_cv$rel.inf>=.1]

#partial dependence
plots = plot_gbmpartial(model.gbm_cv, best_it_cv, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow=3, ncol=3)

plots = plot_gbmpartial_2d(model.gbm_cv, best_it_cv, as.character(level2_interactions$vars[level2_interactions$interaction_score>0.1]), output_type = 'link')
marrangeGrob(plots, nrow=2, ncol=2)

#profiles (norm) with respect to model vars
for(vname in c('sub_area', 'full_sq', 'sale_time') ){
  print(plot_profile(log(pred.gbm_cv[train_index]+1), log(df$price_doc[train_index]+1), df[train_index, vname], bucket_count = 20, min_obs = 10, error_band ='normal') + ggtitle(vname))
}
plots <- llply(imp_vars %in_set% names(df), function(vname){
  plot_result = plot_profile(log(pred.gbm_cv[train_index]+1), log(df$price_doc[train_index]+1), df[train_index, vname], bucket_count = 10, min_obs = 10, error_band ='normal') + ggtitle(vname)
  return (plot_result)
})
marrangeGrob(plots, nrow=3, ncol=3)

#profiles with respect to vars - not in model
residual_vars =  names(df) %!in_set% c(non_vars, all.vars(formula.all))
plots <- llply(residual_vars %in_set% names(df), function(vname){
  plot_result = plot_profile(log(pred.gbm_cv[train_index]+1), log(df$price_doc[train_index]+1), df[train_index, vname], bucket_count = 10, min_obs = 10, error_band ='normal') + ggtitle(vname)
  return (plot_result)
})
marrangeGrob(plots, nrow=3, ncol=3)



print((proc.time() - start_time)[3]/3600)
```

## Cluster SubArea
There are too many sub_area levels, so we cluster them for better out of sample performance 

```{r cluster_sub_area, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE, echo=FALSE}
#### cluster sub_area
model_predictions = pred.gbm_all

sub_areas_by_sample = ddply(df, .(sub_area, sample), function(x) c(count = length(x$price_log) ))

ggplot(sub_areas_by_sample, aes(reorder(sub_area, count), count, fill = sample )) + geom_bar(stat = "identity") + 
    theme( axis.text.x = element_text(angle = 90, hjust = 1)) 

sub_area_res = data.frame(name = df$sub_area[train_index], residual = df$price_log[train_index]- log(model_predictions[train_index] + 1))

sub_area_agg = ddply(sub_area_res, .(name), function(x) c(count = length(x$residual), avg = mean(x$residual) ))
avg_res = sub_area_agg$avg
names(avg_res) <- sub_area_agg$name

ggplot(sub_area_agg, aes(reorder(name, count), count, fill = avg )) + geom_bar(stat = "identity") + 
    theme( axis.text.x = element_text(angle = 90, hjust = 1)) +  scale_fill_distiller(palette = "Spectral")

d = dist(avg_res)

for(cluster_method in c("ward", "complete", "average")) {
  
  print(cluster_method)
  
  hc = hclust(d, method = cluster_method)
  plot(hc)
  
  for(group_count in seq(3, 10)) {
    group = cutree(hc, k = group_count)
    sub_area_groups = data.frame(name =names(group), group)
    
    sub_area_agg_group = merge(sub_area_agg, sub_area_groups, by.x = 'name', by.y = 'name', all.x = TRUE)
    
    write.csv(sub_area_agg_group, file.path(working_folder,sprintf('SberbankHousing/clusters_%s_%d.csv',cluster_method, group_count) ))
    
    print(ggplot(sub_area_agg_group, aes(reorder(name, avg), avg, fill = factor(group) )) + geom_bar(stat = "identity") + 
      theme( axis.text.x = element_text(angle = 90, hjust = 1)))
    
    print(ggplot(sub_area_agg_group, aes(reorder(name, count), count, fill = factor(group) )) + 
            geom_bar(stat = "identity") + theme( axis.text.x = element_text(angle = 90, hjust = 1)))
    
    print(kable(ddply(sub_area_agg_group, "group", summarise, samples = sum(count), areas = length(count), avg_err = mean(avg))))
    
    #temp = ddply(sub_area_agg_group, "group", function(x) { 
    #  data.frame(group = x$group[1], sub_area = paste(as.character(x$name)[1:3], collapse = ' ') ) })
  }
}
###

files = list.files(path = file.path(working_folder,'SberbankHousing/'), pattern = "cluster*", full = TRUE)

sub_area_cluster_info = foreach(i = seq(files), .combine = rbind) %do% {
  temp = read.csv(files[i])
  
  tokens = unlist(strsplit(basename(files[i]), split='[_.]')) #split string
  temp$cluster_algo = tokens[[2]]
  temp$cluster_count = tokens[[3]]
  temp$cluster_group = factor(paste('group_',temp$group, sep = ''))
  return (temp)
}

write.csv(sub_area_cluster_info, file.path(working_folder,'SberbankHousing/clusters_info.csv') )


```

## GBM: On residuals

```{r gbm_res, fig.width = 8, fig.height = 6, dpi = 150, eval = TRUE, echo=TRUE}
start_time <- proc.time()

residual_vars =  names(df) %!in_set% c(non_vars, all.vars(formula.all))

formula.residual = formula (paste( 'price_residual ~', paste(residual_vars, collapse = '+')) )

print(formula.residual)

df_res = df[, residual_vars]
df_res$price_residual = df$price_log - log(pred.gbm_cv+1)

max_it = 20*max_it_mult

model.gbm_res = gbm(formula.residual, 
                data = df_res[train_index, all.vars(formula.residual)], 
                distribution = 'gaussian',
                n.trees = max_it,
                shrinkage = 0.001, #0.001
                bag.fraction = 0.5,
                interaction.depth = 2,#3
                cv.folds = 5, #5
                train.fraction = 1.0,
                n.cores = 4,
                verbose = FALSE)
#model.gbm_cv = gbm.more(model.gbm_cv, 3000)
#show best iteration
best_it_res = gbm.perf(model.gbm_res, method = 'cv') 
print(best_it_res)
grid()
pred.gbm_res = predict(model.gbm_res, n.trees = best_it_res, newdata = df_res)

vars.importance_res = summary(model.gbm_res, n.trees = best_it_res, plotit=FALSE) # influence
plot_gbminfluence(vars.importance_res[vars.importance_res$rel.inf>1e-2,])
kable(vars.importance_res[vars.importance_res$rel.inf>=1e-2,])

imp_vars = as.character(vars.importance_res$var)[vars.importance_res$rel.inf>=1e-2]

plots = plot_gbmpartial(model.gbm_res, best_it_res, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow=3, ncol=3)

plots <- llply(imp_vars %in_set% names(df_res), function(vname){
  plot_result = plot_profile(pred.gbm_res[train_index],df_res$price_residual[train_index] , df_res[train_index, vname], bucket_count = 10, min_obs = 10, error_band ='normal') + ggtitle(vname)
  return (plot_result)
})
marrangeGrob(plots, nrow=3, ncol=3)

print((proc.time() - start_time)[3]/3600)

```

## Summary
```{r res_print, fig.width = 8, fig.height = 6, dpi = 150, eval = TRUE, echo=TRUE}

# Solution  ---- 
results = list()
results$gbm_cv = pred.gbm_cv

res = ldply(results, .id = 'model', function(x) {
  c(rms_log = rms_log(1e3*df$price_doc[train_index],  1e3*x[train_index]),
    na_count = sum(is.na(x[test_index])))
})
print(res)
#
#

for (model_name in names(results) ) {
  plot_df = data.frame(actual = df$price_doc[train_index], model = results[[model_name]][train_index])
  plot_df$error = plot_df$actual - plot_df$model
  p1 = ggplot(plot_df, aes(model, actual)) + geom_point(size = 0.2) + geom_smooth() + geom_abline(slope = 1, color = 'red')
  p2 = ggplot(plot_df, aes(log(model+1), log(actual+1))) + geom_point(size = 0.2) + geom_smooth() + geom_abline(slope = 1, color = 'red')
  print(grid.arrange(p1, p2, ncol = 2))
}


## print solution ---- 
for (model_name in names(results) ){
  submit <- data.frame(id = as.integer( as.numeric(df$id[test_index]) ), price_doc = 1e3*results[[model_name]][test_index])
  submit = submit[order(submit$id),]
  file = file.path(working_folder, sprintf("SberbankHousing/test_sample_%s.csv", model_name))
  write.csv(submit, file = file, row.names = FALSE)
  #zip(paste(file, '.zip', sep = ''), file, flags = "-9jX")
  print(file)
}

## print insample ---- 
for (model_name in names(results) ){
  submit <- data.frame(id = as.integer( as.numeric(df$id) ), price_mdl = 1e3*results[[model_name]], price_act = 1e3*df$price_doc)
  submit = submit[order(submit$id),]
  file = file.path(working_folder, sprintf("SberbankHousing/train_sample_%s.csv", model_name))
  write.csv(submit, file = file, row.names = FALSE)
  #zip(paste(file, '.zip', sep = ''), file, flags = "-9jX")
  print(file)
}


temp = rbind(data.frame(price = df$price_doc, tag = 'actual'), data.frame(price = pred.gbm_cv, tag = 'predicted'))
ggplot(temp, aes(1e-3*price, group = tag, color = tag)   ) + 
  geom_density(adjust = 0.5)

ggplot(temp, aes(log(price + 1 ), group = tag, color = tag)   ) + 
  geom_density(adjust = 0.5)

ggplot(data.frame(price = log(df$price_doc[train_index]+1), error = log(pred.gbm_cv[train_index]+1) - log(df$price_doc[train_index]+1) ), aes(price, error)   ) + 
  geom_point(size = 0.2)

```

## Custom Correction
```{r bias_corr, fig.width = 8, fig.height = 6, dpi = 150, eval = FALSE, echo=TRUE}

  #0.34798 - base
  #0.34553 - 0.1 up
  #0.37786 - 0.1 dn
  file = file.path(working_folder, "SberbankHousing/my_solution_fullfit_gbm_cv_ex.csv")

  solution = read.csv(file = file)
  
  shift = -0.1
  solution_up = solution
  solution_up$price_doc = exp(  log(solution$price_doc + 1.0) + shift) - 1.0
  
  write.csv(solution_up, file = file.path(working_folder, "SberbankHousing/my_solution_fullfit_gbm_cv_ex_up.csv"), row.names = FALSE)
  #zip(paste(file, '.zip', sep = ''), file, flags = "-9jX")
  print(file)

```
