---
title: "Porto Seguro’s Safe Driver Prediction"
author: "Alex"
date: "February 11, 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(gbm)
library(data.table)
library(plyr)
library(stringi)
library(ggplot2)
library(gridExtra)
library(zip)
library(xgboost)

working_folder = 'C:/Dev/Kaggle/'

source(file.path(working_folder, '/Utils/common.R'))


```

## Data

```{r data}

load_data <- function(working_folder){
  
  data_file = file.path(working_folder,'SafeDriverPrediction/data/all_data.rds')

  if(file.exists(data_file)){
    df = readRDS(data_file)
  }else {
    df_train = fread(file.path(working_folder,'SafeDriverPrediction/data/train.csv') )
    df_test  = fread(file.path(working_folder,'SafeDriverPrediction/data/test.csv') )
    
    df_test[, target:=NA]
    
    df = rbind(df_test, df_train)
    
    #repalce -1 with NA
    for (c_name in names(df)) {
      set(df,which(df[[c_name]]==-1),c_name,NA)
    }
    
    #convert to factors
    cat_vars = names(df)[grepl('(_cat)|(_bin)',names(df))]
    df[, (cat_vars):=lapply(.SD, as.factor), .SDcols = cat_vars]
  
    
    saveRDS(df, data_file)
  }
   return (df)
}

df = load_data(working_folder)

#head(df)

test_index = is.na(df$target)
train_index = !is.na(df$target)

results = list()
```

## Data View

```{r data_view}
obj_var = 'target'
#var source: ind, reg, car, calc
#var types: bin, cat

c_names = names(df)
cat_vars = c_names[grepl('_cat',c_names)]

df_train = df[train_index,]

#sapply(df, levels)

#plot cat var counts
plots = llply(cat_vars, function(var_name) {
 p = ggplot(df_train, aes_string(var_name)) + geom_bar()  +
    ggtitle(var_name)
 return( p )
})
marrangeGrob(plots, nrow = 3, ncol = 3, top = NULL)

ggplot(df_train, aes(ps_car_11_cat)) + geom_bar()# a lot of levels

#outcome based sampling
set.seed(101)

n_events = sum(df$target[train_index] == 1)
n_non_events = sum(df$target[train_index] == 0)
n_total = length(df$target[train_index] )
non_event_sample = sample.int(n_non_events,n_events,replace = FALSE)
sampling_correction = log(n_events/(n_total - n_events))
train_index_small = train_index
train_index_small = (df$target==1) & !is.na(df$target)
train_index_small[ which(df$target==0)[non_event_sample]] = TRUE

table(df$target[train_index])
table(df$target[train_index_small])

print(sprintf('%d (p = %.4f)', n_events, 100*n_events/n_total))

```

## GBM Model: All vars
AUC = 0.6353 (0.29154 - best score)
current: 0.27507 0.26964
```{r gbm_model1}

actual = as.numeric(df$target)

all_vars = names(df) %!in_set% c('id', 'target')
exclude_vars = c('ps_car_11_cat', 
                 'ps_ind_11_bin','ps_ind_10_bin','ps_ind_13_bin','ps_car_02_cat', 'ps_calc_20_bin','ps_calc_20_bin','ps_calc_18_bin','ps_ind_12_bin','ps_ind_14','ps_calc_19_bin','ps_car_10_cat', 'ps_calc_17_bin', 'ps_ind_18_bin','ps_calc_15_bin', 'ps_calc_16_bin', 'ps_car_08_cat','ps_calc_03','ps_calc_01', 'ps_calc_09','ps_calc_12') #inf <0.1

set.seed(1012356)

formula.gbm = formula(stri_join( 'target ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('target')
var.monotone = rep(0, length(model_vars))
var.monotone[model_vars %in% c('ps_car_13', 'ps_reg_03', 'ps_reg_02', 'ps_reg_01', 'ps_car_15')]  = 1
var.monotone[model_vars %in% c('ps_ind_15', 'ps_car_14')]  =  -1

model.gbm  = gbm(formula.gbm,
                 distribution = "bernoulli",
                 n.trees = 10000,
                 cv.folds = 0,
                 shrinkage = 0.002,
                 interaction.depth=4,
                 train.fraction = 0.7,
                 bag.fraction = 0.7,
                 n.cores = 2,
                 var.monotone = var.monotone,
                 data = df[train_index_small, all.vars(formula.gbm), with = F],
                 verbose = FALSE)

plot_gbmiterations(model.gbm)
best_it.gbm = gbm.perf(model.gbm, plot.it = F)

pred.gbm = predict(model.gbm, n.trees = best_it.gbm, type = 'response')

#influence
var_inf = summary(model.gbm, n.trees = best_it.gbm, plotit = F)
plot_gbminfluence(var_inf)
print(var_inf)

#interactions
var_interaction = gbm_interactions(model.gbm, df[train_index_small,], iter = best_it.gbm, min_influence = 1, degree = 2) 
plot_gbminteractions(subset(var_interaction, interaction_score>0.05))
print(var_interaction)

plots = plot_gbmpartial_2d(model.gbm, best_it.gbm, as.character(subset(var_interaction,interaction_score>0.1)$vars), output_type = 'response')
marrangeGrob(plots, nrow = 2, ncol = 2, top = NULL)

plot_binmodel_predictions(actual[train_index_small], pred.gbm)

plots = plot_gbmpartial(model.gbm, best_it.gbm, as.character(var_inf$var[var_inf$rel.inf>1.0]), output_type = 'response')
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plots = llply(all_vars, function(var_name) {
  p = plot_profile(pred.gbm, actual[train_index_small],df[[var_name]][train_index_small], error_band = 'binom') +
    ggtitle(var_name)
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)
#roc.default(response = test$Class, predictor = gbm.test, plot = TRUE,     col = "red")

#save solution
pred.gbm_link = predict(model.gbm, n.trees = best_it.gbm, newdata = df, type = 'link')
pred.gbm_full = 1.0 / (1.0 + exp(-pred.gbm_link - sampling_correction))

results$gbm1 = pred.gbm_full

#var not in model
plots = llply(exclude_vars %in_set% names(df), function(var_name) {
  p = plot_profile(pred.gbm_full[train_index], actual[train_index],df[[var_name]][train_index], error_band = 'binom') +
    ggtitle(var_name)
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

p1 = plot_binmodel_percentiles(actual[train_index], pred.gbm_full[train_index], n = 10, equal_count_buckets = T)
p2 = plot_binmodel_percentiles(actual[train_index], pred.gbm_full[train_index], n = 10, equal_count_buckets = F)
grid.arrange(p1,p2)
```

## XGBoost Model:

```{r xgb_model, eval = FALSE}
imp_vars = c('ps_car_13','ps_car_06_cat','ps_ind_05_cat','ps_reg_03')

dfs = df[train_index_small, all.vars(formula.gbm), with = F]
dfs <- sapply( dfs, as.numeric )

dtrain <- xgb.DMatrix(as.matrix(dfs), label = as.numeric(df[[obj_var]][train_index_small]) )

param <- list(
  train = dtrain,
  max_depth = 3, 
              eta = 0.002, 
              silent = 1, 
              nthread = 2,
              subsample = 0.7,
              objective = "binary:logistic",
              eval_metric = "auc",
              monotone_constraints = var.monotone)

bst <- xgb.train(param, dtrain, nrounds = 10)

```


## Output File

```{r output}

for (model_name in names(results) ){
  submit <- data.table(id = df$id[test_index], 
                       target = results[[model_name]][test_index])
  
  submit = submit[order(submit$id),]
  
  file = file.path(working_folder, sprintf("SafeDriverPrediction/my_solution_%s.csv", model_name))
  
  fwrite(submit, file = file, row.names = FALSE)
  
  #utils::zip(paste(file, '.zip', sep = ''), file, flags = "-r9X")
  zip(paste(file, '.zip', sep = ''), file)
  
  print(file)
}
```

