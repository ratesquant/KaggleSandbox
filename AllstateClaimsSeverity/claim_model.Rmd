---
title: "Allstate Claims Severity"
author: "Alex"
date: "October 13, 2016"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(reshape2)
library(ggplot2)
library(GGally)
library(Hmisc)
library(plyr)
library(gridExtra)

library(gbm)
library(np)
library(earth)
library(rpart)
library(randomForest)
library(nnet)
library(e1071)

random_seed = 123

```

## Load Data

```{r load_data}
vars_cat = 116
vars_num = 14

col_classes = c('numeric', rep('factor', vars_cat), rep('numeric', vars_num), 'numeric')

train <- read.csv('C:/Dev/Kaggle/AllstateClaimsSeverity/input/train.csv', colClasses = col_classes)
test  <- read.csv('C:/Dev/Kaggle/AllstateClaimsSeverity/input/test.csv', colClasses = col_classes[-length(col_classes)])

test$loss <- NA
train$tag = 1
test$tag = 0

df = rbind(train, test)

df$log_loss = log(df$loss) 

test_index = df$tag == 0
train_index = df$tag == 1

#create a smaller samples for research
n = dim(df)[1]
train_ids = which(df$tag==1)
ntrain = length(train_ids)
train_index1 = rep(FALSE, n)
train_index2 = rep(FALSE, n)
train_index3 = rep(FALSE, n)
train_index1[sample(train_ids, 0.1*ntrain)] = TRUE #10% of training data
train_index2[sample(train_ids, 0.5*ntrain)] = TRUE #50% of training data
train_index3 = !train_index2 & train_index         #out of sample, sample 3 and 2 partition data

#188318,1074

#combine rare levels for categorical variables 
#sapply(df, class)
#sort(table(df$75))
min_count = 1000
tag_name = "OTHER"
for(cn in cat_vars){
  temp = df[,cn]
  temp = as.character(temp)
  temp[temp %in% names(which(table(temp)<min_count))] = tag_name
  
  #spread observations among other factors 
  other_count = sum(temp==tag_name)
  if(other_count<min_count){ 
    level_counts = table(temp[temp != tag_name])
    levels = sample(names(level_counts), other_count, replace = TRUE, prob = level_counts/sum(level_counts))
    temp[temp==tag_name] <- levels
  }
  df[,cn] = factor(temp)
}

#remove variables with one level
cat_vars_fake = names(which( sapply(df[,cat_vars], function(x) length(levels(x)) ) == 1))

#define variables
cols = names(df)
cat_vars = setdiff( cols[grep("cat", cols)], cat_vars_fake) 
cont_vars = cols[grep("cont", cols)]
allvars = union ( cat_vars , cont_vars) 
formula.all = formula (paste( 'log_loss ~', paste(allvars, collapse = '+')) )
formula.cont = formula (paste( 'log_loss ~', paste(cont_vars, collapse = '+')) )
formula.cat = formula (paste( 'log_loss ~', paste(cat_vars, collapse = '+')) )

#sapply(df[,cont_vars], sd)
#sapply(df[,cat_vars], function(x) length(levels(x)) )

```

```{r functions}

r_sqr <-function(y, x) {
  return( summary(lm(y ~ x))$r.squared )
}
#plot missing values
ggplot_missing <- function(x){
  mx = melt(is.na(x))
  ggplot(mx, aes(Var2, Var1)) + geom_raster(aes(fill = value)) +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  scale_fill_grey(name = "", labels = c("Valid","NA")) +
  labs(x = "Variable name",   y = "Rows") + 
    ggtitle (paste('total number of missing values:',  sum(mx$value)))
}

#plot number of missing values
ggplot_missing_count <- function(x){
  mc = adply(is.na(x), 2, sum)
  names(mc) <- c('name', 'value')
  ggplot(mc, aes(name, value)) + geom_bar(stat = "identity") +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  labs(x = "Variable name",   y = "Missing Variables")
}

# Friedman's H-statistic to assess the relative strength of interaction effects in non-linear models. H is on the scale of [0-1] 
gbm_interactions <- function(gbm_model, data, min_influence = 1, degree = 2){
  gbm_summary = summary(gbm_model, plotit=FALSE)
  vars = gbm_summary$var[gbm_summary$rel.inf > min_influence]
  all_combinations = combn(as.vector(vars), degree, simplify = TRUE)
  df = ldply(seq(dim(all_combinations)[2]), function(i) {
    data.frame(vars = paste(all_combinations[,i], collapse = '-'), 
               interaction_score = interact.gbm(gbm_model, data, all_combinations[,i])) 
    })
  return ( df[order(df$interaction_score, decreasing = TRUE),] )
}
```

## Overview
```{r overview, fig.width = 8, fig.height = 6, dpi = 100}

ggplot(df[train_index,], aes(sample = log_loss )) + stat_qq()
ggplot(df[train_index,], aes(loss, color = cat116)) + stat_ecdf()

#plot
ggplot(df[train_index,], aes(x = loss, fill = cat1)) +stat_density()
ggplot(df[train_index,], aes(x = cat75, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df, aes(cat75, fill = factor(tag) )) + geom_bar() 
```


## Loss Prediction Models

### Recursive Partitioning
```{r rpart, fig.width = 8, fig.height = 6, dpi = 150}
model.rp = rpart(Survived ~ Sex + Pclass + Siblings + Fare + Title + CabinNumber + surv_family_count + died_family_count,
                 data = full_df[train_index,], 
                 control = rpart.control(cp = 0.01, minsplit = 20))

printcp(model.rp)

par(mfrow=c(2,1))
plotcp(model.rp) 
plot(model.rp, uniform=TRUE) 
text(model.rp, use.n=TRUE, all=TRUE, cex=.8)

pred.rp = predict(model.rp, newdata = full_df)

cat(paste('rpart: ', model_ccr(Survived, pred.rp[train_index])))
plot_binmodel_predictions(Survived, pred.rp[train_index])

```

### Random Forest
```{r rf, fig.width = 8, fig.height = 6, dpi = 100}
model.rf <- randomForest(Surv ~ Sex +  Pclass + Siblings + Fare + Title + CabinNumber + surv_family_count + died_family_count, data =  full_df[train_index,])

pred.rf = to_numeric(predict(model.rf, newdata = full_df))

importance(model.rf)

cat(paste('random forest:', model_ccr(Survived, pred.rf[train_index])))
plot_binmodel_predictions(Survived, pred.rf[train_index])
```

### Linear Regression
r2 = 0.5148
```{r lm, fig.width = 8, fig.height = 6, dpi = 100}
model.lm <- lm(formula.all, data = df[train_index2, all.vars(formula.all)])

summary(model.lm)
pred.lm = predict(model.lm, newdata = df)

cat(paste('lm (full/in/out):', 
          round(r_sqr(df$log_loss[train_index],   pred.lm[train_index]), 4), 
          round(r_sqr(df$log_loss[train_index2],  pred.lm[train_index2]), 4),
          round(r_sqr(df$log_loss[train_index3],  pred.lm[train_index3]), 4) ))

```


### N-Networks
```{r nnet, fig.width = 8, fig.height = 6, dpi = 100}
set.seed(random_seed)
model.nnet <- nnet(Survived ~ Sex + Age + Pclass + Siblings + Parch + Embarked + Fare, 
                   data = full_df[train_index,], size=6, maxit = 1000, trace=FALSE, decay = 1e-4)
pred.nnet <- predict(model.nnet, newdata = full_df)

print(model.nnet)

cat(paste('Neural Networks:', model_ccr(Survived, pred.nnet[train_index])))
plot_binmodel_predictions(Survived, pred.nnet[train_index])
```

### Multivariate Adaptive Regression Splines
```{r mars, fig.width = 8, fig.height = 6, dpi = 100}
set.seed(random_seed)

allvars = setdiff ( names(df), c('log_loss', 'loss', 'id', 'tag') ) 
mars_formula = formula (paste( 'log_loss ~', paste(allvars, collapse = '+')) )

model.mars <- earth(mars_formula, 
                    data = df[train_index2, all.vars(mars_formula)], 
                    degree = 2, nfold = 3, trace = 3)

plot(model.mars)
summary(model.mars)
par(mfrow=c(1,1))
plot(evimp(model.mars))
#plotmo(model.mars)

#0.495587497528404
pred.mars = as.vector(predict(model.mars, newdata = df))

cat(paste('MARS:', r_sqr(df$log_loss[train_index],  pred.mars[train_index]) ))
cat(paste('MARS:', r_sqr(df$log_loss[train_index1],  pred.mars[train_index1]) ))
cat(paste('MARS:', r_sqr(df$log_loss[train_index2],  pred.mars[train_index2]) ))
cat(paste('MARS:', r_sqr(df$log_loss[train_index3],  pred.mars[train_index3]) ))
```


### Kernel Regression
```{r np, fig.width = 8, fig.height = 6, dpi = 100, eval = FALSE}
#precompute scale factors
#model.npbw <- npcdensbw(Surv ~ Sex + Age + Pclass + SiblingsOrdered + ParchOrdered + Embarked + Fare, data =  full_df[train_index,]) 
#model.npbw$sfactor$x
bw_scales = c(0.07187361, 3.278909e-06, 3.442791e+00, 6.214479e-01, 4.397527e+00, 4.179354e+00, 3.655486e+00, 3.271476e+00)
bw_scales[8] = 4 * bw_scales[8] # increase scale factor for fare to avoid overfitting

np_formula = formula(Surv ~ Sex + Age + Pclass + SiblingsOrdered + ParchOrdered + Embarked + Fare)
model.npbw = npcdensbw(Surv ~ Sex + Age + Pclass + SiblingsOrdered + ParchOrdered + Embarked + Fare, 
          data =  full_df[train_index,all.vars(np_formula)], 
          bandwidth.compute = FALSE,
          bws = bw_scales,
          bwscaling = TRUE) 

plot(model.npbw)
summary(model.npbw)

#do kernel regression
model.np <- npconmode(model.npbw)
summary(model.np)

#pred.np = model.np$condens
pred.np =  to_numeric( npconmode(model.npbw, newdata = full_df[,all.vars(np_formula)[-1]])$conmode )

#looks like there is a bug in np 
#missing_index = (length(pred.np)+1):dim(full_df)[1]
#pred.np[missing_index] = npconmode(model.npbw, newdata = full_df[missing_index,all.vars(np_formula)[-1]])$conmode

cat(paste('np:', model_ccr(Survived, pred.np[train_index] )))
plot_binmodel_predictions(Survived, pred.np[train_index])

```


### GBM
```{r gbm, fig.width = 8, fig.height = 6, dpi = 150}
fml_name = as.formula(Survived ~ Pclass + Fare + Title + CabinNumber  + surv_family_count + died_family_count)
#fml = as.formula(Surv ~ Sex + Age + Pclass + Siblings + Parch + Embarked + Fare)
set.seed(random_seed)

ntrees = 4000
model.gbm = gbm(fml_name, 
                data = full_df[train_index, all.vars(fml_name)], 
                distribution = 'bernoulli',
                n.trees = ntrees,
                shrinkage = 0.005,
                bag.fraction = 1.0,
                interaction.depth = 2,
                cv.folds = 3)
print(model.gbm)

par(mfrow = c(1, 2), las = 1)
best_it_name = gbm.perf(model.gbm, method = 'cv')
grid()
summary(model.gbm, n.trees = best_it_name) # influence
grid()

par(mfrow=c(4,3))
for(i in seq(length(all.vars(fml_name)) - 1)) {
  plot.gbm(model.gbm, n.trees =best_it_name,  i = i)
  grid()
}

cat('Two-way interactions') 
gbm_interactions(model.gbm, full_df[train_index,], 1, 2)

pred.gbm = predict(model.gbm, n.trees = best_it_name, newdata = full_df, type = 'response')

cat(paste("gbm ccr:", model_ccr(Survived, pred.gbm[train_index])))
plot_binmodel_predictions(Survived, pred.gbm[train_index])
```

### SVM 
```{r svm, fig.width = 8, fig.height = 6, dpi = 150, eval = TRUE}
svn_formula = formula(Surv ~ Sex + Age + Pclass + Siblings + Parch + Embarked + Fare + CabinNumber + surv_family_count + died_family_count)
model.svm <- svm(svn_formula, data=full_df[train_index,all.vars(svn_formula)], cost = 1, gamma = 0.125)
summary(model.svm)

#svm_tune <- tune(svm, train.x=svn_formula,  data=full_df[train_index,],
#                 kernel="radial", 
#                 ranges=list(cost=2^(-6:6), gamma=2^(-6:6)))
#print(svm_tune)


pred.svm = to_numeric(predict(model.svm, newdata = full_df))

cat(paste("svm ccr:", model_ccr(Survived, pred.svm[train_index])))
plot_binmodel_predictions(Survived, pred.svm[train_index])

```


## Compare
```{r compare, fig.width = 8, fig.height = 6, dpi = 100}

results = list()
results$mars =pred.mars

#results$np = pred.np
#results$glm = pred.glm
#results$glm_s = pred.glm_s
#results$gbm = pred.gbm 
#results$nnet = pred.nnet
#results$svm = pred.svm
#results$rf = pred.rf
#results$rp = pred.rp


res = ldply(results, .id = 'model', function(x) {
  c(r2_out = r_sqr(df$log_loss[train_index3],  x[train_index3]),
    r2_in = r_sqr(df$log_loss[train_index2],  x[train_index2]),
    r2_sm = r_sqr(df$log_loss[train_index1],  x[train_index1]),
    na_count = sum(is.na(x[test_index])))
})

print(res)

ggplot(res, aes(model, r2_out)) + geom_bar(stat = "identity") + coord_flip() + coord_cartesian(ylim = c(0.9*min(res$r2_out), 1.0))

```

## Submit
```{r submit, fig.width = 8, fig.height = 6, dpi = 100}

folder = "C:/Dev/Kaggle/AllstateClaimsSeverity/"

for (model_name in names(results) ){
  submit <- data.frame(id = df$id[test_index], 
                       loss = exp(results[[model_name]][test_index]))
  
  submit = submit[order(submit$id),]
  
  file = file.path(folder, sprintf("my_solution_%s.csv", model_name))
  
  write.csv(submit, file = file, row.names = FALSE)
}


```
