---
title: "Allstate Claims Severity"
author: "Alex"
date: "October 13, 2016"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

library(reshape2)
library(ggplot2)
library(GGally)
library(Hmisc)
library(plyr)
library(gridExtra)
library(corrplot)

library(gbm)
library(np)
library(earth)
library(rpart)
library(randomForest)
library(nnet)
library(e1071)
library(MASS)

```

## Load Data

```{r load_data}

random_seed = 12345678

folder = 'C:/Dev/Kaggle/AllstateClaimsSeverity/'

source(file.path(folder, 'claim_model_utils.R'))

df = load_claim_data(file.path(folder, 'input'), min_count = 100, max_cat_levels = 7, scale_by_count = FALSE)

test_index = df$tag == 0
train_index = df$tag == 1

print(table(df$tag))

cols = names(df)
cat_vars = cols[grep("cat", cols)] 
factor_vars = cols[sapply(df, class) == 'factor']
#remove variables with one level
cat_vars_single_level = names(which(sapply(df[train_index, factor_vars ], 
                                    FUN = function(x) length(levels(x))) <= 1))

#define variables 
cat_vars = setdiff(cat_vars, cat_vars_single_level)
cont_vars = cols[grep("cont", cols)]
allvars = union ( cat_vars , cont_vars) 
formula.loss = formula (paste( 'loss ~', paste(allvars, collapse = '+')) )
formula.all = formula (paste( 'log_loss ~', paste(allvars, collapse = '+')) )
formula.cont = formula (paste( 'log_loss ~', paste(cont_vars, collapse = '+')) )
formula.cat = formula (paste( 'log_loss ~', paste(cat_vars, collapse = '+')) )


#useless variables less then 0.1%
least_significant = c('cat40','cont9','cat10','cat50','cat78','cont5','cat9','cat89','cat14','cat33','cat47',
'cat95','cat77','cat8','cat45','cat54','cat17','cat61','cat74','cat66','cat24','cat18','cat43','cat30',
'cat46','cat28','cat29','cat88','cat92','cat97','cat98','cont4')
least_significant = c('') # dont exclude variables

formula.all_sig = formula (paste( 'log_loss ~', paste(allvars[!allvars %in% least_significant], collapse = '+')) )

#sapply(df[,cont_vars], sd)
#sapply(df[,cat_vars], function(x) length(levels(x)) )

```

```{r functions}

r_sqr <-function(y, x) {
  return( summary(lm(y ~ x))$r.squared )
}
#plot missing values
ggplot_missing <- function(x){
  mx = melt(is.na(x))
  ggplot(mx, aes(Var2, Var1)) + geom_raster(aes(fill = value)) +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  scale_fill_grey(name = "", labels = c("Valid","NA")) +
  labs(x = "Variable name",   y = "Rows") + 
    ggtitle (paste('total number of missing values:',  sum(mx$value)))
}

#plot number of missing values
ggplot_missing_count <- function(x){
  mc = adply(is.na(x), 2, sum)
  names(mc) <- c('name', 'value')
  ggplot(mc, aes(name, value)) + geom_bar(stat = "identity") +
  theme(axis.text.x  = element_text(angle=90, vjust=0.5)) + 
  labs(x = "Variable name",   y = "Missing Variables")
}

# Friedman's H-statistic to assess the relative strength of interaction effects in non-linear models. H is on the scale of [0-1] 
gbm_interactions <- function(gbm_model, data, min_influence = 1, degree = 2){
  gbm_summary = summary(gbm_model, plotit=FALSE)
  vars = gbm_summary$var[gbm_summary$rel.inf > min_influence]
  all_combinations = combn(as.vector(vars), degree, simplify = TRUE)
  df = ldply(seq(dim(all_combinations)[2]), function(i) {
    data.frame(vars = paste(all_combinations[,i], collapse = '-'), 
               interaction_score = interact.gbm(gbm_model, data, all_combinations[,i])) 
    })
  return ( df[order(df$interaction_score, decreasing = TRUE),] )
}
```

## Overview
```{r overview, fig.width = 8, fig.height = 6, dpi = 100}

ggplot(df[train_index,], aes(sample = log_loss )) + stat_qq()
ggplot(df[train_index,], aes(log_loss)) + stat_density()

#plot
#cat80, cat79, cat101, cat100, cat12

ggplot(df[train_index,], aes(x = loss, fill = cat1)) +stat_density()
ggplot(df[train_index,], aes(x = log_loss, fill = cat80)) +stat_density()
ggplot(df[train_index,], aes(x = log_loss, fill = cat103)) +stat_density()
ggplot(df[train_index,], aes(x = log_loss, fill = cat111)) +stat_density()

ggplot(df[train_index,], aes(x = cat80, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat79, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat101, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat100, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat116, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat112, log_loss)) + geom_boxplot() + coord_flip() #has other level
ggplot(df[train_index,], aes(x = cat12, log_loss)) + geom_boxplot() + coord_flip()
ggplot(df[train_index,], aes(x = cat81, log_loss)) + geom_boxplot() + coord_flip()

ggplot(df, aes(cat80, fill = factor(tag) )) + geom_bar()
ggplot(df, aes(cat79, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat101, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat100, fill = factor(tag) )) + geom_bar()
ggplot(df, aes(cat112, fill = factor(tag) )) + geom_bar() 
ggplot(df, aes(cat12, fill = factor(tag) )) + geom_bar() 

table(df$cat15)

#plot correlation
con_corr = cor(df[train_index,cont_vars])
corrplot(con_corr, method="ellipse")
corrplot.mixed(con_corr, lower="number", upper="ellipse")

```


## Loss Prediction Models
top 20 variables explain about 90% of variance
top 43 variables explain about 98% of variance

### Random Forest
```{r rf, fig.width = 8, fig.height = 6, dpi = 100, eval = FALSE}

formula.rf = formula.all

model.rf <- randomForest(formula.rf, 
                             data =df[train_index, all.vars(formula.rf)],
                             ntree=500)
  
summary(model.rf)
  
pred.rf = predict(model.rf, newdata = df)
  
cat(paste('rf (in/out):', 
          round(r_sqr(df$log_loss[train_index],  pred.rf[train_index]), 4),
          sum(is.na(pred.lm)) ))
```

### Linear Regression
lm (in/out): 0.519 0.5184 0
```{r lm, fig.width = 8, fig.height = 6, dpi = 100}
model.lm <- lm(formula.all, data = df[train_index, all.vars(formula.all)])

summary(model.lm)
pred.lm = predict(model.lm, newdata = df)

cat(paste('lm (in/out):', 
          round(r_sqr(df$log_loss[train_index],  pred.lm[train_index]), 4),
          sum(is.na(pred.lm)) ))
```

### Linear Regression: Segmented
lm (in/out): 0.5183 0.5135 0 (1244.69108)
```{r lm_seg, fig.width = 8, fig.height = 6, dpi = 100}
#table(df$cat80)

formula.lm = update(formula.all, '~. -cat80')

for (cat80_level in levels(df$cat80))
{
  print(cat80_level)
  seg_index = train_index & df$cat80 == cat80_level
  
  vars_not_in_train = names(which(sapply(df[,factor_vars], FUN = function(x){
    train_value = unique(x[train_index & df$cat80 == cat80_level])
    test_value = unique(x[test_index & df$cat80 == cat80_level])
    length(setdiff(test_value, train_value))
    }  ) >0 ))
  
  #variables with one level
  vars_to_remove = names(which(sapply(df[seg_index,factor_vars], FUN = function(x) length(unique(x))) <= 1))
  vars_to_remove = union(vars_to_remove, vars_not_in_train)
  vars_to_remove = vars_to_remove[vars_to_remove %in% all.vars(formula.lm)]
  
  print(vars_to_remove)
  
  if(length(vars_to_remove) == 0 ) {
    formula.lm_seg = formula.lm
  }else {
    formula.lm_seg = update(formula.lm, paste('~. -', paste(vars_to_remove, collapse = '-')) )
  }
  
  model.lm_seg <- lm(formula.lm_seg, data = df[seg_index, all.vars(formula.lm_seg)])
  
  summary(model.lm_seg)
  
  pred.lm[df$cat80 == cat80_level] = predict(model.lm_seg, newdata = df[df$cat80 == cat80_level,])
  
  cat(paste('lm seg (in/out):', cat80_level, 
          round(r_sqr(df$log_loss[train_index & df$cat80 == cat80_level],  
                      pred.lm[train_index & df$cat80 == cat80_level]), 4),
          sum(is.na(pred.lm[df$cat80 == cat80_level])) ))
  
}

cat(paste('lm seg (in/out):', 
          round(r_sqr(df$log_loss[train_index],  pred.lm[train_index]), 4),
          sum(is.na(pred.lm)) ))
```

```{r error_buckets, fig.width = 8, fig.height = 6, dpi = 150}
#important factors with many levels

print(sort(sapply(df[, cat_vars], FUN = function(x) length(levels(x)))))

#cat107 cat109 cat113 cat110 cat112 cat116 
#   16     22     43     49     50    125 

df$log_loss_err = df$log_loss - pred.lm

ggplot(df[train_index,], aes(sample = log_loss_err )) + stat_qq()

ggplot(df[train_index,], aes(x = cat116, log_loss_err)) + 
  geom_boxplot() + coord_flip() + geom_hline(yintercept = 0, color = 'red')

ggplot(df[train_index,], aes(x = cat112, log_loss_err)) + 
  geom_boxplot() + coord_flip() + geom_hline(yintercept = 0, color = 'red')

ggplot(df[train_index,], aes(x = cat110, log_loss_err)) + 
  geom_boxplot() + coord_flip() + geom_hline(yintercept = 0, color = 'red')

ggplot(df[train_index,], aes(x = cat113, log_loss_err)) + 
  geom_boxplot() + coord_flip() + geom_hline(yintercept = 0, color = 'red')

ggplot(df[train_index,], aes(x = cat100, log_loss_err)) + 
  geom_boxplot() + coord_flip() + geom_hline(yintercept = 0, color = 'red')

ggplot(df, aes(cat116, fill = factor(tag) )) + geom_bar() +  coord_flip() + facet_grid(.~cat80)
ggplot(df, aes(cat112, fill = factor(tag) )) + geom_bar() +  coord_flip() + facet_grid(.~cat80)
ggplot(df, aes(cat110, fill = factor(tag) )) + geom_bar() +  coord_flip() + facet_grid(.~cat80)

```

### GBM: Segmented
best so far 0.5718054  (0.001, cvfolds = 3, depth = 3)
A 0.384 0 1335"
B 0.504 0 77274"
C 0.395 0 5911"
D 0.4172 0 229344"
```{r gbm, fig.width = 8, fig.height = 6, dpi = 150}
set.seed(random_seed)
formula.gbm = update(formula.all, '~. -cat80')

print(formula.gbm)

pred.gbm = rep(NA, length(df$loss))

max_it = data.frame(level = levels(df$cat80), it = c(7, 100, 20, 100)*1000)

for (cat80_level in levels(df$cat80))
{
  #cat80_level = levels(df$cat80)[1]
  print(cat80_level)
  seg_index = train_index & df$cat80 == cat80_level
  
  vars_not_in_train = names(which(sapply(df[,factor_vars], FUN = function(x){
    train_value = unique(x[train_index & df$cat80 == cat80_level])
    test_value = unique(x[test_index & df$cat80 == cat80_level])
    length(setdiff(test_value, train_value))
    }  ) >0 ))
  
  #variables with one level
  vars_to_remove = names(which(sapply(df[seg_index,factor_vars], FUN = function(x) length(unique(x))) <= 1))
  vars_to_remove = union(vars_to_remove, vars_not_in_train)
  vars_to_remove = vars_to_remove[vars_to_remove %in% all.vars(formula.gbm)]
  
  print('variables to remove')
  print(vars_to_remove)
  
  if(length(vars_to_remove) == 0 ) {
    formula.gbm_seg = formula.gbm
  }else {
    formula.gbm_seg = update(formula.gbm, paste('~. -', paste(vars_to_remove, collapse = '-')) )
  }
  
  print(formula.gbm_seg)
  
  max_trees = max_it$it[max_it$level == cat80_level]
  
  print(paste('max_trees:', max_trees))
    
    model.gbm_seg = gbm(formula.gbm_seg, 
                data = df[seg_index, all.vars(formula.gbm_seg)], 
                distribution = 'gaussian',
                n.trees = max_trees,
                shrinkage = 0.001,#0.005
                bag.fraction = 0.5,
                interaction.depth = 2,
                cv.folds = 3,
                train.fraction = 1.0,
                n.cores = 1, 
                verbose =  FALSE)
  
  #show best iteration
  best_it = gbm.perf(model.gbm_seg, method = 'cv')
  print(best_it)
  grid()
  
  #show importance
  par(mfrow = c(1,1), las = 1)
  vars.importance = summary(model.gbm_seg, n.trees = best_it) # influence
  print(vars.importance)
  grid()
  
  #show 16 most important variables
  for(i in seq( pmin(16, length(vars.importance$var))) ) {
    plot.gbm(model.gbm_seg, n.trees =best_it,  i = as.character(vars.importance$var[i]) )
    grid()
  }

  
  pred.gbm[df$cat80 == cat80_level] = predict(model.gbm_seg,  n.trees = best_it, newdata = df[df$cat80 == cat80_level,])
  
  cat(paste('gbm seg (in/out):', cat80_level, 
          round(r_sqr(df$log_loss[train_index & df$cat80 == cat80_level],  
                      pred.gbm[train_index & df$cat80 == cat80_level]), 6),
          sum(is.na(pred.gbm[df$cat80 == cat80_level])) ))
  
}
  cat(paste('gbm seg (in/out):', 
          round(r_sqr(df$log_loss[train_index],  pred.gbm[train_index]), 4),
          sum(is.na(pred.gbm)) ))
  
for (cat80_level in levels(df$cat80)){
   print(paste('gbm seg (in/out):', cat80_level, 
          round(r_sqr(df$log_loss[train_index & df$cat80 == cat80_level],  
                      pred.gbm[train_index & df$cat80 == cat80_level]), 4),
          sum(is.na(pred.gbm[df$cat80 == cat80_level])),
          length(pred.gbm[df$cat80 == cat80_level]) ))
}

df$log_loss_pred = pred.gbm

ggplot(df[train_index,], aes(x = log_loss_pred, fill = cat80)) +stat_density()
ggplot(df[train_index,], aes(x = log_loss, fill = cat80)) +stat_density()
ggplot(df[train_index,], aes(x = log_loss -log_loss_pred, fill = cat80)) +stat_density()
  
```



## Compare
gbm_seg 0.5618776
```{r compare, fig.width = 8, fig.height = 6, dpi = 100}

results = list()
#results$mars =pred.mars
results$lm_seg =pred.lm
results$gbm_seg = pred.gbm

res = ldply(results, .id = 'model', function(x) {
  c(r2 = r_sqr(df$log_loss[train_index],  x[train_index]),
    na_count = sum(is.na(x[test_index])))
})

print(res)

#ggplot(res, aes(model, r2)) + geom_bar(stat = "identity") + coord_flip() + coord_cartesian(ylim = c(0.9*min(res$r2), min(1.0, 1.1*max(res$r2) ) ))

```

## Submit
```{r submit, fig.width = 8, fig.height = 6, dpi = 100}

folder = "C:/Dev/Kaggle/AllstateClaimsSeverity/"

for (model_name in names(results) ){
  submit <- data.frame(id = as.integer( as.numeric(df$id[test_index]) ), 
                       loss = exp(results[[model_name]][test_index]))
  
  submit = submit[order(submit$id),]
  
  file = file.path(folder, sprintf("my_solution_%s.csv", model_name))
  
  write.csv(submit, file = file, row.names = FALSE)
  
  print(file)
}

setwd(folder) 
save.image(file = "workspace.RData") #load("workspace.RData") 

```
