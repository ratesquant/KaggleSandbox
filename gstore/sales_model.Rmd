---
title: "GStore Predictor"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(gbm)
library(data.table)
library(plyr)
library(stringi)
library(ggplot2)
library(gridExtra)
library(zip)
library(xgboost)
library(corrplot)
library(forcats)
#library(pdp)
library(e1071)
library(jsonlite)
library(lubridate)
#library(rjson)
#library(caret)

#working_folder = 'C:/Dev/Kaggle/'
working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')


source(file.path(working_folder, '/Utils/common.R'))
```

## Loan Data

```{r load_data}

df_filename = file.path(working_folder,'gstore/data/all.rds')
df_agg_filename = file.path(working_folder,'gstore/data/all_agg.rds')


if(file.exists(df_filename)) {
  df = readRDS(df_filename)
}else {
  
  df_train = fread(file.path(working_folder,'gstore/data/train.csv'), check.names=T)#, nrows = 10000)
  df_test  = fread(file.path(working_folder,'gstore/data/test.csv'),  check.names=T)#, nrows = 10000)
  
  df_train[,is_train:=T ]
  df_test[, is_train:=F ]
  
  df = rbind(df_train, df_test)
  train_index = df$is_train
  
  #json column:fields 
  # device: browser, operatingSystem, isMobile, deviceCategory
  # geoNetwork: continent, subContinent, country, region, metro, networkDomain
  # totals:  visits, hits, pageviews, bounces, newVisits
  # trafficSource
  
  # TODO: try to think of a better way to do this
  parse_json <- function(json_str, fields){
    values = fromJSON(stri_replace_all_fixed(json_str, '\"\"','\"') )[fields]
    names(values) <-fields
    #values[sapply(values , is.null)] = 'NA'
    values = lapply(values, as.character)
    #print(sapply(values , class))
    return(values)
  }
  parse_json_all <- function(json_str){
    values = fromJSON(stri_replace_all_fixed(json_str, '\"\"','\"') )
    return(values)
  }
  #parse_geo_v <- Vectorize(parse_geo, SIMPLIFY = TRUE)
  
  geo_fields = c('continent', 'subContinent', 'country', 'region', 'metro', 'city', 'networkDomain')
  device_fields = c('browser', 'operatingSystem', 'isMobile', 'deviceCategory')
  trafficSource_fields = c('campaign', 'source', 'medium', 'keyword',
                           'isTrueDirect','adContent',
                           'adwordsClickInfo.page','adwordsClickInfo.slot','adwordsClickInfo.gclId','adwordsClickInfo.adNetworkType','adwordsClickInfo.isVideoAd','referralPath')
  totals_fields = c('visits', 'hits', 'pageviews','bounces', 'newVisits', 'transactionRevenue')
  
  geo_columns           = stri_join('geo_', geo_fields)
  device_columns        = stri_join('device_', device_fields)
  trafficSource_columns = stri_join('trafficSource_', trafficSource_fields)
  totals_columns        = stri_join('totals_', totals_fields)
  
  all_rows = seq_len(nrow(df))
  
  df[, (geo_columns)          := parse_json(geoNetwork, geo_fields),               by = all_rows]
  df[, (device_columns)       := parse_json(device, device_fields),                by = all_rows]
  df[, (trafficSource_columns):= parse_json(trafficSource, trafficSource_fields),  by = all_rows]
  df[, (totals_columns)       := parse_json(totals, totals_fields),                by = all_rows]
  
  df[,geoNetwork:=NULL]
  df[,device:=NULL]
  df[,trafficSource:=NULL]
  df[,totals:=NULL]
  
  bool_columns = c('device_isMobile','trafficSource_isTrueDirect')
  df[, (bool_columns):=lapply(.SD, as.logical), .SDcols = bool_columns]
  
  num_columns = c('totals_hits','totals_pageviews', 'totals_bounces', 'totals_newVisits','totals_transactionRevenue','totals_visits')
  df[, (num_columns):=lapply(.SD, as.numeric), .SDcols = num_columns]

  
  #parse_json(df$device[4],device_fields)
  #parse_json(df$trafficSource[4],trafficSource_fields)
  #parse_json(df$trafficSource[5],trafficSource_fields)
  #df[,geo_columns, with = F]
  #df[,device_columns, with = F]
  #df[,trafficSource_columns, with = F]
  #df[,totals_columns, with = F]
  
  #check forpossible values
  #temp = ldply(sample.int(nrow(df), 10000), function(i) {
  #   res = data.table(t(unlist(parse_json_all(df$trafficSource[i]))))  
  #   return (res)
  #} )
  #str(temp)
  #sapply(temp, function(x) length(unique(x)))
  
  
  #a = df[, .(geoNetwork, geo_continent)]
  #a = df[, .(device, device_isMobile)]
  #a = df[, .(trafficSource, trafficSource_isTrueDirect)]
  #df[, .(trafficSource, trafficSource_isTrueDirect)]
  
  saveRDS(df, df_filename)
}
df[, iso_date := as.Date(as.character(date), format = '%Y%m%d')]
df[, date_month := month(iso_date)]
df[, date_year := year(iso_date)]
df[, date_day := day(iso_date)]
df[, date_wday := weekdays(iso_date)]

#df$totals_transactionRevenue

#table(df[,.(is_train, date_year)])
#table(df[,.(is_train, visitNumber)])

#aggregate by visitor
#agg_visitors <- function(x) {
#    return(list(n = nrow(x), first_date = min(x$iso_date), last_date = max(x$iso_date), revenue = sum(x$totals_transactionRevenue, na.rm = T)))
#  }
#system.time(df[1:10000, agg_visitors(.SD), by = .(fullVisitorId, is_train)])
#system.time(df[1:10000, .(.N, first_date = min(iso_date), last_date = max(iso_date),revenue = sum(totals_transactionRevenue, na.rm = T)), by = .(fullVisitorId, is_train)])
if(file.exists(df_agg_filename)) {
  df_agg = readRDS(df_agg_filename)
}else {
    
  nth_most_freq <- function(x, nth = 1) {names(sort(table(x, useNA = "ifany"), decreasing=TRUE))[nth]}
  count_unique <- function(x) {length(unique(x))}
  
  df_agg = df[, .(.N, 
                  
                  first_date = min(iso_date), last_date = max(iso_date), 
                  day_diff_avg = mean(as.numeric(diff(sort(iso_date) )), na.rm = T), #this can be easily computed 
                  day_diff_max = max(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  day_diff_min = min(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  day_diff_sd  = sd(as.numeric(diff(sort(iso_date) )), na.rm = T),
                  
                  channelGrouping = nth_most_freq(channelGrouping),
                  channelGrouping_count = count_unique(channelGrouping),
                  
                  geo_continent  = nth_most_freq(geo_continent),
                  geo_continent_n  = count_unique(geo_continent),
                  
                  geo_subContinent  = nth_most_freq(geo_subContinent), 
                  geo_subContinent_n  = count_unique(geo_subContinent),
                  
                  geo_country  = nth_most_freq(geo_country), 
                  geo_country_n  = count_unique(geo_country),
                  
                  geo_region  = nth_most_freq(geo_region), 
                  geo_region_n  = count_unique(geo_region),
                  
                  geo_city  = nth_most_freq(geo_city), 
                  geo_city_n  = count_unique(geo_city),
                  
                  date_wday1 = nth_most_freq(date_wday, 1),
                  date_wday2 = nth_most_freq(date_wday, 2),
                  date_wday3 = nth_most_freq(date_wday, 3),
                  
                  date_mon_pct = sum(date_wday == 'Monday', na.rm = T)/.N,
                  date_tue_pct = sum(date_wday == 'Tuesday', na.rm = T)/.N,
                  #date_wed_pct = sum(date_wday == 'Wednesday', na.rm = T)/.N, #exclude wed, since it is redundant 
                  date_thu_pct = sum(date_wday == 'Thursday', na.rm = T)/.N,
                  date_fri_pct = sum(date_wday == 'Friday', na.rm = T)/.N,
                  date_sat_pct = sum(date_wday == 'Saturday', na.rm = T)/.N,
                  date_sun_pct = sum(date_wday == 'Sunday', na.rm = T)/.N,
                  
                  device_operatingSystem  = nth_most_freq(device_operatingSystem),
                  device_operatingSystem_n  = count_unique(device_operatingSystem),
                  
                  trafficSource_source  = nth_most_freq(trafficSource_source),
                  trafficSource_source_n  = count_unique(trafficSource_source),
                  
                  geo_networkDomain  = nth_most_freq(geo_networkDomain),
                  geo_networkDomain_n  = count_unique(geo_networkDomain),
                  
                  trafficSource_medium  = nth_most_freq(trafficSource_medium),
                  trafficSource_medium_n  = count_unique(trafficSource_medium),
                  
                  device_deviceCategory  = nth_most_freq(device_deviceCategory),
                  desktop_n  = sum(device_deviceCategory == 'desktop', na.rm = T)/.N,
                  mobile_n  = sum(device_deviceCategory == 'mobile', na.rm = T)/.N,
                  tablet_n  = sum(device_deviceCategory == 'tablet', na.rm = T)/.N,
                  
                  device_browser  = nth_most_freq(device_browser),
                  device_browser_n  = count_unique(device_browser),
                  
                  totals_hits      = sum(totals_hits, na.rm = T)/.N,
                  totals_pageviews = sum(totals_pageviews, na.rm = T)/.N,
                  totals_bounces   = sum(totals_bounces, na.rm = T)/.N,
                  
                  totals_hits_sd      = sd(totals_hits, na.rm = T),
                  totals_pageviews_sd = sd(totals_pageviews, na.rm = T),
                  
                  totals_hits_max      = max(totals_hits, na.rm = T),
                  totals_pageviews_max = max(totals_pageviews, na.rm = T),
                  
                  revenue = log(1  + sum(totals_transactionRevenue, na.rm = T))),
              , by = .(fullVisitorId, is_train)]
  
  char_columns = names(df_agg)[which(lapply(df_agg, is.character) ==T)] %!in_set% c('fullVisitorId')
  df_agg[, (char_columns):=lapply(.SD, as.factor), .SDcols = char_columns]
  
  #df_agg[1:10, ..char_columns]
  
  saveRDS(df_agg, df_agg_filename)
}
tindex = df_agg$is_train

df_agg[, total_days:= as.numeric(difftime(last_date, first_date, units = 'days'))]
df_agg[, last_iso_date:= as.Date(ISOdate(year(last_date), month(last_date), 1)) ]
df_agg[, first_iso_date:= as.Date(ISOdate(year(first_date), month(first_date), 1)) ]
df_agg[, last_month:=  month(last_date) ]
df_agg[, first_month:= month(first_date) ]
df_agg[, first_week_date := factor(weekdays(first_date)) ]
df_agg[, date_wnd_pct := date_sun_pct + date_sat_pct]

#correct missing variables
df_agg[is.infinite(totals_pageviews_max), totals_pageviews_max:= 0]
df_agg[is.infinite(day_diff_max),         day_diff_max:= NA]
df_agg[is.infinite(day_diff_min),         day_diff_min:= NA]


df_agg[, total_weeks:= as.numeric(difftime(last_date, first_date, units = 'weeks'))]
df_agg[, totals_pageviews_log:= log(totals_pageviews + 1)]
df_agg[, totals_pageviews_sd_log:= log(totals_pageviews_sd + 1)]
df_agg[, totals_pageviews_max_log:= log(totals_pageviews_max + 1) - totals_pageviews_log]

df_agg[, totals_hits_log:= log(totals_hits + 1)]
df_agg[, totals_hits_max_log:= log(totals_hits_max + 1)-totals_hits_log]

df_agg[, count_log:= log(N)]
df_agg[, day_diff_sd_log:= log(day_diff_sd + 1)]
#df_agg[, totals_hits_log:= log(totals_hits + 1)]

combine_levels <- function(x, n = 3000){
  counts = table(x)
  keep_levels = names(counts)[which(counts > n)]
  return( fct_other(x,keep = keep_levels) )
}

df_agg[, geo_city_major:=combine_levels(geo_city)]
df_agg[, geo_country_major:=combine_levels(geo_country)]
df_agg[, geo_region_major:=combine_levels(geo_region)]
df_agg[, geo_networkDomain_major:=combine_levels(geo_networkDomain)]
df_agg[, trafficSource_source_major:=combine_levels(trafficSource_source)]
df_agg[, device_browser_major:=combine_levels(device_browser)]
df_agg[, device_operatingSystem_major:=combine_levels(device_operatingSystem)]


table(df_agg[,.(geo_city_major)])
table(df_agg[,.(geo_country_major)])
table(df_agg[,.(geo_region_major)])
table(df_agg[,.(geo_networkDomain_major)])
table(df_agg[,.(trafficSource_source_major)])
table(df_agg[,.(device_browser_major)])
table(df_agg[,.(device_operatingSystem_major)])

#df[fullVisitorId =='1131660440785968503',log(1+sum(totals_transactionRevenue, na.rm = T)), by =  .(fullVisitorId, is_train) ]
```


## View Data

```{r view_data}

ggplot(df_agg, aes(totals_pageviews_max_log, group = is_train, fill = is_train)) + stat_ecdf()
ggplot(df_agg, aes(totals_hits_log, group = is_train, fill = is_train)) + stat_ecdf()

#cor(df_agg[,.(totals_pageviews_max_log-totals_pageviews_log, totals_pageviews_log)], use = 'pairwise.complete.obs')
```

## One Step Revenue Model

```{r model, eval = FALSE}

actual = df_agg$revenue

#only keep several car_11 levels

all_vars = names(df_agg) %!in_set% c('fullVisitorId','is_train','first_date','last_date','revenue', #id and outcome var
                                     'geo_city','geo_country','geo_region', 'geo_networkDomain','trafficSource_source', #too many levels
                                     'total_days','last_iso_date','first_iso_date', 'last_month', 'first_month',
                                     'totals_pageviews','totals_pageviews_max', 'totals_pageviews_sd', 'totals_hits', 'totals_hits_max', 'N','day_diff_sd') #replaced with logs

exclude_vars = c()

set.seed(1012356)

formula.gbm = formula(stri_join( 'revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('revenue')
var.monotone = rep(0, length(model_vars))

#df_agg[1:10, ..model_vars]
#str(df_agg[, ..model_vars])

#num_vars  = model_vars %in_set% names(which(sapply(df_agg, is.numeric)))
#corr_matrix = cor(df_agg[, ..num_vars ], use="complete.obs")
#corrplot(corr_matrix, method="number", number.cex = 0.5)
#corrplot(corr_matrix, method="circle", order="hclust")


mon_inc_vars = c('totals_pageviews_log')
mon_dec_vars = c()

var.monotone[model_vars %in% mon_inc_vars]  =  1
var.monotone[model_vars %in% mon_dec_vars]  = -1

cv_folds = 0
max_it = 2000
#0.49
model.gbm  = gbm(formula.gbm,
                 distribution = "gaussian",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=4,
                 train.fraction = 0.7,
                 bag.fraction = 0.8,# 0.5 for small samples, 0.7 for large
                 n.cores = 4,
                 var.monotone = var.monotone,
                 data = df_agg[tindex , all.vars(formula.gbm), with = F],
                 verbose = FALSE)

#saveRDS(model.gbm, file.path(working_folder,'gstore/model.rds'))
#model.gbm = readRDS(file.path(working_folder,'gstore/model.rds'))

plot_gbmiterations(model.gbm)

best_it.gbm = gbm.perf(model.gbm, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model.gbm, plot.it = F))

pred.gbm  = predict(model.gbm, n.trees = best_it.gbm, newdata = df_agg)

summary(lm('actual ~ model ', data.frame(actual = actual[tindex], model = pred.gbm[tindex]))) #0.3951
ggplot(data.frame(actual = actual[tindex], model = pred.gbm[tindex]), aes(model, actual)) + geom_point()

#influence
var_inf = summary(model.gbm, n.trees = best_it.gbm, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.5])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model.gbm, best_it.gbm, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)

plots = llply(imp_vars, function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

#all vars
plots = llply(all.vars(formula.gbm), function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$last_date[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$first_date[tindex]), error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$last_iso_date[tindex], error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],df_agg$first_iso_date[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$first_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$last_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],month(df_agg$last_date)[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$total_weeks[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$date_mon_pct[tindex], error_band = 'normal')

```

## Two Step Model 
first predict that transaction probability is non-zero, then predict value

### Logistic
```{r tr_predict_model}
df_agg[, is_revenue := as.numeric(revenue>9)]

actual = df_agg$is_revenue

#only keep several car_11 levels

all_vars = names(df_agg) %!in_set% c('fullVisitorId','is_train','first_date','last_date','revenue','is_revenue', #id and outcome var
                                     'geo_city','geo_country','geo_region', 'geo_networkDomain','trafficSource_source','device_browser', #too many levels
                                     'total_days','last_iso_date','first_iso_date', 'last_month', 'first_month',
                                     'totals_pageviews','totals_pageviews_max', 'totals_pageviews_sd', 'totals_hits', 'totals_hits_max', 'N','day_diff_sd') #replaced with logs


set.seed(1012356)

formula.gbm = formula(stri_join( 'is_revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('is_revenue')
var.monotone = rep(0, length(model_vars))

#df_agg[1:10, ..model_vars]
#str(df_agg[, ..model_vars])

#num_vars  = model_vars %in_set% names(which(sapply(df_agg, is.numeric)))
#corr_matrix = cor(df_agg[, ..num_vars ], use="complete.obs")
#corrplot(corr_matrix, method="number", number.cex = 0.5)
#corrplot(corr_matrix, method="circle", order="hclust")


mon_inc_vars = c('totals_pageviews_sd_log','totals_pageviews_log','totals_pageviews_max_log','count_log')
mon_dec_vars = c('totals_hits_log')

var.monotone[model_vars %in% mon_inc_vars]  =  1
var.monotone[model_vars %in% mon_dec_vars]  = -1

cv_folds = 0
max_it = 2000

model_1.gbm  = gbm(formula.gbm,
                 distribution = "bernoulli",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=4,
                 train.fraction = 0.7,
                 bag.fraction = 0.8,# 0.5 for small samples, 0.7 for large
                 n.cores = 4,
                 var.monotone = var.monotone,
                 data = df_agg[tindex , all.vars(formula.gbm), with = F],
                 verbose = FALSE)

#saveRDS(model_1.gbm, file.path(working_folder,'gstore/model_1.rds'))
#model_1.gbm = readRDS(file.path(working_folder,'gstore/model_1.rds'))

plot_gbmiterations(model_1.gbm)

best_it.gbm = gbm.perf(model_1.gbm, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model_1.gbm, plot.it = F))

pred.gbm  = predict(model_1.gbm, n.trees = best_it.gbm, newdata = df_agg, type = 'response')
plot_binmodel_roc(actual[tindex], pred.gbm[tindex])
plot_binmodel_cdf(actual[tindex], pred.gbm[tindex])
plot_binmodel_percentiles(actual[tindex], pred.gbm[tindex], 10)

df_agg[,is_revenue_prob := pred.gbm]

#summary(glm('actual ~ model ', data = data.frame(actual = actual[tindex], model = pred.gbm[tindex]), family = binomial(link = "logit"))) 

#influence
var_inf = summary(model_1.gbm, n.trees = best_it.gbm, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.5])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model_1.gbm, best_it.gbm, imp_vars, output_type = 'response')
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)

plots = llply(imp_vars, function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'binom') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

#all vars
plots = llply(all.vars(formula.gbm), function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'binom') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$last_date[tindex]), error_band = 'binom')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$date_wday1[tindex], error_band = 'binom')

```


### Revenue Model
```{r revenue_predict_model, eval = FALSE}

actual = df_agg$revenue

#only keep several car_11 levels

exclude_vars = c('fullVisitorId','is_train','first_date','last_date','revenue', 'is_revenue', #id and outcome var
                                     'geo_city','geo_country','geo_region', 'geo_networkDomain','trafficSource_source', #too many levels
                                     'total_days','last_iso_date','first_iso_date', 'last_month', 'first_month',
                                     'totals_pageviews','totals_pageviews_max', 'totals_pageviews_sd', 'totals_hits', 'totals_hits_max', 'N','day_diff_sd') #replaced with logs

all_vars = names(df_agg) %!in_set% exclude_vars

set.seed(1012356)

formula.gbm = formula(stri_join( 'revenue ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('revenue')
var.monotone = rep(0, length(model_vars))

#df_agg[1:10, ..model_vars]
#str(df_agg[, ..model_vars])

#num_vars  = model_vars %in_set% names(which(sapply(df_agg, is.numeric)))
#corr_matrix = cor(df_agg[, ..num_vars ], use="complete.obs")
#corrplot(corr_matrix, method="number", number.cex = 0.5)
#corrplot(corr_matrix, method="circle", order="hclust")


mon_inc_vars = c('totals_pageviews_log')
mon_dec_vars = c()

var.monotone[model_vars %in% mon_inc_vars]  =  1
var.monotone[model_vars %in% mon_dec_vars]  = -1

cv_folds = 0
max_it = 100
#0.49
model.gbm  = gbm(formula.gbm,
                 distribution = "gaussian",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.01,
                 interaction.depth=4,
                 train.fraction = 0.7,
                 bag.fraction = 0.8,# 0.5 for small samples, 0.7 for large
                 n.cores = 4,
                 var.monotone = var.monotone,
                 data = df_agg[tindex , all.vars(formula.gbm), with = F],
                 verbose = FALSE)

#saveRDS(model.gbm, file.path(working_folder,'gstore/model.rds'))
#model.gbm = readRDS(file.path(working_folder,'gstore/model.rds'))

plot_gbmiterations(model.gbm)

best_it.gbm = gbm.perf(model.gbm, plot.it = F) #ifelse(cv_folds==0, max_it, gbm.perf(model.gbm, plot.it = F))

pred.gbm  = predict(model.gbm, n.trees = best_it.gbm, newdata = df_agg)

summary(lm('actual ~ model ', data.frame(actual = actual[tindex], model = pred.gbm[tindex]))) #0.3951
#ggplot(data.frame(actual = actual[tindex], model = pred.gbm[tindex]), aes(model, actual)) + geom_point()

#influence
var_inf = summary(model.gbm, n.trees = best_it.gbm, plotit = F)
var_inf = subset(var_inf, rel.inf>0.1)
#fwrite(var_inf, file = file.path(working_folder, "gstore/variables.csv"), row.names = FALSE)
plot_gbminfluence(var_inf)
print(var_inf)

imp_vars = as.character(var_inf$var[var_inf$rel.inf>0.5])
#df_agg[1:100,..imp_vars]

plots = plot_gbmpartial(model.gbm, best_it.gbm, imp_vars, output_type = 'link')
marrangeGrob(plots, nrow = 3, ncol = 4, top = NULL)

plots = llply(imp_vars, function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

#all vars
plots = llply(all.vars(formula.gbm), function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)


plots = llply(exclude_vars %!in_set% c('fullVisitorId'), function(var_name) {
  p = plot_profile(pred.gbm[tindex], actual[tindex],df_agg[[var_name]][tindex], error_band = 'normal') +
    ggtitle(var_name) +  theme(title =element_text(size=6))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

exclude_vars

plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$last_date[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],weekdays(df_agg$first_date[tindex]), error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$last_iso_date[tindex], error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],df_agg$first_iso_date[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$first_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],factor(month(df_agg$last_date)[tindex]), error_band = 'normal')
plot_profile(pred.gbm[tindex], actual[tindex],month(df_agg$last_date)[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$total_weeks[tindex], error_band = 'normal')

plot_profile(pred.gbm[tindex], actual[tindex],df_agg$date_mon_pct[tindex], error_band = 'normal')

```

## Save Results
1.4520

```{r save_results}

test_index = df_agg$is_train == FALSE

submit = data.table(fullVisitorId = df_agg$fullVisitorId[test_index], PredictedLogRevenue = pmax(0, pred.gbm[test_index]))
  
file = file.path(working_folder, "gstore/solution.csv")
  
fwrite(submit, file = file, row.names = FALSE)

zip(paste(file, '.zip', sep = ''), file)
  
print(file)

#fullVisitorId,PredictedLogRevenue

```