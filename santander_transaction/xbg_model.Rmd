---
title: "Santander Transaction Model"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(plyr)

library(data.table)
library(stringi)
library(ggplot2)
library(tabplot)
library(gridExtra)
library(zip)
library(corrplot)
library(forcats)
#library(pdp)
library(e1071)
library(lubridate)

library(gbm)
#library(randomForestSRC)
library(xgboost)
library(pdp) 
library(vip) 

library(rBayesianOptimization)

#library(lightgbm)

#working_folder = 'C:/Dev/Kaggle/'
#working_folder = 'F:/Github/KaggleSandbox/'
working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')


source(file.path(working_folder, '/Utils/common.R'))
```

## Load Data

```{r load_data}

  df_train = fread(file.path(working_folder,'santander_transaction/data/train.csv'), check.names=T)#, nrows = 10000)
  df_test  = fread(file.path(working_folder,'santander_transaction/data/test.csv'),  check.names=T)#, nrows = 10000)

  df_train[,is_train:=TRUE  ]
  df_test[, is_train:=FALSE ]
  df_test[, target:=NA ]
  
  df = rbind(df_train, df_test)
  df = df[sample.int(nrow(df), nrow(df)),]
  train_index = df$is_train
  
  gc(reset = TRUE)
  
  ggplot(df, aes(var_0, group = is_train)) + stat_ecdf()
  
  ggplot(df_train[1:10000,], aes(var_81, var_139, color = factor(target) )) + geom_point(alpha = 0.2)
  
  #tableplot(df_train[1:10000,], select = c('target',stri_join('var_', seq(0, 10))), sortCol = 'var_0')
  #tableplot(df_train[1:10000,], select = c('target','var_1'), sortCol = 'var_0')
```

## XGBoost Model

```{r xgboost_model}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

#only keep several car_11 levels
exclude_vars = c('ID_code', 'is_train', obj_var) #replaced with logs
all_vars = names(df) %!in_set% c(exclude_vars)

mon_inc_vars = stri_join('var_', c(110, 53, 26, 22, 6, 99, 2, 78,
                                   164,94,133,190,179,40,0,1,170,184,191,
                                   18,147,173,91,67,118,95,
                                   71, 52, 167, 51, 89, 180, 130, 35, 106, 
                                   157, 128, 32, 119, 162, 49, 195, 90, 175, 24, 82,111,
                                   155,5,145,163,125,151,137,48,135,55,196,105,144,97,
                                   66,8,140,16,176,171,189,46,19,5,11,15,62,25,181))
mon_dec_vars = stri_join('var_', c(81, 139, 12, 146, 174, 166, 109, 80, 76, 165, 21, 198,
                                   44, 13, 148, 108,34, 33, 92,
                                   9,169,121,123,115,86,122,188,75,149,127,107,56,
                                   177, 87, 23, 172, 36, 154, 192, 141,43, 131, 186,
                                   193,142,152,129, 197, 93, 150, 83,132,58,114,104, 102))


var.monotone = rep(0, length(all_vars))
var.monotone[all_vars %in% mon_inc_vars]  =  1
var.monotone[all_vars %in% mon_dec_vars]  = -1

dtrain = data.matrix(df[tindex,all_vars, with = F])
#Round = 98	eta = 0.2366	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 6.0000	Value = 0.8965 

model.xgb <- xgboost(
  max_depth = 1, 
  eta = 0.2366, 
  nthread = 4,
  subsample = 1.0,
  min_child_weight = 6.0,
  objective = "binary:logistic",
  eval_metric = "auc",
  monotone_constraints = var.monotone,
  data = dtrain, label = actual[tindex],
  print_every_n = 100,
  nrounds = 3000, verbose = 1)

ggplot(model.xgb$evaluation_log, aes(iter, train_auc)) + geom_line()


pred.xgb <- predict(model.xgb, data.matrix(df[,all_vars, with = F]) )

plot_binmodel_roc(actual[tindex], pred.xgb[tindex])
plot_binmodel_cdf(actual[tindex], pred.xgb[tindex])
plot_binmodel_percentiles(actual[tindex], pred.xgb[tindex], 100)
gbm.roc.area(actual[tindex], pred.xgb[tindex]) #0.7474184

importance_matrix <- xgb.importance(model = model.xgb)
print(importance_matrix)
xgb.ggplot.importance(importance_matrix = importance_matrix)
xgb.ggplot.deepness(model.xgb)
xgb.ggplot.deepness(model.xgb, which = '2x1')
xgb.ggplot.deepness(model.xgb, which = 'max.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.depth')
#xgb.plot.shap(as.matrix(df[tindex,all_vars, with = F]), model = model.xgb, top_n = 12, n_col = 4) #takes a very long time time to produce
#xgb.plot.importance(importance_matrix = importance_matrix)

plots = llply(as.character(importance_matrix$Feature), function(var_name) {
  p = plot_profile(pred.xgb[tindex], actual[tindex],df[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6), axis.title.y = element_blank()) 
  return( p )
})
#marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

gplots = lapply(plots, ggplotGrob)
ggsave(filename = file.path(working_folder,"santander_transaction/xbg.profiles_d20.pdf"), plot = marrangeGrob(gplots, nrow=5, ncol=6), device = 'pdf', width = 11, height = 8.5, dpi = 240)

#PD plots -------------
pd_plots = llply(as.character(importance_matrix$Feature), function(vname){
  temp = partial(model.xgb, pred.var = vname, train = dtrain, prob = TRUE)
  ggplot(temp, aes_string(vname, 'yhat')) + geom_line()
})
marrangeGrob(pd_plots, nrow = 5, ncol = 5, top = NULL)
gplots = lapply(pd_plots, ggplotGrob)
ggsave(filename = file.path(working_folder,"santander_transaction/xbg.pd.pdf"), plot = marrangeGrob(gplots, nrow=5, ncol=6), device = 'pdf', width = 14, height = 8.5, dpi = 240)

```

## XGBoost Model Params

```{r xgboost_model_params}
tindex = df$is_train

obj_var = 'target'
actual = df[[obj_var]]

#only keep several car_11 levels
exclude_vars = c('ID_code', 'is_train', obj_var) #replaced with logs
all_vars = names(df) %!in_set% c(exclude_vars)

mon_inc_vars = stri_join('var_', c(110, 53, 26, 22, 6, 99, 2, 78,
                                   164,94,133,190,179,40,0,1,170,184,191,
                                   18,147,173,91,67,118,95,
                                   71, 52, 167, 51, 89, 180, 130, 35, 106, 
                                   157, 128, 32, 119, 162, 49, 195, 90, 175, 24, 82,111,
                                   155,5,145,163,125,151,137,48,135,55,196,105,144,97,
                                   66,8,140,16,176,171,189,46,19,5,11,15,62,25,181))
mon_dec_vars = stri_join('var_', c(81, 139, 12, 146, 174, 166, 109, 80, 76, 165, 21, 198,
                                   44, 13, 148, 108,34, 33, 92,
                                   9,169,121,123,115,86,122,188,75,149,127,107,56,
                                   177, 87, 23, 172, 36, 154, 192, 141,43, 131, 186,
                                   193,142,152,129, 197, 93, 150, 83,132,58,114,104, 102))


var.monotone = rep(0, length(all_vars))
var.monotone[all_vars %in% mon_inc_vars]  =  1
var.monotone[all_vars %in% mon_dec_vars]  = -1

train.fraction = 0.7

dtrain <- xgb.DMatrix(as.matrix(df[tindex,all_vars, with = F]), label = actual[tindex] )

train.sample = sample.int(nrow(dtrain), train.fraction * nrow(dtrain))
eval.sample = seq(nrow(dtrain)) %!in_set% train.sample

param <- list(max_depth = 6, 
              eta = 0.02, 
              nthread = 4,
              subsample = 0.9,
              objective = "binary:logistic",
              eval_metric = "auc",
              base_score = mean(actual[tindex]),
              monotone_constraints = var.monotone)

model.xgb <- xgb.train(param, data = dtrain[train.sample,], 
                       watchlist = list(train = dtrain[train.sample,], eval = dtrain[eval.sample,]),
                       nrounds = 4000, 
                       verbose = 1, 
                       print_every_n = 10,
                       early_stopping_rounds = 30)

#a[eval_auc == max(eval_auc),]

ggplot(model.xgb$evaluation_log, aes(iter, train_auc)) + geom_line() + geom_line(aes(iter, eval_auc), color = 'red')

pred.xgb <- predict(model.xgb, data.matrix(df[,all_vars, with = F]), ntreelimit = 3787 )

#plot_binmodel_roc(actual[tindex], pred.xgb[tindex])
#plot_binmodel_cdf(actual[tindex], pred.xgb[tindex])
#plot_binmodel_percentiles(actual[tindex], pred.xgb[tindex], 20)
gbm.roc.area(actual[tindex], pred.xgb[tindex]) #0.7474184

vip(model.xgb, num_features = 30) 

importance_matrix <- xgb.importance(model = model.xgb)
print(importance_matrix)
xgb.ggplot.importance(importance_matrix = importance_matrix)
xgb.ggplot.deepness(model.xgb)
xgb.ggplot.deepness(model.xgb, which = '2x1')
xgb.ggplot.deepness(model.xgb, which = 'max.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.depth')
xgb.ggplot.deepness(model.xgb, which = 'med.weight')
#xgb.plot.shap(as.matrix(df[tindex,all_vars, with = F]), model = model.xgb, top_n = 12, n_col = 4) #takes a very long time time to produce
#xgb.plot.importance(importance_matrix = importance_matrix)

pd_plots = llply(stri_join('var_',as.character(importance_matrix$Feature))[1:10], function(vname){
  temp = partial(model.xgb, pred.var = vname, train = data.matrix(df[tindex,all_vars, with = F]), prob = TRUE)
  ggplot(temp, aes_string(vname, 'yhat')) + geom_line()
})
marrangeGrob(pd_plots, nrow = 3, ncol = 4, top = NULL)

#partial(model.xgb, pred.var = c("var_1", "var_2"),
#              plot = TRUE, chull = TRUE, plot.engine = "ggplot2", train = x)


plots = llply(stri_join('var_',as.character(importance_matrix$Feature))[1:40], function(var_name) {
  p = plot_profile(pred.xgb[tindex], actual[tindex],df[[var_name]][tindex], error_band = 'binom', bucket_count = 20) +
    ggtitle(var_name) +  theme(title =element_text(size=6), axis.title.y = element_blank()) 
  return( p )
})
#marrangeGrob(plots, nrow = 5, ncol = 7, top = NULL)

### CV ----------------
my_params = expand.grid(depth =c(1,2,3,4,5), eta = c(0.1, 0.02), subsample = c(0.9, 1.0), mono =c(TRUE, FALSE))

df_train_set = data.matrix(df[tindex,all_vars, with = F])

param_res = ldply(seq(nrow(my_params)), function(run_index){
  print(my_params[run_index,])
  
  set.seed(132140937)
  
  xgb_cv <- xgboost::xgb.cv(
    data = df_train_set, label = actual[tindex], 
    verbose = 1, objective = "binary:logistic", eval_metric = "auc",
    nrounds = 2000, 
    max_depth = my_params$depth[run_index], 
    subsample = my_params$subsample[run_index],
    eta = my_params$eta[run_index], 
    monotone_constraints = ifelse(rep(my_params$mono[run_index],length(var.monotone)), var.monotone, rep(0, length(var.monotone))),
    gamma = 0, 
    nfold = 5,  
    nthread = 4, 
    print_every_n = 100,
    early_stopping_rounds = 30)
  return ( data.frame(best_it = xgb_cv$best_iteration, xgb_cv$evaluation_log[xgb_cv$best_iteration,]) ) 
})
param_res = cbind(param_res, my_params)

ggplot(param_res, aes(depth, test_auc_mean, group = eta, color = factor(eta))) + geom_point() + geom_line() + 
  geom_errorbar(aes(ymin = test_auc_mean - test_auc_std, ymax = test_auc_mean + test_auc_std), alpha = 0.6, width = 0.1) + 
  facet_grid(mono ~ subsample)

ggplot(param_res, aes(depth, train_auc_mean/test_auc_mean, group = eta, color = factor(eta))) + geom_point() + 
  facet_grid(mono ~ subsample)
  

ggplot(param_res, aes(depth,  best_it, group = eta)) + geom_point() + facet_grid(mono ~ subsample)

### Param optimization ----------------
set.seed(132140937)
#Best Parameters Found:  Round = 29	eta = 0.1663	max_depth = 8.0000	subsample = 1.0000	Value = -522.3625 
#Best Parameters Found:  Round = 47	eta = 0.0335	max_depth = 13.0000	subsample = 0.7085	monotone = 0.0000	Value = -518.4975 
#Round = 98	eta = 0.2366	max_depth = 1.0000	subsample = 1.0000	monotone = 1.0000	min_child_weight = 6.0000	Value = 0.8965 

xgb_cv_bayes <- function(eta, max_depth, subsample, monotone, min_child_weight) {
  cv <- xgb.cv(params = list(eta = eta,
                             max_depth = max_depth,
                             subsample = subsample, 
                             min_child_weight = min_child_weight,
                             monotone_constraints = ifelse(rep(monotone==1,length(var.monotone)), var.monotone, rep(0, length(var.monotone))),
                             colsample_bytree = 1.0,
                             oobjective = "binary:logistic", eval_metric = "auc"),
               data = df_train_set, label = actual[tindex],
               nround = 2000, #2000
               nfold = 5,
               gamma = 0, 
               early_stopping_rounds = 30,  
               verbose = 0)
  gc(reset = TRUE)
  
  list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration], Pred = cv$best_iteration)
}
sink(file.path(working_folder,'santander_transaction/log.txt'), append=FALSE, split=TRUE)
OPT_Res <- BayesianOptimization(xgb_cv_bayes,
                                bounds = list(
                                  eta = c(0.001, 1.0),
                                  max_depth = c(1L, 10L),
                                  subsample = c(0.5, 1.0),
                                  monotone = c(0L, 1L),
                                  min_child_weight = c(1L, 10L)),
                                init_grid_dt = NULL, 
                                init_points = 10, #10
                                n_iter = 100,     #50
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
sink()

ggplot(OPT_Res$History, aes(eta, Value, size = min_child_weight)) + geom_point(alpha = 0.6) + geom_smooth() + facet_wrap(~monotone)
ggplot(OPT_Res$History, aes(subsample, Value, size = min_child_weight)) + geom_point(alpha = 0.6) + geom_smooth() + facet_wrap(~monotone)
ggplot(OPT_Res$History, aes(max_depth, Value, size = min_child_weight)) + geom_point(alpha = 0.3) + geom_smooth() + facet_wrap(~monotone)


```

## Save Results
gbm - 0.895
xgb - 0.898

```{r save_results}

submit = df[,.(ID_code, target = pred.xgb)]

submit = submit[df$is_train==FALSE,]

setorder(submit, ID_code)

file = file.path(working_folder, "santander_transaction/solution.csv")
  
fwrite(submit, file = file, row.names = FALSE)

zip(paste(file, '.zip', sep = ''), file)
  
print(file)

#fullVisitorId,PredictedLogRevenue

```