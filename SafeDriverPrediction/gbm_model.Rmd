---
title: "Porto Seguro’s Safe Driver Prediction"
author: "Alex"
date: "February 11, 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5, dpi = 240)
options(warn=-1)

library(gbm)
library(data.table)
library(plyr)
library(stringi)
library(ggplot2)
library(gridExtra)
library(zip)
library(xgboost)
#library(caret)

working_folder = 'C:/Dev/Kaggle/'

source(file.path(working_folder, '/Utils/common.R'))


```

## Data

```{r data}

load_data <- function(working_folder){
  
  data_file = file.path(working_folder,'SafeDriverPrediction/data/all_data.rds')

  if(file.exists(data_file)){
    df = readRDS(data_file)
  }else {
    df_train = fread(file.path(working_folder,'SafeDriverPrediction/data/train.csv') )
    df_test  = fread(file.path(working_folder,'SafeDriverPrediction/data/test.csv') )
    
    df_test[, target:=NA]
    
    df = rbind(df_test, df_train)
    
    #repalce -1 with NA
    for (c_name in names(df)) {
      set(df,which(df[[c_name]]==-1),c_name,NA)
    }
    
    #convert to factors
    cat_vars = names(df)[grepl('(_cat)|(_bin)',names(df))]
    df[, (cat_vars):=lapply(.SD, as.factor), .SDcols = cat_vars]
    
    int_columns = names(df)[which(lapply(df, class) == 'integer')] %!in_set% c('id')
    df[, (int_columns):=lapply(.SD, as.numeric), .SDcols = int_columns]
    
    saveRDS(df, data_file)
  }
   return (df)
}

df = load_data(working_folder)

#head(df)

test_index = is.na(df$target)
train_index = !is.na(df$target)

results = list()
```

## Data View

```{r data_view}
obj_var = 'target'
#var source: ind, reg, car, calc
#var types: bin, cat

c_names = names(df)
cat_vars = c_names[grepl('_cat',c_names)]

df_train = df[train_index,]

#sapply(df, levels)

#plot cat var counts
plots = llply(cat_vars, function(var_name) {
 p = ggplot(df_train, aes_string(var_name)) + geom_bar()  +
    ggtitle(var_name)
 return( p )
})
marrangeGrob(plots, nrow = 3, ncol = 3, top = NULL)

plots = llply(c('ps_car_11_cat', 'ps_car_01_cat','ps_car_06_cat','ps_car_04_cat'), function(var_name) {
 p = ggplot(df_train, aes_string(var_name)) + geom_bar()  +
    ggtitle(var_name) +  theme(title =element_text(size=8))
 return( p )
})
marrangeGrob(plots, nrow = 2, ncol = 2, top = NULL)


#outcome based sampling
set.seed(101)

n_events = sum(df$target[train_index] == 1)
n_non_events = sum(df$target[train_index] == 0)
n_total = length(df$target[train_index] )
non_event_sample_size = min(3*n_events,n_non_events)
sample_size = non_event_sample_size + n_events
non_event_sample = sample.int(n_non_events,non_event_sample_size,replace = FALSE)
sampling_correction = log(( sample_size - n_events ) / (n_total - n_events))
train_index_small = train_index
train_index_small = (df$target==1) & !is.na(df$target)
train_index_small[ which(df$target==0)[non_event_sample]] = TRUE

table(df$target[train_index])
table(df$target[train_index_small])

print(sprintf('%d (p = %.4f)', n_events, 100*n_events/n_total))

```

## GBM Model: All vars
AUC = 0.6353 (0.29154 - best score)
current: 0.27507 0.26964
0.27279
```{r gbm_model1}

actual = as.numeric(df$target)

#only keep several car_11 levels

df[,ps_car_11_cat_ex:=as.character(ps_car_11_cat)]
df[ps_car_11_cat %!in% c('49','51','50','62','84','20','17','18','94','58','63','81','72','35','97','85','46','65','21','35'), ps_car_11_cat_ex:='otr']
df[,ps_car_11_cat_ex:=factor(ps_car_11_cat_ex)]
table(df$ps_car_11_cat_ex[train_index_small])

#df[,ps_car_04_cat_ex:=as.character(ps_car_04_cat)]
#df[ps_car_04_cat %in% c('3','4','5','7','6'),ps_car_04_cat_ex:='otr']
#df[,ps_car_04_cat_ex:=factor(ps_car_04_cat_ex)]
#table(df$ps_car_04_cat_ex[train_index_small])

all_vars = names(df) %!in_set% c('id', 'target')
exclude_vars = c('ps_car_11_cat',
                 'ps_ind_11_bin','ps_ind_10_bin','ps_ind_13_bin', 'ps_calc_20_bin','ps_calc_20_bin','ps_calc_18_bin','ps_ind_12_bin','ps_ind_14','ps_calc_19_bin','ps_car_10_cat', 'ps_calc_17_bin', 'ps_ind_18_bin','ps_calc_15_bin', 'ps_calc_16_bin','ps_calc_01', 'ps_calc_09','ps_calc_12','ps_car_02_cat') #inf <0.1

set.seed(1012356)

formula.gbm = formula(stri_join( 'target ~ ', stri_join(all_vars %!in_set% exclude_vars,collapse = ' + ')))

model_vars = all.vars(formula.gbm) %!in_set% c('target')
var.monotone = rep(0, length(model_vars))
var.monotone[model_vars %in% c('ps_car_13', 'ps_reg_03', 'ps_reg_02', 'ps_reg_01', 'ps_car_15','ps_calc_10','ps_calc_03','ps_calc_08','ps_car_12','ps_car_02','ps_calc_14','ps_calc_02')]  = 1
var.monotone[model_vars %in% c('ps_ind_15', 'ps_car_14','ps_calc_13','ps_calc_04')]  =  -1

cv_folds = 0
max_it = 8100

model.gbm  = gbm(formula.gbm,
                 distribution = "bernoulli",
                 n.trees = max_it,
                 cv.folds = cv_folds,
                 shrinkage = 0.002,
                 interaction.depth=7,
                 train.fraction = 1.0,
                 bag.fraction = 0.7,# 0.5 for small samples, 0.7 for large
                 n.cores = 2,
                 var.monotone = var.monotone,
                 data = df[train_index_small, all.vars(formula.gbm), with = F],
                 verbose = FALSE)

saveRDS(model.gbm, file.path(working_folder,'SafeDriverPrediction/model.rds'))
#model.gbm = readRDS(file.path(working_folder,'SafeDriverPrediction/model.rds'))

plot_gbmiterations(model.gbm)

best_it.gbm = ifelse(cv_folds==0, max_it, gbm.perf(model.gbm, plot.it = F))

pred.gbm = predict(model.gbm, n.trees = best_it.gbm, type = 'response')

#influence
var_inf = summary(model.gbm, n.trees = best_it.gbm, plotit = F)
plot_gbminfluence(var_inf)
print(var_inf)

#interactions
var_interaction = gbm_interactions(model.gbm, df[train_index_small,], iter = best_it.gbm, min_influence = 1, degree = 2) 
plot_gbminteractions(subset(var_interaction, interaction_score>0.05))
print(var_interaction)

plots = plot_gbmpartial_2d(model.gbm, best_it.gbm, as.character(subset(var_interaction,interaction_score>0.1)$vars), output_type = 'response')
marrangeGrob(plots, nrow = 2, ncol = 2, top = NULL)

plot_binmodel_predictions(actual[train_index_small], pred.gbm)

plots = plot_gbmpartial(model.gbm, best_it.gbm, as.character(var_inf$var[var_inf$rel.inf>0.1]), output_type = 'response')
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plots = llply(all.vars(formula.gbm), function(var_name) {
  p = plot_profile(pred.gbm, actual[train_index_small],df[[var_name]][train_index_small], error_band = 'binom') +
    ggtitle(var_name) +  theme(title =element_text(size=8))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)
#roc.default(response = test$Class, predictor = gbm.test, plot = TRUE,     col = "red")

#save solution
pred.gbm_link = predict(model.gbm, n.trees = best_it.gbm, newdata = df, type = 'link')
pred.gbm_full = 1.0 / (1.0 + exp(-pred.gbm_link - sampling_correction))

results$gbm1 = pred.gbm_full

#var not in model
plots = llply(exclude_vars %in_set% names(df), function(var_name) {
  p = plot_profile(pred.gbm_full[train_index], actual[train_index],df[[var_name]][train_index], error_band = 'binom') + ggtitle(var_name) +  theme(title =element_text(size=8))
  return( p )
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

plot_profile(pred.gbm_full[train_index], actual[train_index],df[['ps_car_11_cat']][train_index], error_band = 'binom') 

plot_profile(pred.gbm_full[train_index], actual[train_index],df[['ps_car_11_cat_ex']][train_index], error_band = 'binom') 

p1 = plot_binmodel_percentiles(actual[train_index], pred.gbm_full[train_index], n = 20, equal_count_buckets = T)
p2 = plot_binmodel_percentiles(actual[train_index], pred.gbm_full[train_index], n = 20, equal_count_buckets = F)
grid.arrange(p1,p2)

#df_plot = data.table(act = actual[train_index], model = pred.gbm_full[train_index], prof = df[['ps_car_11_cat']][train_index])
#cc(df_plot[,.(.N,model = mean(model), act = mean(act)), by =.(prof)])
#ggplot(df_plot[,.(.N,model = mean(model), act = mean(act)), by =.(prof)], aes(prof, model)) + geom_point() + geom_point(aes(prof, act), color = 'red')
```

## XGBoost Model:

```{r xgb_model, eval = FALSE}
imp_vars = c('ps_car_13','ps_car_06_cat','ps_ind_05_cat','ps_reg_03')

dfs = df[train_index_small, all.vars(formula.gbm), with = F]
dfs <- sapply( dfs, as.numeric )

dtrain <- xgb.DMatrix(as.matrix(dfs), label = as.numeric(df[[obj_var]][train_index_small]) )

param <- list(
  train = dtrain,
  max_depth = 3, 
              eta = 0.002, 
              silent = 1, 
              nthread = 2,
              subsample = 0.7,
              objective = "binary:logistic",
              eval_metric = "auc",
              monotone_constraints = var.monotone)

bst <- xgb.train(param, dtrain, nrounds = 10)

```


## Output File

```{r output}

for (model_name in names(results) ){
  submit <- data.table(id = df$id[test_index], 
                       target = results[[model_name]][test_index])
  
  submit = submit[order(submit$id),]
  
  file = file.path(working_folder, sprintf("SafeDriverPrediction/my_solution_%s.csv", model_name))
  
  fwrite(submit, file = file, row.names = FALSE)
  
  #utils::zip(paste(file, '.zip', sep = ''), file, flags = "-r9X")
  zip(paste(file, '.zip', sep = ''), file)
  
  print(file)
}
```

