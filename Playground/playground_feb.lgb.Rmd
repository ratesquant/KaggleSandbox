---
title: 'Kaggle Playground: Feb 2020'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(stringi)
library(gbm)
library(ggplot2)
library(gridExtra)
#library(dplyr)
library(plyr)
library(corrplot)
library(xgboost)
#library(zip)
library(caret)
library(lightgbm)
library(forcats)
library(rBayesianOptimization)
library(tune) #https://datascienceplus.com/grid-search-and-bayesian-hyperparameter-optimization-using-tune-and-caret-packages/


working_folder = 'D:/Github/KaggleSandbox'
#working_folder = file.path(Sys.getenv("HOME"), 'source/github/KaggleSandbox/')

source(file.path(working_folder, 'Utils/common.R'))

rmsqr <-function(actual, model) {
  sqrt( mean( (actual - model) * (actual - model) ) )
}

```

## Load Data

```{r load_data}
load_existing = FALSE

if (load_existing) {
  df <- fread(file.path(working_folder,'Playground/Feb2021/data/df.csv'), check.names = TRUE)
  
} else{
  train <- fread(file.path(working_folder,'Playground/Feb2021/data/train.csv'), check.names = TRUE)
  test  <- fread(file.path(working_folder,'Playground/Feb2021/data/test.csv'),  check.names = TRUE) # 1459   80
  test[, target:=NA]
  df = rbind(train, test)
  
  gc(reset=TRUE)
}
  

test_index = is.na(df$target)
train_index = !test_index

obj_var = 'target'
all_vars = names(df) %!in_set% c('id', obj_var) #14 variables
all_vars = all_vars[grep('^(cont|cat)', all_vars)]
cat_vars = all_vars[grep('^(cat)', all_vars)]
con_vars = all_vars[grep('^(cont)', all_vars)]

df[, (cat_vars):=lapply(.SD, function(x) fct_infreq(fct_lump_min(x, 0.03*nrow(df), other_level = "OT"))), .SDcols = cat_vars]

plot_profiles <-function(model, data)
{
  #stri_join('p_',all_vars)
    plots = llply(all_vars, function(var_name) {
    p = plot_profile(model,  data[['target']], data[[var_name]], bucket_count = 20, error_band = 'norm') +
      ggtitle(var_name) +  theme(title =element_text(size=6))
    return( ggplotGrob(p) )
  })
  marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)
}

#plot_profiles(df$target_lgb[train_index], df[train_index,])

plot_profiles_2d <-function(model, data)
{
   all_comb = data.table(t(combn(all_vars, m = 2)) )
   all_comb = all_comb[V1!=V2]
   #all_comb = all_comb[1:36]
  
    plots = llply(seq(nrow(all_comb)), function(i) {
      var1 = all_comb$V1[i]
      var2 = all_comb$V2[i]
     p = ggplot(cbind(data, model), aes_string(var1, var2, z = 'target - model')) + stat_summary_hex(fun = function(x) ifelse(length(x)>100, mean(x), NA), bins = 10) + scale_fill_gradient2() +theme(title =element_text(size=6)) +  theme(legend.position = "None")
     #p = ggplot(data, aes_string(var1, var2)) + geom_hex(bins = 10) + theme(legend.position = "None")
    
    return( ggplotGrob(p) )
  })
  marrangeGrob(plots, nrow = 6, ncol = 6, top = NULL)
}

#plot_profiles_2d(df$target_lgb[train_index], df[train_index,])
partialPlot <- function(obj, pred.data, xname, n.pt = 19, discrete.x = FALSE, 
                        subsample = pmin(1, n.pt * 100 / nrow(pred.data)), which.class = NULL,
                        xlab = deparse(substitute(xname)), ylab = "", type = if (discrete.x) "p" else "b",
                        main = "", rug = TRUE, seed = NULL, ...) {
  stopifnot(dim(pred.data) >= 1)
  
  if (subsample < 1) {
    if (!is.null(seed)) {
      set.seed(seed)
    } 
    n <- nrow(pred.data)
    picked <- sample(n, trunc(subsample * n))
    pred.data <- pred.data[picked, , drop = FALSE]
  }
  xv <- pred.data[, xname]
  
  if (discrete.x) {
    x <- unique(xv)
  } else {
    x <- quantile(xv, seq(0.03, 0.97, length.out = n.pt), names = FALSE)
  }
  y <- numeric(length(x))
  
  isRanger <- inherits(obj, "ranger")
  isLm <- inherits(obj, "lm") | inherits(obj, "lmrob") | inherits(obj, "lmerMod")

  for (i in seq_along(x)) {
   pred.data[, xname] <- x[i]

    if (isRanger) {
      if (!is.null(which.class)) {
        if (obj$treetype != "Probability estimation") {
          stop("Choose probability = TRUE when fitting ranger multiclass model") 
        }
        preds <- predict(obj, pred.data)$predictions[, which.class]
      }
      else {
        preds <- predict(obj, pred.data)$predictions
      }
    } else if (isLm) {
      preds <- predict(obj, pred.data) 
    } else {
      if (!is.null(which.class)) {
        preds <- predict(obj, pred.data, reshape = TRUE)[, which.class + 1] 
      } else {
        preds <- predict(obj, pred.data)
      }
    }
    
    y[i] <- mean(preds)
  }
  
  #plot(x, y, xlab = xlab, ylab = ylab, main = main, type = type, ...)
  data.frame(x = x, y = y)
}

```
##Plot Data

```{r plot_data}

cor_mat = cor(data.matrix(df[train_index,c('target', all_vars), with = FALSE]), use = 'pairwise.complete.obs')
corrplot(cor_mat, method="number", number.cex = 0.8, number.digits = 2,  order="hclust")
corrplot(cor_mat, method="circle", number.cex = 0.5, order="hclust")

p_index = sample(which(train_index), 10000 )
ggplot(df[p_index], aes(id, target)) + geom_point()

ggplot(df[p_index], aes(cont8, target)) + geom_point() + geom_smooth()
ggplot(df[p_index], aes(cont3, target)) + geom_point() + geom_smooth()

#check sample
s_index = sample.int(nrow(df), nrow(df))
plots = llply(all_vars, function(var_name){
  ggplot(df[s_index ], aes_string(var_name, group = 'is.na(target)', color = 'is.na(target)')) + geom_density(adjust = 0.1) + ggtitle(var_name)
  })
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)

var_pairs = data.frame(t(combn(all_vars, 2, simplify = TRUE)))
plots = llply(seq(nrow(var_pairs)), function(i) { 
   ggplot(df[p_index ], aes_string(var_pairs$X1[i], var_pairs$X2[i])) + geom_point(alpha = 0.5)
  })
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)
#ggplot(melt(data.table(cor_mat)), aes(Var1, Var2, fill = value)) + geom_tile()

#check correlations

combn(as.character(all_vars), 2, simplify = TRUE)
all_comb = data.table(expand.grid(all_vars, all_vars, all_vars, all_vars))
#all_comb = all_comb[Var1 != Var2]

res = ldply(seq(nrow(all_comb)), function(i) { 
  a1 = df[[all_comb$Var1[i]]][p_index]
  a2 = df[[all_comb$Var2[i]]][p_index]
  a3 = df[[all_comb$Var3[i]]][p_index]
  a4 = df[[all_comb$Var4[i]]][p_index]
  
  data.frame(i, rho = cor(df[p_index, target], (a1 - a2)/ (2 + a3 - a4) , use = 'pairwise.complete.obs' ))  } )
setDT(res)
res[order(abs(rho))]
res[!is.na(rho)]
all_comb[38415    ]

ggplot(df[p_index], aes((cont13 / cont14 ), target)) + geom_point() + geom_smooth()

```

## DCT
does not help
```{r dct,  eval = FALSE}
library(dtt)

dct_vars = stri_join('dct_', con_vars)
dst_vars = stri_join('dst_', con_vars)
dht_vars = stri_join('dht_', con_vars)

df_dct =  data.table(dct(df[, con_vars, with = FALSE], inverted = FALSE))
names(df_dct) <- dct_vars
df = cbind(df, df_dct)

df_dst =  data.table(dst(df[, con_vars, with = FALSE], inverted = FALSE))
names(df_dst) <- dst_vars
df = cbind(df, df_dst)

df_dht =  data.table(dht(df[, con_vars, with = FALSE], inverted = FALSE))
names(df_dht) <- dht_vars
df = cbind(df, df_dht)

```



## Winsorization

```{r Winsorization,  eval = FALSE}

winsoraze<-function(x, xt, alpha = 0.05) {
  q_bounds = quantile(xt, c(alpha/2, 1- alpha/2))
  x = pmax(pmin(x, q_bounds[2]), q_bounds[1])
  return (x)
}

w_vars = stri_join('w_', all_vars)
df[, (w_vars):=lapply(.SD, function(x) winsoraze(x, x[train_index], 0.009123116) ), .SDcols = all_vars]

#d_vars = stri_join('d6_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 6 ), .SDcols = all_vars]

#d_vars = stri_join('d7_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 7 ), .SDcols = all_vars]

```

## TSNE
takes a really long time
```{r tsne,  eval = FALSE}
library(Rtsne)

set.seed(132140937)

c_index = sample.int(nrow(df), 10000)

tsne <- Rtsne(df[c_index, con_vars, with = FALSE], perplexity = 30, num_threads = 2, verbose = 1, theta=0.5, max_iter = 1000)

ggplot(data.frame(tsne$Y, label = factor(df[c_index, is.na(target)]) ), aes(X1, X2, group = label, color = label)) + geom_point()

#head(tsne$Y)
#all data 
#tsne_all <- Rtsne(df[, con_vars, with = FALSE], perplexity = 100)

#fwrite(data.frame(id = df$id[c_index], x = tsne$Y ), file.path(working_folder,'Playground/Feb2021/data/tsne.csv'))

#all 
tsne <- Rtsne(df[, con_vars, with = FALSE], perplexity = 30, num_threads = 2, verbose = 1, theta=0.5, max_iter = 1000)
fwrite(data.frame(id = df$id, x = tsne$Y ), file.path(working_folder,'Playground/Feb2021/data/tsne.csv'))

df = cbind(df, tsne1 = tsne$Y[,1], tsne2 = tsne$Y[,1])

```


## Count vars

```{r count_vars, eval = FALSE}

count_rows<-function(x, alpha = 0.5) {
  print(x)
  return ( sum(as.numeric(x > alpha)) )
}

df[, v_max := apply(.SD, 1, max ), .SDcols = all_vars]
df[, v_min := apply(.SD, 1, min ), .SDcols = all_vars]
df[, v7 := apply(.SD, 1, function(x) sum(as.numeric(x>0.7)) ), .SDcols = all_vars]
df[, v9 := apply(.SD, 1, function(x) sum(as.numeric(x>0.9)) ), .SDcols = all_vars]

ggplot(df[p_index, ], aes(temp, target )) + geom_point(alpha = 0.2) + geom_smooth()

#d_vars = stri_join('d6_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 6 ), .SDcols = all_vars]

#d_vars = stri_join('d7_', all_vars)
#df[, (d_vars):=lapply(.SD, function(x) (100*x) %% 7 ), .SDcols = all_vars]

```


#LightGBM
Round = 62	learning_rate = 0.0092	bagging_fraction = 0.8911	min_data = 959.0000	num_leaves = 16.0000	min_data_in_bin = 4.0000	Value = -0.8499 

```{r light_gbm, eval = FALSE}

set.seed(132140937)

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

lgb_vars = all_vars
#lgb_vars = c(all_vars, 'tsne1', 'tsne2') 
#lgb_vars = c(all_vars, dct_vars, dst_vars, dht_vars) 
#lgb_vars = stri_join('p_', all_vars)
#lgb_vars = stri_join('w_', all_vars) #0.6564345

#lgb_vars = c(stri_join('w_', all_vars), stri_join('d6_', all_vars), stri_join('d7_', all_vars))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
params <- list(objective = "regression", metric = "rmse")
#params <- list(objective = "huber")

var.monotone = rep(0, length(lgb_vars))
mon_inc_vars = c()
mon_dec_vars = c()
var.monotone[lgb_vars %in% mon_inc_vars]  =  1
var.monotone[lgb_vars %in% mon_dec_vars]  = -1


set.seed(140937345)

model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nrounds = 10000,
  nfold = 10,
  num_threads = 5, 
  
  monotone_constraints= var.monotone,
  monotone_constraints_method  = 'intermediate',
  
  min_data = 974,
  learning_rate = 0.0087,
  num_leaves = 15,
  bagging_fraction = 0.9192,
  min_data_in_bin = 5,
  
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100,
  force_col_wise=TRUE
)

#best.iter = model.lgb$best_iter #
#model.lgb$best_score #0.6983437

cv_error = as.numeric(model.lgb$record_evals$valid$rmse$eval)
ggplot(data.frame( i = seq(length(cv_error)), cv_error ), aes(i, cv_error)) + geom_line()

min(cv_error)

dm_all = data.matrix(df[,lgb_vars, with = F])
pred.lgb_cvi = ldply(seq(length(model.lgb$boosters)), function(i){ data.frame(cv = i, id = df$id, pred=  predict(model.lgb$boosters[[i]], dm_all)$booster) } )
setDT(pred.lgb_cvi)

pred.lgb_cv_summary = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(cv)]
pred.lgb_cv         = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(id)]

#var_imp   = lgb.importance(model.lgb$boosters[[1]][[1]], percentage = TRUE)
#lgb.plot.importance(var_imp, top_n = 20, measure = "Gain")

rmsqr(df$target[train_index], pred.lgb_cv$avg[train_index] )

#%% Plot -----
#plot_profiles(df$target_lgb[train_index], df[train_index,])
#plot_profiles_2d(df$target_lgb[p_index], df[p_index,])

df[pred.lgb_cv, target_lgb :=  i.avg, on=.(id)]

lgb_importance = lgb.importance(model.lgb$boosters[[1]][[1]], percentage = TRUE)
lgb.plot.interpretation(lgb_importance)
#
p_index = sample(which(train_index), 10000)
ggplot(df[p_index, ], aes(target_lgb, target)) + geom_point(alpha = 0.2) + geom_abline(slope = 1, color = 'red')
#ggplot(df[p_index, ], aes(p_cont3, target)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE)
ggplot(df[p_index, ], aes(sin(10*cont7), target)) + geom_point(alpha = 0.5) + geom_smooth(se = FALSE, span = 0.1)
ggplot(df[p_index, ], aes(cont7, sin(10*cont7))) + geom_point(alpha = 0.5)

#all y ~ x counts
plots = llply(lgb_vars, function(vname){
  p = ggplot(df[train_index & target >= 4, ], aes_string(vname, 'target')) + stat_bin_2d(bins = 200) +ggtitle(vname) + 
    theme(axis.title.x = element_blank(), legend.position = 'none')
})
marrangeGrob(plots, nrow = 5, ncol = 5, top = NULL)


ggplot(df[train_index & target > 4, ], aes(cont13, cont3)) + stat_bin_2d(bins = 100)
ggplot(df[train_index & target > 4, ], aes(cont13, cont3, z = target)) + stat_summary_2d(bins = 100)
ggplot(df[train_index & target > 4, ], aes(cont13, cont3, z =target_lgb)) + stat_summary_2d(bins = 100)
ggplot(df[train_index & target > 4, ], aes(cont10, target)) + stat_bin_2d(bins = 200)

ggplot(df[p_index, ], aes(sample = target_lgb - target)) + stat_qq() + stat_qq_line()

ggplot(df[p_index, ], aes(target_lgb, target)) + geom_point(alpha = 0.2) + geom_abline(slope = 1, color = 'red')
ggplot(df[p_index, ], aes(target_lgb, target  - target_lgb)) + geom_point(alpha = 0.2)
ggplot(df[p_index, ], aes(target_lgb, sqrt((target - target_lgb)^2) )) + geom_point(alpha = 0.2)
ggplot(df[p_index, ], aes(sample = (target - target_lgb) )) + stat_qq()


ggplot(df[train_index, ], aes(target)) + stat_ecdf()
ggplot(df[train_index, ], aes(target_lgb - target)) + stat_ecdf()

plot_profile(df[train_index, target],  df[train_index, target_lgb], 
             df[train_index,  cont8], bucket_count = 50, error_band = 'norm') 

plot_profiles(df$target_lgb[train_index], df[train_index,])

#cor(df[train_index, .(target, shift(cont13, 100) )], use = 'pairwise.complete.obs')

pdp_index = sample(which(train_index), 1000)

my_model = model.lgb$boosters[[1]][[1]]
df_plot = partialPlot(my_model, data.matrix(df[pdp_index,lgb_vars, with = FALSE]), xname = "cont1")
ggplot(df_plot, aes(x, y)) + geom_line()

plots = llply(lgb_vars %!in_set% cat_vars, function(vname){
  df_plot = partialPlot(my_model, data.matrix(df[pdp_index,lgb_vars, with = FALSE]), xname = vname, n.pt = 100)
  df_plot[vname] = df_plot$x
  p = ggplot(df_plot, aes_string(vname, 'y')) + geom_line() +
    geom_rug(data = df[pdp_index,c(vname, 'target'), with = FALSE], aes_string(x = vname), sides = 'b', alpha = 0.2, size = 0.1, inherit.aes = FALSE)
})
marrangeGrob(plots, nrow = 4, ncol = 4, top = NULL)

```

#LightGBM: Bagging

```{r light_gbm_bagging, eval = FALSE}

set.seed(132140937)

n_runs = 30

for (run_id in 1:n_runs){

t_index_v = which(train_index)
#t_index_v1 = sample(t_index_v, 1.0*length(t_index_v), replace = TRUE)
t_index_v1 = sample(t_index_v, 0.8*length(t_index_v), replace = FALSE)

lgb_vars = all_vars

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
params <- list(objective = "regression", metric = "rmse")

model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nrounds = 20000,
  nfold = 10,
  num_threads = 5, 
  
  #monotone_constraints= var.monotone,
  #monotone_constraints_method  = 'intermediate',
  
  min_data = 974,
  learning_rate = 0.0087,
  num_leaves = 15,
  bagging_fraction = 0.9192,
  min_data_in_bin = 5,
  
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100,
  force_col_wise=TRUE
)

#best.iter = model.lgb$best_iter #
#model.lgb$best_score #0.6983437

cv_error = as.numeric(model.lgb$record_evals$valid$rmse$eval)
#ggplot(data.frame( i = seq(length(cv_error)), cv_error ), aes(i, cv_error)) + geom_line()

min(cv_error)

dm_all = data.matrix(df[,lgb_vars, with = F])
pred.lgb_cvi = ldply(seq(length(model.lgb$boosters)), function(i){ data.frame(cv = i, id = df$id, pred=  predict(model.lgb$boosters[[i]], dm_all)$booster) } )
setDT(pred.lgb_cvi)

pred.lgb_cv_summary = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(cv)]
pred.lgb_cv         = pred.lgb_cvi[, .(.N, avg=mean(pred), sigma = sd(pred)), by =.(id)]

my_rms = rmsqr(df$target[train_index], pred.lgb_cv$avg[train_index] )

df[pred.lgb_cv, target_lgb :=  i.avg, on=.(id)]

print(sprintf('%d, best_it: %d, best_cv: %f, rms: %f', run_id,  model.lgb$best_iter, model.lgb$best_score, my_rms ))

fwrite(df, file.path(working_folder, sprintf('Playground/Feb2021/data/df_bag_%d.csv', run_id)))

}

```


#LightGBM: Read Bagging results
best so far: 0.84309
10 runs without replacement (70%) 0.84318
30 runs without replacement (80%) 0.84310
```{r light_gbm_bagging_results, eval = FALSE}
library(foreach)

 bag_files = list.files(file.path(working_folder,'Playground/Feb2021/data/'), pattern = glob2rx('df_bag_*.csv'), full.names = TRUE)

 df_t = foreach(name = bag_files, .combine = rbind) %do% {
  df_t = fread(name)
  df_t[, name := basename(name) ]
  return(df_t)
 }
 
 df_res = df_t[, .(.N, target = mean(target_lgb, na.rm = TRUE)), by =.(id)]
 
  file = file.path(working_folder, "Playground/Feb2021/submit_v8.lgb.csv")
  fwrite(df_res[id %in% df$id[test_index] , .(id, target)], file = file, row.names = FALSE)
  zip(paste(file, '.zip', sep = ''), file, flags = '-r9Xj')
  print(file)
```


# Regress error
```{r error_regression, eval = FALSE}
df[, error_lgb := target - target_lgb]

sqrt(mean(df$error_lgb[train_index]^2))

formula.knn    = formula(stri_join( 'error_lgb', ' ~ ', stri_join(unique(all_vars), collapse = ' + ')))

#control = trainControl(method = "repeatedcv", number = 10,repeats = 3)
control = trainControl("cv", number = 10)

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, all.vars(formula.knn), with = FALSE]
system.time(model.knn <- train(formula.knn, data = dfs, 
                               method = "knn", #kknn
                               trControl = control,
                               tuneGrid = data.frame(k = seq(200, 500,50)), #use instead of tuneLength
                               tuneLength = 10,
                               metric = "RMSE"))
model.knn
plot(model.knn)

pred.knn = predict(model.knn, df, type = 'raw')

```

#LightGBM: NON-CV

```{r light_gbm_non_cv, eval = FALSE}

set.seed(132140937)

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

lgb_vars = all_vars
#lgb_vars = stri_join('p_', all_vars)
#lgb_vars = stri_join('w_', all_vars) #0.6564345

#lgb_vars = c(stri_join('w_', all_vars), stri_join('d6_', all_vars), stri_join('d7_', all_vars))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
params <- list(objective = "regression", metric = "rmse")
#params <- list(objective = "huber")

set.seed(140937345)

model.lgb <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 6100,
  num_threads = 5, 
  
  min_data = 959,
  learning_rate = 0.0092,
  num_leaves = 16,
  bagging_fraction = 0.8911,
  min_data_in_bin = 4,
  
  boost_from_average = TRUE,
  eval_freq = 100,
  force_col_wise=TRUE
)


pred.lgb = predict(model.lgb, data.matrix(df[,lgb_vars, with = F]))
df[, target_lgb :=  pred.lgb]
#df[, target_lgb :=  predict(model.lgb, data.matrix(df[,lgb_vars, with = F]))]

lgb_importance = lgb.importance(model.lgb, percentage = TRUE)
lgb.plot.interpretation(lgb_importance)

```

#LightGBM Tuning
 best_it best_score  elapsed learning_rate bagging_fraction min_data min_data_in_bin num_leaves rank
1:    2521   0.843085 3.575185   0.007635167        0.9311752      619               9         72    1
1:    2521   0.843085 3.575185   0.007635167        0.95           700               3         70    1
1:    3132   0.842883 3.246344   0.007594201        0.9331378      932               6         42    1

   best_it best_score  elapsed learning_rate bagging_fraction min_data min_data_in_bin num_leaves rank
1:    7346  0.8426274 3.710974    0.00821259         0.967491     1152               6         12    1

```{r light_gbm_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = stri_join('w_', all_vars)
#lgb_vars = c(all_vars, 'target_knn')

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

params <- list(objective = "regression", metric = "rmse")

my_params = data.table(
                       learning_rate = 0.00821259,#0.0110, 
                       bagging_fraction = 0.967491, 
                       min_data = 1152, #default 20
                       min_data_in_bin = 6, #default: 3
                       num_leaves = seq(10, 20) )  #default: 31

n_runs = 100 # 10 runs per hour
my_params = data.table(
                       learning_rate = runif(n_runs, 0.007, 0.009), 
                       bagging_fraction = runif(n_runs, 0.9, 1.0), 
                       min_data = sample(seq(from = 850, to = 1200),n_runs, TRUE),
                       min_data_in_bin = sample(seq(5, 7),n_runs, TRUE),
                       num_leaves = sample(seq(10, 30),n_runs, TRUE))

monotone_constraints 
param_res_raw = ldply(seq(nrow(my_params)), function(run_index){
  #run_index = 1
  print(run_index)
  print(my_params[run_index,])
  
  start_time <- Sys.time()
  
  dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
  
  model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nfold = 5,
  num_threads = 5, 
  verbose = 0,
  force_col_wise=TRUE,
  
  learning_rate = my_params$learning_rate[run_index],
  bagging_fraction = my_params$bagging_fraction[run_index],
  min_data = my_params$min_data[run_index],
  num_leaves = my_params$num_leaves[run_index],
  min_data_in_bin = my_params$min_data_in_bin[run_index],
  
  nrounds = 10000,
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100)
  
  print(model.lgb$best_score)
  
  gc(reset = TRUE)
  elapsed = as.numeric(difftime(Sys.time(),start_time,units="secs"))/60
  return ( data.frame(best_it = model.lgb$best_iter, best_score = model.lgb$best_score, elapsed = elapsed ) ) 
})

param_res = cbind(param_res_raw, my_params)
setDT(param_res)
setorder(param_res, best_score)
param_res[, rank:=seq(nrow(param_res))]
param_res[best_score  == min(best_score)]

#learning_rate = 0.008, bagging_fraction=0.95, min_data = 950, min_data_in_bin = 6

ggplot(param_res, aes(best_it, elapsed)) + geom_point()
ggplot(param_res, aes(best_it, learning_rate)) + geom_point()
ggplot(param_res, aes(learning_rate, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(bagging_fraction, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(min_data, best_score)) + geom_point()+ geom_smooth()
ggplot(param_res, aes(min_data_in_bin, best_score)) + geom_point() + geom_smooth(span  = 0.5)
ggplot(param_res, aes(num_leaves, best_score)) + geom_point()  + geom_smooth(span  = 0.5)

```

#LightGBM Tuning: 1D
 
```{r light_gbm_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = stri_join('w_', all_vars)
#lgb_vars = c(all_vars, 'target_knn')

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

params <- list(objective = "regression", metric = "rmse")

my_params = data.table(cat_min  = seq(0.01, 0.1, by = 0.01))  #default: 31

param_res_raw = ldply(seq(nrow(my_params)), function(run_index){
  #run_index = 1
  print(run_index)
  print(my_params[run_index,])
  
  start_time <- Sys.time()
  
  
  dfs[, (cat_vars):=lapply(.SD, function(x) fct_infreq(fct_lump_min(x, my_params$cat_min[run_index]*nrow(dfs), other_level = "OT"))), .SDcols = cat_vars]
  
  dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
  
  model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nfold = 10,
  num_threads = 5, 
  verbose = 0,
  force_col_wise=TRUE,

  min_data = 974,
  learning_rate = 0.0087,
  num_leaves = 15,
  bagging_fraction = 0.9192,
  min_data_in_bin = 5,
  
  nrounds = 10000,
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100)
  
  print(model.lgb$best_score)
  
  gc(reset = TRUE)
  elapsed = as.numeric(difftime(Sys.time(),start_time,units="secs"))/60
  return ( data.frame(best_it = model.lgb$best_iter, best_score = model.lgb$best_score, elapsed = elapsed ) ) 
})

param_res = cbind(param_res_raw, my_params)
setDT(param_res)
setorder(param_res, best_score)
param_res[, rank:=seq(nrow(param_res))]
param_res[best_score  == min(best_score)]

#learning_rate = 0.008, bagging_fraction=0.95, min_data = 950, min_data_in_bin = 6

ggplot(param_res, aes(best_it, elapsed)) + geom_point()
ggplot(param_res, aes(cat_min, best_score)) + geom_point()
ggplot(param_res, aes(learning_rate, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(bagging_fraction, best_score)) + geom_point() + geom_smooth()
ggplot(param_res, aes(min_data, best_score)) + geom_point()+ geom_smooth()
ggplot(param_res, aes(min_data_in_bin, best_score)) + geom_point() + geom_smooth(span  = 0.5)
ggplot(param_res, aes(num_leaves, best_score)) + geom_point()  + geom_smooth(span  = 0.5)

```

#LightGBM Tuning: Mono
```{r light_gbm_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = stri_join('w_', all_vars)
#lgb_vars = c(all_vars, 'target_knn')

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

params <- list(objective = "regression", metric = "rmse")

n_cat_vars = length(cat_vars)
n_con_vars = length(lgb_vars)-n_cat_vars
n_runs = 1 + 2*n_con_vars # 10 runs per hour
my_params = matrix(rep(0, length(lgb_vars) * n_runs), nrow = n_runs )

for(i in seq(1,n_runs-1, by =2) ){
  my_params[i  ,n_cat_vars + 1 + (i%/%2) %% n_con_vars] =  1
  my_params[i+1,n_cat_vars + 1 + (i%/%2) %% n_con_vars] = -1
}

n_runs = 1
my_params =  matrix(sample(c(-1, 0, 1), length(lgb_vars) * n_runs, replace = TRUE), nrow = n_runs )
my_params[,1:n_cat_vars] = 0
my_params[,] = 0
#all monotonicity combinations
#data.table(expand.grid(V11 = c(-1, 0, 1), V12 = c(-1, 0, 1), V13 = c(-1, 0, 1), V14 = c(-1, 0, 1), V15 = c(-1, 0, 1), V16 = c(-1, 0, 1), V17 = c(-1, 0, 1), V18 = c(-1, 0, 1),V18 = c(-1, 0, 1) ))

my_params = data.table( my_params)
#my_params[, V15 := 1] #0.8426315 
 
param_res_raw = ldply(seq(nrow(my_params)), function(run_index){
  #run_index = 1
  print(run_index)
  print(my_params[run_index,])
  
  start_time <- Sys.time()
  
  dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
  
  
  model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nfold = 5,
  num_threads = 5, 
  verbose = 0,
  force_col_wise=TRUE,
  
  monotone_constraints= as.numeric(my_params[run_index,]),
  
  min_data = 974,
  learning_rate = 0.0087,
  num_leaves = 15,
  bagging_fraction = 0.9192,
  min_data_in_bin = 5,
  
  monotone_constraints_method  = 'intermediate',
  nrounds = 10000,
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100)
  
  print(model.lgb$best_score)
  
  gc(reset = TRUE)
  elapsed = as.numeric(difftime(Sys.time(),start_time,units="secs"))/60
  return ( data.frame(best_it = model.lgb$best_iter, best_score = model.lgb$best_score, elapsed = elapsed, run_index= run_index ) ) 
})

setDT(param_res_raw)

param_res = cbind(param_res_raw, my_params)
#param_res_raw[best_score  == min(best_score)]
#param_res = param_res_raw[my_params, on =.(run_index)]
#setorder(param_res, best_score)
param_res[, score_diff := best_score - min(best_score)]
param_res[best_score  == min(best_score)]
param_res[, score_diff := best_score - min(best_score) ]

#learning_rate = 0.008, bagging_fraction=0.95, min_data = 950, min_data_in_bin = 6
ggplot(param_res, aes(run_index , best_score)) + geom_point()

```

#LightGBM Tuning: Var Exclusion
    best_it best_score  elapsed run_index    var
 1:    5911  0.8426344 6.822805         8   cat7
 2:    6337  0.8427150 7.585254        13  cont2
 3:    7387  0.8427190 8.681920        15  cont4
 4:    6469  0.8427234 7.527228         5   cat4
 5:    6548  0.8427421 7.801312         6   cat5
```{r light_gbm_var_exclude, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4]

params <- list(objective = "regression", metric = "rmse")

my_params = data.table( var = c('', all_vars))

param_res_raw = ldply(seq(nrow(my_params)), function(run_index){
  #run_index = 1
  print(run_index)
  print(my_params[run_index,])
  
  start_time <- Sys.time()
  
  dtrain <- lgb.Dataset(data.matrix(dfs[, lgb_vars %!in_set% my_params[run_index,] , with = FALSE]), label = dfs$target, categorical_feature = cat_vars %!in_set% my_params[run_index,])
  
  
  model.lgb <- lgb.cv(
  params = params,
  data = dtrain,
  nfold = 10,
  num_threads = 5, 
  verbose = 0,
  force_col_wise=TRUE,
  
  min_data = 974,
  learning_rate = 0.0087,
  num_leaves = 15,
  bagging_fraction = 0.9192,
  min_data_in_bin = 5,
  
  monotone_constraints_method  = 'intermediate',
  nrounds = 10000,
  boost_from_average = TRUE,
  eval_freq = 100,
  early_stopping_rounds = 100)
  
  print(model.lgb$best_score)
  
  gc(reset = TRUE)
  elapsed = as.numeric(difftime(Sys.time(),start_time,units="secs"))/60
  return ( data.frame(best_it = model.lgb$best_iter, best_score = model.lgb$best_score, elapsed = elapsed, run_index= run_index ) ) 
})

setDT(param_res_raw)

param_res = cbind(param_res_raw, my_params)
#param_res_raw[best_score  == min(best_score)]
#param_res = param_res_raw[my_params, on =.(run_index)]
#setorder(param_res, best_score)
param_res[, score_diff := best_score - min(best_score)]
param_res[best_score  == min(best_score)]
param_res[, score_diff := best_score - min(best_score) ]

#learning_rate = 0.008, bagging_fraction=0.95, min_data = 950, min_data_in_bin = 6
ggplot(param_res, aes(run_index , best_score)) + geom_point()

```


#LightGBM Bayes Tuning
Round = 14	learning_rate = 0.0092	bagging_fraction = 0.9005	min_data = 818.0000	num_leaves = 14.0000	min_data_in_bin = 6.0000	Value = -0.8501 
Round = 62	learning_rate = 0.0092	bagging_fraction = 0.8911	min_data = 959.0000	num_leaves = 16.0000	min_data_in_bin = 4.0000	Value = -0.8499 
Round = 14	learning_rate = 0.0087	bagging_fraction = 0.9192	min_data = 974.0000	num_leaves = 15.0000	min_data_in_bin = 5.0000	Value = -0.8581 
```{r light_gbm_bayes_tune, eval = FALSE}

set.seed(132140937)

lgb_vars = all_vars
#lgb_vars = all_vars

t_index_v = which(train_index)
t_index_v1 = sample(t_index_v, 1.0*length(t_index_v))

dfs = df[t_index_v1, c('target',lgb_vars), with = FALSE]
dfs = dfs[target >= 4] #exclude target < 5.0


params <- list(objective = "regression", metric = "rmse")

lgb_cv_bayes <- function(learning_rate, bagging_fraction, min_data, num_leaves, min_data_in_bin) {
    #set.seed(140937345)
  
    dtrain <- lgb.Dataset(as.matrix(dfs[, lgb_vars , with = FALSE]), label = dfs$target, categorical_feature = cat_vars)
    
    model.lgb <- lgb.cv(
    params = params,
    data = dtrain,
    nfold = 10,
    num_threads = 5, 
    verbose = 0,
    
    learning_rate = learning_rate,
    bagging_fraction = bagging_fraction,
    min_data =min_data,
    num_leaves = num_leaves,
    min_data_in_bin = min_data_in_bin,
    
    nrounds = 10000,
    boost_from_average = TRUE,
    eval_freq = 100,
    early_stopping_rounds = 100,
    force_col_wise=TRUE)
   
  gc(reset = TRUE)
  
  list(Score = -model.lgb$best_score, Pred =  model.lgb$best_iter)
}

OPT_Res <- BayesianOptimization(lgb_cv_bayes,
                                bounds = list(
                                learning_rate = c(0.008, 0.012),
                                bagging_fraction = c(0.85, 0.95), #default: 1.0
                                min_data = c(800L, 1200L), #default: 20
                                num_leaves =c(12L, 20L), #default: 31
                                min_data_in_bin =c(3L, 6L)), #default: 3
                                init_grid_dt = NULL, 
                                init_points = 5, #10
                                n_iter = 100,     #50
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
opt_res = data.table(OPT_Res$History, it_count=as.numeric(OPT_Res$Pred))
setorder(opt_res, Value)

opt_res[Value == max(Value)]

ggplot(opt_res, aes(Round , -Value)) + geom_line()
ggplot(opt_res, aes(learning_rate, -Value)) + geom_point()
ggplot(opt_res, aes(bagging_fraction, -Value)) + geom_point()
ggplot(opt_res, aes(min_data, -Value)) + geom_point()
ggplot(opt_res, aes(num_leaves, -Value)) + geom_point()
ggplot(opt_res, aes(min_data_in_bin, -Value)) + geom_point()
ggplot(opt_res, aes(Round, -Value)) + geom_point()
#ggplot(opt_res, aes(Round, it_count)) + geom_line(alpha = 0.6)

```

#Bayes test
```{r bayes_tune, eval = FALSE}

test_bayes <- function(x, y) {
  
  a = 1
  b = 100
  
  list(Score = - ((a - x)^2 + b*(y - x*x)^2), Pred =  model.lgb$best_iter)
}

OPT_Res <- BayesianOptimization(test_bayes,
                                bounds = list(
                                x = c(-10, 10),
                                y = c(-10, 10)),
                                init_points = 10, 
                                n_iter = 50,
                                acq = "ucb", kappa = 2.576, eps = 0.0,
                                verbose = TRUE)
opt_res = data.table(OPT_Res$History, it_count=as.numeric(OPT_Res$Pred))
setorder(opt_res, Value)

opt_res[Value == max(Value)]

ggplot(opt_res, aes(x, y)) + geom_path() + geom_point()

ggplot(opt_res, aes(x, log(-Value) )) + geom_point() + geom_vline(xintercept = 1)
ggplot(opt_res, aes(y, log(-Value) )) + geom_point() + geom_vline(xintercept = 1)

```

#Submit
MY BEST: 0.84357
v1: 0.84357 min(0.8431303)
v2: 0.84324 min(0.8426456)
v3: 0.84317 min(0.8426591)
v4: 0.84327
v5: 0.84317
v6: 0.84309
v7: 0.84398
    0.84309
```{r submit, echo=FALSE}
  #fwrite(df, file.path(working_folder,'Playground/Feb2021/data/df.csv'))
 
  file = file.path(working_folder, "Playground/Feb2021/submit_v7.lgb.csv")
  fwrite(df[test_index, .(id, target=target_lgb)], file = file, row.names = FALSE)
  zip(paste(file, '.zip', sep = ''), file, flags = '-r9Xj')
  print(file)

```


