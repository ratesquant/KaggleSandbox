---
title: "Optimization Primer"
output: html_document
date: '2022-10-13'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(rBayesianOptimization)
library(dfoptim)
library(pso)
library(optimx)
library(DEoptim)
library(proxy)
library(MASS)
library(data.table)
library(ggplot2)

# YMMV, MLSL
library(nloptr)

```

## 1D

```{r opt_1d}

objective_1d <- function(x){  -exp(-(x - 2)^2) }

Test_Fun <- function(x) {
  list(Score = -objective_1d(x),   Pred = 0)
}
####  BayesianOptimization --------
#### Round = 14	x = 1.99999	Value = -9.058776e-11 
OPT_Res <- BayesianOptimization(Test_Fun,
                                bounds = list(x = c(1, 3)),
                                init_points = 3, n_iter = 20,
                                verbose = TRUE)


optimize(objective_1d, c(-6, 6), maximum = FALSE) #2.000866

opt_control = list(trace = 2)
res = optim(c(3), objective_1d, method = 'BFGS', control = opt_control)
res = optim(c(3), objective_1d, method = 'L-BFGS-B', control = opt_control)
res = optim(c(3), objective_1d, method = "CG", control = opt_control)
res = optim(c(3), objective_1d, method = "SANN", control = opt_control)
res = optim(c(3), objective_1d, method = "Brent", lower = -6, upper = 6, control = opt_control) #same as optimize
```


```{r rbf_optim}
rbf_optim <- function(x0, obj_fun, lower=-2, upper=2, max_it = 100){

  rbf_linear_kernel <- function(x) x
  rbf_sqrt_kernel <- function(x) sqrt(x)
  rbf_cubic_kernel <- function(x) x*x*x
  kernel_fun <- rbf_cubic_kernel
  #kernel_fun <- rbf_sqrt_kernel
  
  x0 <- c(0, 0)
  n = length(x0) #number of parameters
  n_mult = 10
  xr = runif(n_mult*n * n, min = lower, max = upper)
  
  XM = matrix(c(x0, xr), nrow = n_mult*n + 1, byrow = TRUE)
  Y = apply(XM, 1, obj_fun)
    
  for(i in 1:max_it) {
    
    M = cbind(1, kernel_fun(dist(XM, XM, method = 'L2')) ) 
    #w = ginv(t(M) %*% M) %*% t(M) %*% Y #least squares
    w = ginv(M) %*% Y  #solve
    
    if(FALSE){
      nodes = X[1:3, ]
      M = kernel_fun(dist(X, nodes, method = 'L2'))
      pred = w[1] +  w[-1] %*% M
    
      xt = data.table( expand.grid(x = seq(-2, 2, by =0.1), y = seq(-2, 2, by =0.1)) )
      xt[, f:= apply(xt, 1, obj_fun)]
      M = kernel_fun(dist(XM, xt[,.(x, y)], method = 'L2'))
      xt[, rbf:=as.numeric(w[1] +  w[-1] %*% M) ]
      
      ggplot(xt, aes(x, y, fill = asinh(f) )) + geom_tile()
      ggplot(xt, aes(x, y, fill = asinh(rbf) )) + geom_tile() + geom_point(data = data.frame(XM), aes(X1, X2), color = 'red', inherit.aes = FALSE)+ scale_fill_distiller(palette ='YlGnBu')
      ggplot(xt, aes(x, y, fill = asinh(f) )) + geom_tile()+ geom_point(data = data.frame(XM, Y), aes(X1, X2 ), color = 'red' , inherit.aes = FALSE) + scale_fill_distiller(palette ='YlGnBu')
      #ggplot(xt, aes(x, y, fill = rbf )) + geom_tile() + geom_point(data = data.frame(XM), aes(X1, X2), color = 'red', inherit.aes = FALSE)
    }
    
    #rbf solver
    #XM[which.min(Y),]
    #XM[sample.int(nrow(XM), 1),]
    opt_par = optim(runif(n, min = lower, max = upper), function(x)  w[1] +  w[-1] %*% kernel_fun(dist(XM, matrix(x, nrow = 1), method = 'L2')) , method = 'L-BFGS-B', lower = lower, upper = upper)
    
    XM = rbind(XM, opt_par$par)
    Y = c(Y, obj_fun(opt_par$par))
    
    print(sprintf('%d %f [%f %f]', i, obj_fun(opt_par$par), opt_par$par[1], opt_par$par[2]))
  }
  print(XM[which.min(Y),])
  return (XM[which.min(Y),])
}
obj_fun <- function(x){  (1 - x[1])^2 + 100 * (x[2] - x[1] * x[1])^2  } # solution is c(1, 1)

```

## 2D

```{r opt_2d}
#typical initial value (2,2), (5,5)
objective_2d <- function(x){  (1 - x[1])^2 + 100 * (x[2] - x[1] * x[1])^2  } # solution is c(1, 1)

####  BayesianOptimization --------
#Round = 20	x = -0.03818981	y = -0.04093246	Value = -1.257537 
OPT_Res <- BayesianOptimization(function(...){ x = as.numeric(list(...)); list(Score = -objective_2d(x),   Pred = 0)},
                                bounds = list(x = c(-5, 5), y = c(-5, 5)),
                                init_points = 20, n_iter = 10,
                                verbose = TRUE)


opt_control = list(trace = 2, maxit = 10000)
init_params = c(0, 10)
res = optim(init_params, objective_2d, method = 'BFGS', control = opt_control)
res = optim(init_params, objective_2d, method = 'L-BFGS-B', control = opt_control)
res = optim(init_params, objective_2d, method = "CG", control = opt_control)
res = optim(init_params, objective_2d, method = "SANN", control = opt_control)
res = optim(init_params, objective_2d, method = "Nelder-Mead", control = opt_control)

res = hjk(init_params, objective_2d, control = list(info = TRUE))
res = mads(init_params, objective_2d,  lower=-10, upper=10, control = list(trace = FALSE)) #too many evaluations
res = nmk(init_params, objective_2d, control = list(trace = TRUE))
res = nmkb(init_params, objective_2d, control = list(trace = TRUE), lower=-10, upper=10) #does not work

nmkb(power_v, function(x) objective_function(power_x, x)[2], lower = 0.1, upper = 2.0, control  = list(trace = TRUE)) #p

res = psoptim(init_params,objective_2d,lower=-10,upper=10,control=list(abstol=1e-8, trace = 1))

optimx(init_params, objective_2d, control = list(all.methods = TRUE))
optimx(init_params, objective_2d, method = "nmkb")
res = optimx(init_params, objective_2d, method = "Nelder-Mead")

DEoptim(objective_2d, c(-10, -10), c(10, 10))
```

## n-D

```{r opt_Nd}
objective_nd <- function(x){
  n <- length(x)
  sum (100*(x[1:(n-1)]^2 - x[2:n])^2 + (x[1:(n-1)] - 1)^2)
}

init_params <- rep(5, 10)

hjk(init_params, objective_nd)
nmk(init_params, objective_nd)
#mads(init_params, objective_nd)
optim(init_params, objective_nd, method = "Nelder-Mead", control = opt_control)
optim(init_params, objective_nd, method = 'BFGS', control = opt_control)
optim(init_params, objective_nd, method = "CG", control = opt_control)

psoptim(init_params,objective_nd,lower=-10,upper=10,control=list(abstol=1e-8))

optimx(init_params, objective_nd, control = list(all.methods = TRUE))
res = optimx(init_params, objective_nd, method = "Nelder-Mead")

res = DEoptim(objective_nd, rep(-10, length(init_params)), rep(10, length(init_params)), DEoptim.control(itermax = 4000, trace = FALSE))

plot(log(1 + res$member$bestvalit))

#https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/
#res <- nloptr( x0=init_params, eval_f=objective_nd, eval_grad_f=NULL, opts = list("algorithm"="NLOPT_GD_STOGO_RAND", "xtol_rel"=1.0e-8))


### A non-smooth problem from Hock & Schittkowski #78
hs78 <- function(x){
f <- rep(NA, 3)
f[1] <- sum(x^2) - 10
f[2] <- x[2]*x[3] - 5*x[4]*x[5]
f[3] <- x[1]^3 + x[2]^3 + 1
F <- prod(x) + 10*sum(abs(f))
return(F)
}
p0 <- c(-2,1.5,2,-1,-1)
ans2 <- mads(p0, hs78, control=list(trace=FALSE)) #minimum value around -2.81
```
